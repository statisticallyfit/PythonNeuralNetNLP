{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866e2391",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Tutorial 1: FARM Building Blocks\n",
    "\n",
    "## TASK 1: Text Classification\n",
    "\n",
    "GermEval 2018 (GermEval2018) (https://projects.fzai.h-da.de/iggsa/) is an open data set containing texts that need to be classified by whether they are offensive or not. There are a set of coarse and fine labels, but here we will only be looking at the coarse set which labels each example as either OFFENSE or OTHER. To tackle this task, we are going to build a classifier that is composed of Google's BERT language model and a feed forward neural network prediction head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbfc670",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import TextClassificationHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.train import Trainer\n",
    "from farm.utils import MLFlowLogger\n",
    "\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c93adf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### STEP 1: Setup\n",
    "Adjust the working directory to the current folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba994e6",
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "os.chdir(\"/development/projects/statisticallyfit/github/learningmathstat/PythonNeuralNetNLP/src/FARMHaystackStudy/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0c315",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Setup to be able to import my util functions in other folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9156d",
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "PATH: str = '/development/projects/statisticallyfit/github/learningmathstat/PythonNeuralNetNLP'\n",
    "\n",
    "UTIL_PATH: str = PATH + \"/src/utils/ModelUtil/\"\n",
    "\n",
    "FARM_PATH: str = PATH + \"/src/FARMHaystackStudy/TutorialsFARM/\"\n",
    "\n",
    "\n",
    "sys.path.append(PATH)\n",
    "sys.path.append(UTIL_PATH)\n",
    "sys.path.append(FARM_PATH)\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61448d0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Farm allows simple logging of many parameters & metrics. Let's use MLflow framework to track our experiment ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f8c73",
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "mlLogger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
    "mlLogger.init_experiment(experiment_name=\"Public_FARM\", run_name=\"Tutorial1_Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58824f26",
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "# We need to fetch the right device to drive the growth of our model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Devices available: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cae26",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### STEP 2: Data Handling\n",
    "Here we initialize a tokenizer to preprocess text. This is the BERT Tokenizer which uses byte pair encoding method (currently loaded with a German model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a8982",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.load(pretrained_model_name_or_path = \"bert-base-german-cased\",\n",
    "                           do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f37ed",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837831a8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To prepare the data for the model, we need a set of functions to transform data files into PyTorch Datasets.\n",
    "We group these together in Processor objects.\n",
    "We will need a new Processor object for each new source of data.\n",
    "The abstract class is in `farm.data_handling.processor.Processor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754f530",
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "LABEL_LIST: List[str] = [\"OTHER\", \"OFFENSE\"]\n",
    "BATCH_SIZE: int = 32\n",
    "MAX_SEQ_LEN: int = 128\n",
    "METRIC: str = \"f1_macro\"\n",
    "\n",
    "processor = TextClassificationProcessor(tokenizer = tokenizer,\n",
    "                                        max_seq_len = MAX_SEQ_LEN,\n",
    "                                        data_dir = \"data/germeval18\",\n",
    "                                        label_list = LABEL_LIST,\n",
    "                                        metric = METRIC,\n",
    "                                        label_column_name = \"coarse_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47911dbf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We need a `DataSilo` to keep our train, dev, and test sets separate. The `DataSilo` will call the functions in the `Processor` to generate these sets.\n",
    "\n",
    "From the `DataSilo` we can fetch a PyTorch `DataLoader` object which will be passed on to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e7835",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "dataSilo  = DataSilo(processor = processor,\n",
    "                     batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91e3d4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### STEP 3: Modeling\n",
    "In FARM, we make a strong distinction between the language model and prediction head so that you can mix and match different building blocks for your needs.\n",
    "\n",
    "For example, in the transfer learning paradigm, you might have the one language model that you will be using for both document classification and NER. Or you perhaps you have a pretrained language model which you would like to adapt to your domain, then use for a downstream task such as question answering.\n",
    "\n",
    "All this is possible within FARM and requires only the replacement of a few modular components, as we shall see below.\n",
    "\n",
    "Let's first have a look at how we might set up a model.\n",
    "\n",
    "#### Language Model\n",
    "* The language model is the foundation on which modern NLP systems are built. They encapsulate a general understanding of sentence semantics and are not specific to any one task.\n",
    "* Here we are using Google's BERT model as implemented by HuggingFace. The model being loaded is a German model that we trained.\n",
    "* Can also change the MODEL_NAME_OR_PATH to point to a BERT model that you have saved or download one connected to the HuggingFace repository. See https://huggingface.co/models for a list of available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9aad2",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME_OR_PATH = \"bert-base-german-cased\"\n",
    "languageModel = LanguageModel.load(MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e7fe4",
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "languageModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869e720",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Prediction Head\n",
    "* A Prediction head is a model that processes the output of the language model for a specific task. It will look different depending on the task (text classification, NER, QA ...)\n",
    "* Prediction heads should generate logits over the available prediction classes and contain methods to convert these logits to losses or predictions.\n",
    "\n",
    "Here we use `TextClassificationHead` prediction head which receives a single fixed length sentence vector and processes it using a feed forward neural network.\n",
    "* NOTE: `layer_dims` is a list of dimensions = `[input_dims, hidden_1_dims, hidden_2_dims, ..., output_dims]`\n",
    "* Using a single layer network that takes in a vector length 768 (default size of BERT output), and the prediction head outputs a vector of length 2 (number of classes in the GermEval18 coarse dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e0edb",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "predictionHead = TextClassificationHead(num_labels = len(LABEL_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167cc5f",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "predictionHead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc39363",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Adaptive Model\n",
    "The language model and prediction head are coupled together in the `AdaptiveModel`, which is a class that takes care of model saving and loading. Also coordinates cases where there is more than one prediction head.\n",
    "\n",
    "Its parameter `EMBEDS_DROPOUT_PROB` is the probability that an element of the output vector from the language model will be set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecdf7d2",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "EMBEDS_DROPOUT_PROB: float = 0.1\n",
    "\n",
    "model = AdaptiveModel(language_model = languageModel,\n",
    "                      prediction_heads = [predictionHead],\n",
    "                      embeds_dropout_prob = EMBEDS_DROPOUT_PROB,\n",
    "                      lm_output_types = [\"per_sequence\"],\n",
    "                      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4bc42",
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f82f4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### STEP 4: Training\n",
    "Here we initialize a BERT Adam optimizer with linear warmup and warmdown. Can set learning rate, warmup proportion and number of epochs to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052238c",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE: float = 2e-5\n",
    "NUM_EPOCHS: int = 1\n",
    "\n",
    "modelOpt, optimizer, learnRateSchedule = initialize_optimizer(\n",
    "          model = model,\n",
    "          device = device,\n",
    "          learning_rate = LEARNING_RATE,\n",
    "          n_batches = len(dataSilo.loaders[\"train\"]),\n",
    "          n_epochs = NUM_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46ff56",
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "# NOTE: the modelOpt (after optimizer initialization and previous model seem to be exactly the same (tested using getParamInfo() for each from my ModelUtils to see if the tensor numbers differed some how but they seem the same)\n",
    "modelOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6279a2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Training loop here can trigger evaluation using the dev data and can trigger evaluation after training using the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715050bf",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "NUM_GPU: int = 1 # positive if CUDA is available, else 0\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = modelOpt, \n",
    "    optimizer = optimizer, \n",
    "    data_silo = dataSilo, \n",
    "    epochs = NUM_EPOCHS, \n",
    "    n_gpu = NUM_GPU, \n",
    "    lr_schedule = learnRateSchedule,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633b1cf",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed6b04",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "modelTrain = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a2b8f",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "modelTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b34f4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### STEP 5: Inference\n",
    "Test the model on a sample (doing inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09f28e",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "from farm.infer import Inferencer \n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "modelInfer = Inferencer(\n",
    "    processor = processor,\n",
    "    model = modelTrain, \n",
    "    task_type = \"text_classification\", \n",
    "    gpu = True\n",
    ")\n",
    "\n",
    "basicTexts: List[Dict[str, str]] = [\n",
    "    {\"text\" : \"Martin ist ein Idiot\"},\n",
    "    {\"text\" : \"Martin MÃ¼ller spielt Handball in Berlin\"}\n",
    "]\n",
    "\n",
    "result = modelInfer.inference_from_dicts(dicts = basicTexts)\n",
    "\n",
    "PrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e01cd7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Can see it was very confident that the second text about handball wasn't offensive while the first one was. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81428196",
   "metadata": {},
   "source": [
    "\n",
    "## TASK 2: Named Entity Recognition (NER)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
