{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/manual-back-prop-with-tensorflow-decoupled-recurrent-neural-network-modified-nn-from\n",
    "# -google-f9c085fe8fae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, sys\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [],
   "source": [
    "np.random.seed(678)\n",
    "tf.set_random_seed(678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): # sigmoid\n",
    "    return tf.div(tf.constant(1.0), tf.add(tf.constant(1.0), tf.exp(tf.negative(x)) ) )\n",
    "\n",
    "def derivativeSigmoid(x):\n",
    "    return tf.multiply(sigmoid(x), tf.subtract(tf.constant(1.0), sigmoid(x)))\n",
    "\n",
    "def tanh(x):\n",
    "    return tf.tanh(x)\n",
    "\n",
    "def derivativeTanh(x):\n",
    "    return tf.subtract(tf.constant(1.0), tf.square(tf.tanh(x)))\n",
    "\n",
    "def arctan(x):\n",
    "    return tf.atan(x)\n",
    "def derivativeArctan(x):\n",
    "    return tf.div(tf.constant(1.0), tf.subtract(tf.constant(1.0), tf.square(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Declare Training Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnistData = input_data.read_data_sets(\"data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = mnistData.test\n",
    "images, labels = train.images, train.labels\n",
    "onlyZeroIndex, onlyOneIndex = np.where(labels == 0)[0], np.where(labels == 1)[0]\n",
    "onlyZeroImage, onlyZeroLabel = images[onlyZeroIndex], np.expand_dims(labels[onlyZeroIndex], axis = 1)\n",
    "onlyOneImage, onlyOneLabel = images[onlyOneIndex], np.expand_dims(labels[onlyOneIndex], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "print(type(np.where(labels==0)))\n",
    "print(type(np.where(labels == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onlyZeroIndex:  (1135,)  |  <class 'numpy.ndarray'>\nlabels[onlyZeroIndex]:  (980,) | <class 'numpy.ndarray'>\n\nimages:  (10000, 784) | <class 'numpy.ndarray'>\nonlyZeroImage:  (980, 784) | <class 'numpy.ndarray'>\nonlyZeroLabel:  (980, 1) | <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "\n",
    "# onlyZeroImage, onlyZeroLabel = images[onlyZeroIndex], np.expand_dims(labels[onlyZeroIndex], axis = 1)\n",
    "print(\"onlyZeroIndex: \", onlyOneIndex.shape, \" | \", type(onlyZeroIndex))\n",
    "print(\"labels[onlyZeroIndex]: \", labels[onlyZeroIndex].shape, \"|\", type(labels[onlyZeroIndex]))\n",
    "print()\n",
    "print(\"images: \", images.shape, \"|\", type(images))\n",
    "print(\"onlyZeroImage: \", onlyZeroImage.shape, \"|\", type(onlyZeroImage))\n",
    "print(\"onlyZeroLabel: \", onlyZeroLabel.shape, \"|\", type(onlyZeroLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onlyOneIndex:  (1135,)  |  <class 'numpy.ndarray'>\nlabels[onlyOneIndex]:  (1135,) | <class 'numpy.ndarray'>\n\nimages:  (10000, 784) | <class 'numpy.ndarray'>\nonlyOneImage:  (1135, 784) | <class 'numpy.ndarray'>\nonlyOneLabel:  (1135, 1) | <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "\n",
    "# onlyOneImage, onlyOneLabel = images[onlyOneIndex], np.expand_dims(labels[onlyOneIndex], axis=1)\n",
    "print(\"onlyOneIndex: \", onlyOneIndex.shape, \" | \", type(onlyOneIndex))\n",
    "print(\"labels[onlyOneIndex]: \", labels[onlyOneIndex].shape, \"|\", type(labels[onlyOneIndex]))\n",
    "print()\n",
    "print(\"images: \", images.shape, \"|\", type(images))\n",
    "print(\"onlyOneImage: \", onlyOneImage.shape, \"|\", type(onlyOneImage))\n",
    "print(\"onlyOneLabel: \", onlyOneLabel.shape, \"|\", type(onlyOneLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape, labels.shape =  (2115, 784) (2115, 1)\nimages.shape, labels.shape =  (2115, 784) (2115, 1)\n"
     ]
    }
   ],
   "source": [
    "images = np.vstack((onlyZeroImage, onlyOneImage)) # stacking arrays as rows vertically (rows on top of each other)\n",
    "labels = np.vstack((onlyZeroLabel, onlyOneLabel))\n",
    "print(\"images.shape, labels.shape = \", images.shape, labels.shape)\n",
    "\n",
    "images, labels = shuffle(images, labels) # shuffles the rows among each array: images and labels, but\n",
    "# the objects themselves are kept separate so images remains images, and labels remains labels.\n",
    "print(\"images.shape, labels.shape = \", images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images:  (2115, 784)\nlabels:  (2115, 1)\na.shape =  (4, 3)\n[[1 2 3]\n [8 1 1]\n [4 5 6]\n [2 6 4]]\n\n[[ 7  8  9]\n [10 12 13]\n [ 1  1  0]\n [ 2  4  2]]\na.shape =  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "print(\"images: \", images.shape)\n",
    "print(\"labels: \", labels.shape)\n",
    "\n",
    "# testing shuffle with vstack \n",
    "a = np.vstack(([1,2,3], [4,5,6], [2,6,4], [8,1,1]))\n",
    "a\n",
    "b = np.vstack(([7,8,9],[1,1,0], [2,4,2], [10,12,13]))\n",
    "b\n",
    "print(\"a.shape = \", a.shape)\n",
    "a, b = shuffle(a, b) # returns shuffled indices and sets values of a and b by the shuffled indices. \n",
    "# so both a and b are shuffled in the same order. \n",
    "print(a)\n",
    "print()\n",
    "print(b)\n",
    "print(\"a.shape = \", a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImageNum, trainingImageNum = 20, 100\n",
    "testingImages, testingLabels = images[:testImageNum, :], labels[:testImageNum]\n",
    "trainingImages, trainingLabels = images[testImageNum : testImageNum + trainingImageNum , :], \\\n",
    "                                 labels[testImageNum : testImageNum + trainingImageNum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testingImages.shape:  (20, 784)\ntestingLabels.shape:  (20, 1)\n\ntrainingImages.shape:  (100, 784)\ntrainingLabels.shape:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "print(\"testingImages.shape: \", testingImages.shape)\n",
    "print(\"testingLabels.shape: \", testingLabels.shape)\n",
    "print()\n",
    "print(\"trainingImages.shape: \", trainingImages.shape)\n",
    "print(\"trainingLabels.shape: \", trainingLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpoch = 100\n",
    "totalCost = 0\n",
    "costArray = []\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x7fd6268dea58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What weights do I need? And how to initialize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    learningRate_x = tf.Variable(tf.constant(0.001))\n",
    "    learningRate_rec = tf.Variable(tf.constant(0.000001))\n",
    "    learningRate_sg = tf.Variable(tf.constant(0.0001))\n",
    "    \n",
    "    hiddenStates = tf.Variable(tf.random_normal([784, 3]))\n",
    "    \n",
    "    W_x = tf.Variable(tf.random_normal([784, 784], stddev=0.45) * tf.constant(0.2))\n",
    "    W_rec = tf.Variable(tf.random_normal([784, 784], stddev=0.035) * tf.constant(0.2))\n",
    "    W_fc = tf.Variable(tf.random_normal([784, 1], stddev=0.95) * tf.constant(0.2))\n",
    "    \n",
    "    W_sg_1 = tf.Variable(tf.random_normal([784, 784], stddev=0.35) * tf.constant(0.2))\n",
    "    W_sg_2 = tf.Variable(tf.random_normal([784, 784], stddev=0.35) * tf.constant(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 1])\n",
    "    update = []\n",
    "    hiddenLayerUpdate = []\n",
    "    \n",
    "    layer1 = tf.add(tf.matmul(x, W_x), \n",
    "                    tf.matmul(tf.expand_dims(hiddenStates[:,0],axis=0), W_rec))\n",
    "    layer1A = tanh(layer1)\n",
    "    hiddenLayerUpdate.append(tf.assign(hiddenStates[:,1], tf.squeeze(layer1A)))\n",
    "\n",
    "    # # ----- Time Stamp 1 Syn Grad Update --------------------------------------------\n",
    "    grad_1sg_part_1 = tf.matmul(layer1A, W_sg_1)\n",
    "    grad_1sg_part_2 = derivativeTanh(layer1)\n",
    "    grad_1sg_part_rec = tf.expand_dims(hiddenStates[:,0], axis=0)\n",
    "    grad_1sg_part_x = x\n",
    "    \n",
    "    grad_1sg_rec = tf.matmul(tf.transpose(grad_1sg_part_rec), \n",
    "                             tf.multiply(grad_1sg_part_1, grad_1sg_part_2))\n",
    "    grad_1sg_x = tf.matmul(tf.transpose(grad_1sg_part_x), \n",
    "                           tf.multiply(grad_1sg_part_1, grad_1sg_part_2))\n",
    "    \n",
    "    update.append(tf.assign(W_rec, tf.add(W_rec, tf.multiply(learningRate_rec, grad_1sg_rec))))\n",
    "    update.append(tf.assign(W_x, tf.add(W_x, tf.multiply(learningRate_rec, grad_1sg_x))))\n",
    "    \n",
    "    grad_true_0 = tf.matmul(tf.multiply(grad_1sg_part_1, grad_1sg_part_2), \n",
    "                            tf.transpose(W_rec))\n",
    "    # end of time stamp 1 --------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    layer2 = tf.add(tf.matmul(x, W_x), tf.matmul(tf.expand_dims(hiddenStates[:,1],axis=0), W_rec))\n",
    "    layer2A = tanh(layer2)\n",
    "    hiddenLayerUpdate.append(tf.assign(hiddenStates[:,2], tf.squeeze(layer2A)))\n",
    "\n",
    "\n",
    "    # # ----- Time Stamp 2 Syn Grad Update ----------------------------------------------\n",
    "    grad_2sg_part_1 = tf.matmul(layer2A, W_sg_2)\n",
    "    grad_2sg_part_2 = derivativeTanh(layer2)\n",
    "    grad_2sg_part_rec = tf.expand_dims(hiddenStates[:,1],axis=0)\n",
    "    grad_2sg_part_x = x \n",
    "    \n",
    "    grad_2sg_rec = tf.matmul(tf.transpose(grad_2sg_part_rec), \n",
    "                             tf.multiply(grad_2sg_part_1, grad_2sg_part_2))\n",
    "\n",
    "    grad_2sg_x = tf.matmul(tf.transpose(grad_2sg_part_x),\n",
    "                         tf.multiply(grad_2sg_part_1, grad_2sg_part_2))\n",
    "    \n",
    "    update.append(tf.assign(W_rec, tf.add(W_rec, tf.multiply(learningRate_rec, grad_2sg_rec))))\n",
    "    update.append(tf.assign(W_x, tf.add(W_x, tf.multiply(learningRate_rec, grad_2sg_x))))\n",
    "    # HELP: shouldn't the xlayer have learningRate_x not learningRate_rec? Same for\n",
    "    # previous time stamp?\n",
    "    \n",
    "    grad_true_1_from_2 = tf.matmul(tf.multiply(grad_2sg_part_1, grad_2sg_part_2), \n",
    "                                   tf.transpose(W_rec))\n",
    "    # end of time stamp 2 --------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # # ----- Time Stamp 1 True Gradient Update ----------------------------------------\n",
    "    grad_true_1_part_1 = tf.subtract(grad_1sg_part_1, grad_true_1_from_2)\n",
    "    grad_true_1_part_2 = tf.expand_dims(hiddenStates[:,1],axis=0)\n",
    "    grad_true_1 = tf.matmul(tf.transpose(grad_true_1_part_2), grad_true_1_part_1)\n",
    "    update.append(tf.assign(W_sg_1, \n",
    "                            tf.subtract(W_sg_1, tf.multiply(learningRate_sg, grad_true_1))))\n",
    "    # end of true time stamp 1 ---------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    # # ----- Fully Connected for Classification ------\n",
    "    layer3 = tf.matmul(tf.expand_dims(hiddenStates[:,2], axis=0), W_fc)\n",
    "    layer3A = sigmoid(layer3)\n",
    "    # -------------------------------------------------\n",
    "\n",
    "    # # -- MAN BACK PROP --------------------------------\n",
    "    costFunction = tf.multiply(tf.square(tf.subtract(layer3A, y)), tf.constant(0.5))\n",
    "    # ---------------------------------------------------\n",
    "    \n",
    "    # # -- AUTO BACK PROP ------------------------------\n",
    "    costFunctionAuto = tf.train.GradientDescentOptimizer(0.1).minimize(costFunction)\n",
    "    # ---------------------------------------------------\n",
    "\n",
    "    # # ------- FC weight update ---------------------\n",
    "    grad_fc_part_1 = tf.subtract(layer3A, y)\n",
    "    grad_fc_part_2 = derivativeSigmoid(layer3)\n",
    "    grad_fc_part_3 = tf.expand_dims(hiddenStates[:,2], axis=0)\n",
    "    grad_fc = tf.matmul(tf.transpose(grad_fc_part_3), \n",
    "                        tf.multiply(grad_fc_part_1, grad_fc_part_2))\n",
    "    update.append(tf.assign(W_fc, tf.subtract(W_fc, tf.multiply(learningRate_x, grad_fc))))\n",
    "    \n",
    "    grad_true_2_from_3 = tf.matmul(tf.multiply(grad_fc_part_1, grad_fc_part_2), tf.transpose(W_fc))\n",
    "    # end FC weight update ---------------------------\n",
    "\n",
    "    # # ----- Time Stamp 2 True Gradient Update -------------------------------------------\n",
    "    grad_true_2_part_1 = tf.subtract(grad_2sg_part_1, grad_true_2_from_3)\n",
    "    grad_true_2_part_2 = tf.expand_dims(hiddenStates[:,2], axis=0)\n",
    "    grad_true_2 = tf.matmul(tf.transpose(grad_true_2_part_2), grad_true_2_part_1)\n",
    "    update.append(tf.assign(W_sg_2, tf.subtract(W_sg_2, tf.multiply(learningRate_sg, grad_true_2))))\n",
    "    # end time stamp 2 true update ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  0  current cost:  16.04809685483997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  1  current cost:  14.027080203220066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  2  current cost:  13.532829136438522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  3  current cost:  11.63827915292859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  4  current cost:  14.698083102077362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  5  current cost:  12.337620037062152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  6  current cost:  12.554667362830514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  7  current cost:  9.160612510802821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  8  current cost:  9.660438825277197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  9  current cost:  8.267872954660561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  10  current cost:  9.824754162641511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  11  current cost:  8.216326195017245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  12  current cost:  10.820889492759306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  13  current cost:  8.368131554938373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  14  current cost:  9.369503091620572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  15  current cost:  10.24295429963604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  16  current cost:  8.731696812242262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  17  current cost:  11.297150840116956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  18  current cost:  12.987962413092646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  19  current cost:  13.125591440337303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  20  current cost:  11.484021779207069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  21  current cost:  12.088455268203688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  22  current cost:  10.066632865044994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  23  current cost:  13.537876663202042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  24  current cost:  11.720953665040724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  25  current cost:  12.212204265779292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  26  current cost:  11.631006009614794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  27  current cost:  11.690727245317248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  28  current cost:  9.793007940563257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  29  current cost:  8.143402113932098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  30  current cost:  6.2222765883798274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  31  current cost:  6.926407913521871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  32  current cost:  8.32681396001135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  33  current cost:  8.091886250540028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  34  current cost:  9.879842570248002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  35  current cost:  11.216188499533018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  36  current cost:  12.658732529002009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  37  current cost:  11.418035481440711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  38  current cost:  11.035859185420122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  39  current cost:  12.93567504668772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  40  current cost:  11.78818746325669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  41  current cost:  12.236155717495421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  42  current cost:  11.116389961654932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration:  43  current cost:  11.407148235099157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6d145f319263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# if you want to do manual backprop, run this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             output = sess.run([costFunction, update, hiddenLayerUpdate],\n\u001b[0;32m---> 14\u001b[0;31m                               feed_dict={x:currentImage, y:currentIndex})\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# if you want to do auto differential uncomment this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/development/bin/python/conda3_shared/envs/datasci_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/development/bin/python/conda3_shared/envs/datasci_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/development/bin/python/conda3_shared/envs/datasci_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/development/bin/python/conda3_shared/envs/datasci_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/development/bin/python/conda3_shared/envs/datasci_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/development/bin/python/conda3_shared/envs/datasci_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    totalCost = 0\n",
    "    \n",
    "    for i in range(numEpoch):\n",
    "        for currentImageIndex in range(len(trainingImages)):\n",
    "            \n",
    "            currentImage = np.expand_dims(trainingImages[currentImageIndex], axis=0)\n",
    "            currentIndex = np.expand_dims(trainingLabels[currentImageIndex], axis=0)\n",
    "            \n",
    "            # if you want to do manual backprop, run this line\n",
    "            output = sess.run([costFunction, update, hiddenLayerUpdate],\n",
    "                              feed_dict={x:currentImage, y:currentIndex})\n",
    "            \n",
    "            # if you want to do auto differential uncomment this line\n",
    "            #output = sess.run([costFunction, costFunctionAuto, hiddenLayerUpdate],\n",
    "            #                  feed_dict={x:currentImage, y:currentIndex})\n",
    "            \n",
    "            totalCost = totalCost + output[0].sum()\n",
    "            \n",
    "        print(\"Current iteration: \", i, \" current cost: \", totalCost)\n",
    "        costArray.append(totalCost)\n",
    "        totalCost = 0 \n",
    "    \n",
    "    plt.plot(np.arange(numEpoch), costArray)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    for currentImageIndex in range(len(testingImages)):\n",
    "        currentImage = np.expand_dims(testingImages[currentImageIndex], axis=0)\n",
    "        currentLabel = testingLabels[currentImageIndex]\n",
    "        output = sess.run([layer3A, hiddenLayerUpdate], feed_dict={x:currentImage})\n",
    "        print(currentImageIndex, \" : \", output[0], \" : \", np.round(output[0]), \" : \", currentLabel)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
