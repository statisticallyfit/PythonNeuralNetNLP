{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "[Blog Source](https://synergo.atlassian.net/wiki/spaces/DataScience/pages/1511359082/Building+the+Transformer+XL+from+Scratch)\n",
    "$\\hspace{1em}$ | $\\hspace{1em}$\n",
    "[Code Source](https://github.com/keitakurita/Practical_NLP_in_PyTorch/blob/master/deep_dives/transformer_xl_from_scratch.ipynb)\n",
    "# Building the [Transformer XL](https://synergo.atlassian.net/wiki/spaces/KnowRes/pages/1513586716) from Sratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.tensor as Tensor\n",
    "from torch import Size, Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import Dropout, LayerNorm, Linear, Sequential, ReLU, Embedding, ModuleList, CrossEntropyLoss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "from typing import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Making files in utils folder visible here: to import my local print functions for nn.Module objects\n",
    "sys.path.append(os.getcwd() + \"/src/utils/\")\n",
    "\n",
    "from src.utils.ModelUtil import *\n",
    "\n",
    "# For being able to import files within TransformerXL folder\n",
    "sys.path.append(os.getcwd() + '/src/ModelStudy/TransformerXL/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "# Building pathname for images (from Colab)\n",
    "\n",
    "# Set current working directory\n",
    "\n",
    "imagePath = os.getcwd() # now path is the above\n",
    "print(f\"imagePath = {imagePath}\\n\")\n",
    "imagePath += \"/src/ModelStudy/images/\"\n",
    "print(f\"imagePath = {imagePath}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "# Training the Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "TESTING: bool = True\n",
    "\n",
    "\n",
    "N = 1000\n",
    "L = 4 # num layers\n",
    "M = 5 # memory length\n",
    "H = 4 # num heads\n",
    "S = 7 # sequence length (sentence length)\n",
    "P = 6 # previous sequence length\n",
    "B = 3 # batch size\n",
    "E = 32 # embedding dimension\n",
    "I, F = 17, 71 # mhaInnerDim, ffInnerDim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "### Train Step 1: Prepare Configurations\n",
    "The configurations we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "from src.ModelStudy.TransformerXL.Config import Config\n",
    "\n",
    "\n",
    "# We will use prime numbers as a dummy test to ensure our implementation is correct\n",
    "config: Config = Config(seed = 101, debug = False, warmupStep = 0,\n",
    "                        minLearnRate = 0., # Check default params:\n",
    "                        dropoutA = 0., # dropout for attention\n",
    "                        clip = 0.25,\n",
    "                        logInterval = 200,\n",
    "                        evalInterval = 100)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    config.update(fromDict = dict(\n",
    "        debug = True,\n",
    "        learningRate = 0.00025,\n",
    "        batchSize = 8, # batch size\n",
    "        numEpochs = 2,\n",
    "        maxStep = 10000, # shorten for testing\n",
    "        numLayers = L, # 4\n",
    "        numHeads = H, # 3\n",
    "        modelDim = E, # 32\n",
    "        mhaInnerDim = I, # 17\n",
    "        ffInnerDim = 71,\n",
    "        dropoutO = 0.1,\n",
    "        trainBPTT = 33,\n",
    "        evalBPTT = 41,\n",
    "        memoryLen = 41,\n",
    "        evalMemoryLen = 63\n",
    "    ))\n",
    "else:\n",
    "    config.update(fromDict = dict(\n",
    "        #debug = True,\n",
    "        learningRate = 0.00025,\n",
    "        batchSize = 22, # batch size\n",
    "        numEpochs = 2,\n",
    "        maxStep = 400000, # shorten for testing\n",
    "        numLayers = 12,\n",
    "        numHeads = 8,\n",
    "        modelDim = 512,\n",
    "        mhaInnerDim = 64,\n",
    "        ffInnerDim = 2048,\n",
    "        dropoutO = 0.1,\n",
    "        trainBPTT = 512,\n",
    "        evalBPTT = 128,\n",
    "        memoryLen = 512,\n",
    "        evalMemoryLen = 2100\n",
    "    ))\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "### Train Step 2: Preparing the Data Loader\n",
    "Data loading for the Transformer Xl is similar to data loading for an RNN based language model but is different from standard data loading.\n",
    "\n",
    "**Data Loading for Transformer XL:** Suppose we chunked the input into sequence of `batchSize = 4` words to feed into the model. Remember that Transformer XL is stateful, meaning the computations of each minibatch are carried over to the next minibatch. ($\\color{red}{\\text{Question: is this referring to how } \\texttt{newMemory } \\text{is computed in the } \\texttt{forward } \\text{method of the } \\texttt{TransformerXL} \\text{class?}}$). For a minibatch of size `batchSize = 1`, handling this is simple. We just chunk the input and feed it into the model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "Image(filename =imagePath + \"batchsizeone_wrong.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Now what happens if the `batchSize = 2`? We can't split the sentence like this (below) otherwise we would be breaking the dependencies between segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "Image(filename =imagePath + \"batchsizetwo_wrong.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "The correct way to split the corpus with `batchSize = 2` is to feed the batches like this (below). We should have the sentences split across batches rather than keeping as much of the sentence within the batch, and letting the rest of the sentence split across the rest of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "Image(filename =imagePath + \"batchsizetwo_correct.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "markdown"
   },
   "source": [
    "**General Rule:** Generalizing this, we first divide the corpus into `batchSize` length segments, then feed each segment piece by piece into the model.\n",
    "\n",
    "**Example of Batching and Feeding:** Suppose `batchSize = 4` and our entire corpus looks like this:\n",
    "\n",
    "`pytorch is an amazing deep learning framework that makes nlp really easy`\n",
    "\n",
    "We want to ensure the previous batch contains the previous segment at the same position. In other words, assuming we fed the model one word at a time, we want to iterate over this sentence like this:\n",
    "\n",
    "`Batch 1: pytorch  amazing   framework  nlp\n",
    "Batch 2: is       deep      that       really\n",
    "Batch 3: an       learning  makes      easy`\n",
    "\n",
    "**Key feature of the Method:** We can reconstruct the original sentence by reading from  **top to bottom -> left to right** instead of **left to right -> top to bottom**. Basically we create batches by splitting the sentence *across* batch structure not *within* batch structure.\n",
    "\n",
    "In reality, we feed the model with a sequence of words for each batch. The length of this sequence is commonly referred to the `bptt` (back propagation through time) length, since this is the maximum length the gradients propagate through in the sequence direction. With a longer `bptt` length of 2 for example, the `minibatch` would be of shape `(batchSize, bptt)` and would look like:\n",
    "\n",
    "`Batch 1: pytorch  amazing   framework  nlp\n",
    "         is       deep      that       really\n",
    "Batch 2: an       learning  makes      easy`\n",
    "\n",
    "We can implement this in a `DataLoader` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "from src.ModelStudy.TransformerXL.LMDataLoader import LMDataLoader\n",
    "\n",
    "\n",
    "# Testing out the data loader implementation\n",
    "(N, B, BPTT) = (1000, 16, 10)\n",
    "testCorpus: Tensor = torch.arange(N)\n",
    "testCorpus[:BPTT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "loader: LMDataLoader = LMDataLoader(data = testCorpus, batchSize = B, bptt = BPTT)\n",
    "\n",
    "loaderIter: List[Tuple[Tensor, Tensor, int]] = list(iter(loader))\n",
    "batch_0, target_0, diff_0 = loaderIter[0]\n",
    "batch_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "target_0\n",
    "\n",
    "assert (batch_0 + 1 == target_0).all(), \"Test target values are shifted one higher than values in batch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "assert (batch_0 == batch_0[0:BPTT, :]).all(), \"Test first dimension of batch has length BPTT\"\n",
    "assert batch_0.shape == (BPTT, B) == (10, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "allBatches: List[Tensor] = [b for b,_,_ in loaderIter]\n",
    "allTargets: List[Tensor] = [t for _,t,_ in loaderIter]\n",
    "allDiffs: List[Tensor] = [d for _,_,d in loaderIter]\n",
    "\n",
    "BatchTensor = Tensor\n",
    "BPTTTensor = Tensor\n",
    "\n",
    "def getBPTTCols(colIndex: int, tensors: List[BatchTensor]) -> List[BPTTTensor]:\n",
    "    \"\"\"Expects the elements in tensors list to have shape == (BPTT, B) so that when indexing along columns, it gets a list of tensors which are all shape == (BPTT, )\n",
    "    \"\"\"\n",
    "    return [tensors[i][:,colIndex] for i in range(0, len(tensors))]\n",
    "\n",
    "getBPTTCols(0, allBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "getBPTTCols(0, allTargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "getBPTTCols(1, allBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "getBPTTCols(2, allBatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "### Train Step 3: Loading the Actual Data\n",
    "Using the Penn Treebank dataset to benchmark our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATASET_NAME_STR: str = \"penn\"\n",
    "\n",
    "# os.chdir('/development/projects/statisticallyfit/github/learningmathstat/PythonNeuralNetNLP')\n",
    "dataPath: str = os.getcwd() + \"/src/ModelStudy/TransformerXL/data/\"\n",
    "DATA_DIR: Path = Path(dataPath) / DATASET_NAME_STR\n",
    "DATA_DIR.absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Using a utility vocabulary class borrowed directly from the Transformer XL repo to numericalize our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "# sys.path.append(os.getcwd() + \"/src/ModelStudy/TransformerXL/utils/\")\n",
    "sys.path.append(os.getcwd() + \"/src/ModelStudy/TransformerXL/\")\n",
    "# sys.path.pop()\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "from src.ModelStudy.TransformerXL.vocabulary import Vocab\n",
    "\n",
    "vocab: Vocab = Vocab(special = [\"<eos>\"], lower_case = True)\n",
    "\n",
    "assert (DATA_DIR / \"train.txt\").absolute() == (DATA_DIR / \"train.txt\")\n",
    "\n",
    "(DATA_DIR / \"train.txt\").absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "trainVocab: List[List[str]] = vocab.count_file(DATA_DIR / \"train.txt\")\n",
    "# The `Counter` object in `vocab` counts how many times the token / word has appeared (cumulatively for all the text)\n",
    "print(list(vocab.counter.items())[:20]) # some token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "validVocab: List[List[str]] = vocab.count_file(DATA_DIR / \"valid.txt\")\n",
    "print(list(vocab.counter.items())[:20]) # validation text has added 10 'aer' tokens, for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "testVocab: List[List[str]] = vocab.count_file(DATA_DIR / \"test.txt\")\n",
    "print(list(vocab.counter.items())[:20]) # some token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "print(f\"trainVocab length = {len(trainVocab)}\")\n",
    "print(f\"validVocab length = {len(validVocab)}\")\n",
    "print(f\"testVocab length = {len(testVocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "print(trainVocab[3000:3010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "print(validVocab[3000:3010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "print(testVocab[3000:3010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "lengthsTrain: List[int] = [len(tokenList) for tokenList in trainVocab]\n",
    "print(f\"lengthsTrain[4000:4200]: \\n\\n{lengthsTrain[4000:4200]}\\n\")\n",
    "\n",
    "lengthsValid: List[int] = [len(tokenList) for tokenList in validVocab]\n",
    "print(f\"lengthsValid[2000:2200]: \\n\\n{lengthsValid[2000:2200]}\\n\")\n",
    "\n",
    "lengthsTest: List[int] = [len(tokenList) for tokenList in testVocab]\n",
    "print(f\"lengthsTest[2000:2200]: \\n\\n{lengthsTest[2000:2200]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "### Train Step 4: Build the Vocabulary\n",
    "Encoding the vocabulary text `List[List[str]]` into Tensor of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "vocab.build_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Encoding the text into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "ADD_EOS, ADD_DOUBLE_EOS = True, False\n",
    "trainData: Tensor = vocab.encode_file(path = DATA_DIR / \"train.txt\", ordered = True, add_eos = ADD_EOS, add_double_eos = ADD_DOUBLE_EOS, verbose = True)\n",
    "trainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "trainData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Illustrating for the first line  how tokenization and encoding occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "print(trainVocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "line_0: str = ' '.join(trainVocab[0])\n",
    "line_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "symbols_0: List[str] = vocab.tokenize(line = line_0, add_eos = ADD_EOS, add_double_eos = ADD_DOUBLE_EOS)\n",
    "print(symbols_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "tensor_0: Tensor = vocab.convert_to_tensor(symbols = symbols_0)\n",
    "tensor_0\n",
    "\n",
    "assert (tensor_0 == trainData[0:25]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "validData: Tensor = vocab.encode_file(path = DATA_DIR / \"valid.txt\", ordered = True, add_eos = True, add_double_eos = False, verbose = True)\n",
    "validData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "testData: Tensor = vocab.encode_file(path = DATA_DIR / \"test.txt\", ordered = True, add_eos = True, add_double_eos = False, verbose = True)\n",
    "testData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "### Train Step 5: Prepare the Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "trainIter: LMDataLoader = LMDataLoader(data = trainData,\n",
    "                                       batchSize = config.batchSize,\n",
    "                                       bptt = config.trainBPTT,\n",
    "                                       device = device)\n",
    "\n",
    "validIter: LMDataLoader = LMDataLoader(data = validData,\n",
    "                                       batchSize = config.batchSize,\n",
    "                                       bptt = config.trainBPTT,\n",
    "                                       device = device)\n",
    "\n",
    "testIter: LMDataLoader = LMDataLoader(data = testData,\n",
    "                                      batchSize = config.batchSize,\n",
    "                                      bptt = config.trainBPTT,\n",
    "                                      device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Checking the sizes and shapes of the `batch`, `target`, and `diff` in the `trainLoaderIter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "# Studying the shapes for trainIter loader:\n",
    "\n",
    "# This is the output of the __iter__() method in LMDataLoader\n",
    "trainLoaderIter: List[Tuple[Tensor, Tensor, int]] = list(iter(trainIter))\n",
    "\n",
    "\n",
    "for batch, target, diff in trainLoaderIter[: len(trainLoaderIter) - 1]:\n",
    "    assert batch.names == target.names == ('S', 'B')\n",
    "    assert batch.shape == target.shape == (config.trainBPTT, config.batchSize)\n",
    "    assert (batch == batch[0 : config.trainBPTT, :]).all()\n",
    "    assert diff == config.trainBPTT\n",
    "\n",
    "batchTargShapes = [(batch.shape, target.shape , diff) for batch, target, diff in trainLoaderIter]\n",
    "\n",
    "# Last one is the remainder sizes:\n",
    "print(batchTargShapes[:5], \"\\n...\\n\", batchTargShapes[3520:3522]) # note: last one is different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "allBatches: List[Tensor] = [b for b,_,_ in trainLoaderIter]\n",
    "allTargets: List[Tensor] = [t for _,t,_ in trainLoaderIter]\n",
    "allDiffs: List[Tensor] = [d for _,_,d in trainLoaderIter]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Visualizing the selected BPTT-length columns from selected batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "(BATCH_TUPLE_POS, TARG_TUPLE_POS) = (0, 1)\n",
    "\n",
    "ITER: int = 0\n",
    "COL_ID: int = 0\n",
    "\n",
    "assert (allBatches[ITER] == trainLoaderIter[ITER][BATCH_TUPLE_POS]).all()\n",
    "\n",
    "print(f\"Column ID = {COL_ID} of batch = {ITER} in allBatches: \\n\\n {getBPTTCols(COL_ID, allBatches)[ITER]}\\n\\n\")\n",
    "\n",
    "print(f\"Batch ID = {ITER}:\\n\\n {allBatches[ITER]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "(BATCH_TUPLE_POS, TARG_TUPLE_POS) = (0, 1)\n",
    "\n",
    "ITER: int = 1\n",
    "COL_ID: int = 0\n",
    "\n",
    "assert (allBatches[ITER] == trainLoaderIter[ITER][BATCH_TUPLE_POS]).all()\n",
    "\n",
    "print(f\"Column ID = {COL_ID} of batch = {ITER} in allBatches: \\n\\n {getBPTTCols(COL_ID, allBatches)[ITER]}\\n\\n\")\n",
    "\n",
    "print(f\"Batch ID = {ITER}:\\n\\n {allBatches[ITER]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "(BATCH_TUPLE_POS, TARG_TUPLE_POS) = (0, 1)\n",
    "\n",
    "ITER: int = 2\n",
    "COL_ID: int = 3\n",
    "\n",
    "assert (allBatches[ITER] == trainLoaderIter[ITER][BATCH_TUPLE_POS]).all()\n",
    "\n",
    "print(f\"Column ID = {COL_ID} of batch = {ITER} in allBatches: \\n\\n {getBPTTCols(COL_ID, allBatches)[ITER]}\\n\\n\")\n",
    "\n",
    "print(f\"Batch ID = {ITER}:\\n\\n {allBatches[ITER]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "(BATCH_TUPLE_POS, TARG_TUPLE_POS) = (0, 1)\n",
    "\n",
    "ITER: int = 4\n",
    "COL_ID: int = 5\n",
    "\n",
    "assert (allBatches[ITER] == trainLoaderIter[ITER][BATCH_TUPLE_POS]).all()\n",
    "\n",
    "print(f\"Column ID = {COL_ID} of batch = {ITER} in allBatches: \\n\\n {getBPTTCols(COL_ID, allBatches)[ITER]}\\n\\n\")\n",
    "\n",
    "print(f\"Batch ID = {ITER}:\\n\\n {allBatches[ITER]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "(BATCH_TUPLE_POS, TARG_TUPLE_POS) = (0, 1)\n",
    "\n",
    "ITER: int = 10\n",
    "COL_ID: int = config.batchSize - 1\n",
    "\n",
    "assert (allBatches[ITER] == trainLoaderIter[ITER][BATCH_TUPLE_POS]).all()\n",
    "\n",
    "print(f\"Column ID = {COL_ID} of batch = {ITER} in allBatches: \\n\\n {getBPTTCols(COL_ID, allBatches)[ITER]}\\n\\n\")\n",
    "\n",
    "print(f\"Batch ID = {ITER}:\\n\\n {allBatches[ITER]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "(BATCH_TUPLE_POS, TARG_TUPLE_POS) = (0, 1)\n",
    "\n",
    "ITER: int = 2\n",
    "COL_ID: int = 3\n",
    "\n",
    "assert (allTargets[ITER] == trainLoaderIter[ITER][TARG_TUPLE_POS]).all()\n",
    "\n",
    "print(f\"Column ID = {COL_ID} of target = {ITER} in allTargets: \\n\\n {getBPTTCols(COL_ID, allTargets)[ITER]}\\n\\n\")\n",
    "\n",
    "print(f\"Batch ID = {ITER}:\\n\\n {allTargets[ITER]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "### Train Step 6: Initialization\n",
    "Initializing the weights and biases, [borrowing the implementation from the Transformer XL repo](https://github.com/kimiyoung/transformer-xl/blob/81b1b1955b5729b311e1548998eb2a89cb528178/pytorch/train.py#L207-L256):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "def initWeight(weight: Tensor) -> Tensor:\n",
    "    # TODO shape of this tensor??\n",
    "    # Sets a value INSIDE the given argument, and also RETURNS its value at the same time\n",
    "    # so this is not good data mutability principle!!!! (bad python)\n",
    "    nn.init.normal_(tensor = weight, mean = 0.0, std = 0.02)\n",
    "\n",
    "\n",
    "def initBias(bias: Tensor) -> Tensor:\n",
    "    # Fills tensor `bias` with value `val`\n",
    "    nn.init.constant_(tensor = bias, val = 0.0)\n",
    "\n",
    "def moduleWeightsInit(module: nn.Module) -> Tensor:\n",
    "    classname: str = module.__class__.__name__\n",
    "\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "            initWeight(module.weight)\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            initBias(module.bias)\n",
    "\n",
    "    elif classname.find('Embedding') != -1:\n",
    "        if hasattr(module, 'weight'):\n",
    "            initWeight(module.weight)\n",
    "\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        if hasattr(module, 'weight'):\n",
    "        # Fill the argument tensor with normal random values with mu = 1, sigma = 0.02\n",
    "            nn.init.normal_(tensor = module.weight, mean = 1.0, std = 0.02)\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            initBias(module.bias)\n",
    "\n",
    "    else:\n",
    "        if hasattr(module, 'u'):\n",
    "            initWeight(module.u)\n",
    "        if hasattr(module, 'v'):\n",
    "            initWeight(module.v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "### Train Step 7: Training Loop\n",
    "Training loop is standard, going to write our own to simplify things, but could use ignite, allennlp, fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "import torch.optim as optim #\n",
    "# from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import time\n",
    "#import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.ModelStudy.TransformerXL.TransformerXL import  TransformerXL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Language models are usually evaluated by perplexity.\n",
    "\n",
    "**Definition: Perplexity:**\n",
    "\n",
    "Perplexity is the exponential of the cross entropy loss,\n",
    "and is also equivalent to the reciprocal of the likelihood. If the language model assigns a probability of $0.1$ to\n",
    "each word in the input sentence on average, it would receive a perplexity of $100$.\n",
    "\n",
    "Intuitively, perplexity represents how many tries it would take for the model to guess the correct word. A\n",
    "perplexity of $100$ signifies the model would need $100$ tries to guess each word in the input sequence correctly.\n",
    "\n",
    "Then the evaluation code becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model: TransformerXL, validLoader: DataLoader) -> float:\n",
    "\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "\n",
    "    model.resetLength(seqLen = config.evalBPTT,\n",
    "                      extLen = 0,\n",
    "                      memoryLen=config.evalMemoryLen + config.trainBPTT - config.evalBPTT)\n",
    "\n",
    "    # Evaluation\n",
    "    totalLen, totalLoss = 0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        memories: List[Tensor] = None\n",
    "\n",
    "        for i, (data, target, seqLen) in enumerate(validLoader):\n",
    "            outDict: Dict[str, Tensor] = model(data, target, memory = memories) # TODO type\n",
    "\n",
    "            resultLoss: Tensor = outDict[\"loss\"] # tensor of single number\n",
    "            resultMemories: List[Tensor] = outDict[\"memory\"] # list of tensor memories\n",
    "\n",
    "            totalLoss += seqLen * resultLoss.float().item()\n",
    "            totalLen += seqLen\n",
    "\n",
    "    # Switch back to the training mode (to maintain state since this evaluate() function is used in the training loop\n",
    "    # so we need to set the state back to training state)\n",
    "    model.resetLength(seqLen = config.trainBPTT,\n",
    "                      extLen = 0,\n",
    "                      memoryLen = config.memoryLen)\n",
    "    model.train()\n",
    "\n",
    "    return totalLoss / totalLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "\n",
    "trainLossChange: List[Tensor] = []\n",
    "validLossChange = [] # TODO type\n",
    "\n",
    "\n",
    "def trainEpoch(numEpoch: int,\n",
    "               model: TransformerXL,\n",
    "               trainLoader: DataLoader, validLoader: DataLoader,\n",
    "               optimizer: optim.Optimizer,\n",
    "               scheduler,\n",
    "               trainStepStart: float = 0.):\n",
    "\n",
    "    # Turn on training mode which enables dropout\n",
    "    model.train()\n",
    "    memories: List[Tensor] = None\n",
    "    trainStep: float = trainStepStart\n",
    "    trainLoss: float = 0\n",
    "\n",
    "    # Time-book-keeping\n",
    "    loggerStartTime: float = time.time()\n",
    "\n",
    "    bestValidationLoss: float = float('inf')\n",
    "\n",
    "    progressBar = tqdm(trainLoader,\n",
    "                       total = min(config.maxStep - trainStepStart, len(trainLoader)))\n",
    "\n",
    "\n",
    "    for iBatch, (data, target, seqLen) in enumerate(progressBar):\n",
    "        model.zero_grad()\n",
    "\n",
    "        outDict: Dict[str, Tensor] = model(data, target, memory = memories)\n",
    "        resultLoss: Tensor = outDict[\"loss\"] # tensor of single number\n",
    "        resultMemories: List[Tensor] = outDict[\"memory\"] # list of tensor memories\n",
    "\n",
    "        resultLoss.backward()\n",
    "        trainLoss += resultLoss.item()\n",
    "        trainLossChange.append(resultLoss.item())\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(parameters = model.parameters(),\n",
    "                                       max_norm = config.clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step-wise learning rate annlealing\n",
    "        trainStep += 1\n",
    "\n",
    "        # Linear warm up stage\n",
    "        if trainStep < config.warmupStep:\n",
    "            currLearnRate = config.learningRate * trainStep / config.warmupStep\n",
    "            optimizer.param_groups[0]['lr'] = currLearnRate\n",
    "\n",
    "        else:\n",
    "            scheduler.step(trainStep)\n",
    "\n",
    "\n",
    "        # LOGGING updates\n",
    "        if trainStep % config.logInterval == 0:\n",
    "            currLoss: float = trainLoss / config.logInterval\n",
    "\n",
    "            elapsedTime: float = time.time() - loggerStartTime\n",
    "\n",
    "            loggerTimeStr: str = '| epoch {:3d} step {:>8d} | lr {:.3g} ' \\\n",
    "                                 '| loss {:5.2f}'.format(\n",
    "                numEpoch, trainStep, optimizer.param_groups[0]['lr'], currLoss)\n",
    "\n",
    "            loggerTimeStr += ' | PPL (perplexity) {:9.3f}'.format(math.exp(currLoss))\n",
    "\n",
    "            progressBar.set_description(desc = loggerTimeStr)\n",
    "\n",
    "            trainLoss = 0 # reset the training loss after reporting it\n",
    "\n",
    "            loggerStartTime = time.time() # start again for this point on\n",
    "\n",
    "        # EVALUATION updates\n",
    "        if trainStep % config.evalInterval == 0:\n",
    "            validLoss: float = evaluate(model, validLoader)\n",
    "            validLossChange.append(validLoss)\n",
    "\n",
    "            evaluationStartTime = time.time()\n",
    "\n",
    "        if trainStep == config.maxStep:\n",
    "            return trainStep\n",
    "\n",
    "    return trainStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO argument and return types ...\n",
    "def train(model: TransformerXL, trainLoader: DataLoader, validLoader: DataLoader):\n",
    "\n",
    "    optimizer: optim.Optimizer = optim.Adam(params = model.parameters(),\n",
    "                                            lr = config.learningRate)\n",
    "\n",
    "    numTotalSteps: int = min(config.maxStep, len(trainLoader) * config.numEpochs)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer,\n",
    "                                                     T_max = numTotalSteps,\n",
    "                                                     eta_min = config.minLearnRate)\n",
    "    trainStepStart: int = 0\n",
    "\n",
    "    for numEpoch in range(config.numEpochs):\n",
    "        if trainStepStart >= config.maxStep:\n",
    "            break\n",
    "\n",
    "        trainStepStart = trainEpoch(numEpoch = numEpoch,\n",
    "                                    model = model ,\n",
    "                                    trainLoader = trainLoader,\n",
    "                                    validLoader = validLoader,\n",
    "                                    optimizer = optimizer,\n",
    "                                    scheduler = scheduler,\n",
    "                                    trainStepStart = trainStepStart)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "def evaluateFinal(model: TransformerXL,\n",
    "                  validLoader: DataLoader) -> Dict[str, float]:\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    model.resetLength(seqLen = config.evalBPTT,\n",
    "                      extLen = 0,\n",
    "                      memoryLen=config.evalMemoryLen + config.trainBPTT - config.evalBPTT)\n",
    "\n",
    "    # Evaluation\n",
    "    totalLen, totalLoss = 0, 0.0\n",
    "\n",
    "    evalStartTime: float = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        memories: List[Tensor] = None\n",
    "\n",
    "        for i, (data, target, seqLen) in enumerate(validLoader):\n",
    "            outDict: Dict[str, Tensor] = model(data, target, memory = memories)\n",
    "\n",
    "            resultLoss: Tensor = outDict[\"loss\"] # tensor of single number\n",
    "            resultMemories: List[Tensor] = outDict[\"memory\"] # list of tensor memories\n",
    "\n",
    "            totalLoss += seqLen * resultLoss.item() # item inside tensor\n",
    "            totalLen += seqLen\n",
    "\n",
    "        elapsedTime = time.time() - evalStartTime\n",
    "\n",
    "    # TODO setting back to train mode? if so then don't we need `model.train()` like in evaluate()???\n",
    "    model.resetLength(seqLen = config.trainBPTT,\n",
    "                      extLen = 0,\n",
    "                      memoryLen = config.memoryLen)\n",
    "\n",
    "    validLoss: float = totalLoss / totalLen\n",
    "\n",
    "    return {\"validationLoss\": validLoss, \"PPL\": math.exp(validLoss)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "title": "markdown"
   },
   "source": [
    "### Train Step 8: Train the Model!\n",
    "Now all we have to do is initialize the model and start training it - actually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "transformerXLToTrain: TransformerXL = TransformerXL(\n",
    "    numEmbeddings = len(vocab),\n",
    "    numLayers = config.numLayers,\n",
    "    numHeads = config.numHeads,\n",
    "    modelDim = config.modelDim,  # E\n",
    "    mhaInnerDim = config.mhaInnerDim, # I\n",
    "    ffInnerDim = config.ffInnerDim,  # F\n",
    "    dropoutO = config.dropoutO,\n",
    "    dropoutA = config.dropoutA,\n",
    "    seqLen = config.trainBPTT, # S\n",
    "    memoryLen = config.memoryLen # M\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    transformerXLToTrain.cuda()\n",
    "\n",
    "transformerXLToTrain.apply(fn = moduleWeightsInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "briefParams(transformerXLToTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "train(model = transformerXLToTrain,\n",
    "      trainLoader = trainIter,\n",
    "      validLoader = validIter\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Now evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "resultDict: Dict[str, float] = evaluateFinal(model = transformerXLToTrain, validLoader = validIter)\n",
    "resultDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "### Visualizing: Loss Change\n",
    "Overall the loss is decreasing - both the `lossChange` and `validLossChange`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(trainLossChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "plt.plot(validLossChange)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
