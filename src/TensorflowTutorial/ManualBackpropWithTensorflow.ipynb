{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "source": [
    "https://towardsdatascience.com/manual-back-prop-with-tensorflow-decoupled-recurrent-neural-network-modified-nn-from-google-f9c085fe8fae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, sys\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [],
   "source": [
    "np.random.seed(678)\n",
    "tf.set_random_seed(678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): # sigmoid\n",
    "    return tf.div(tf.constant(1.0), tf.add(tf.constant(1.0), tf.exp(tf.negative(x)) ) )\n",
    "\n",
    "def derivativeSigmoid(x):\n",
    "    return tf.multiply(sigmoid(x), tf.subtract(tf.constant(1.0), sigmoid(x)))\n",
    "\n",
    "def tanh(x):\n",
    "    return tf.tanh(x)\n",
    "\n",
    "def derivativeTanh(x):\n",
    "    return tf.subtract(tf.constant(1.0), tf.square(tf.tanh(x)))\n",
    "\n",
    "def arctan(x):\n",
    "    return tf.atan(x)\n",
    "def derivativeArctan(x):\n",
    "    return tf.div(tf.constant(1.0), tf.subtract(tf.constant(1.0), tf.square(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: Declare Training Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnistData = input_data.read_data_sets(\"data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = mnistData.test\n",
    "images, labels = train.images, train.labels\n",
    "onlyZeroIndex, onlyOneIndex = np.where(labels == 0)[0], np.where(labels == 1)[0]\n",
    "onlyZeroImage, onlyZeroLabel = images[onlyZeroIndex], np.expand_dims(labels[onlyZeroIndex], axis = 1)\n",
    "onlyOneImage, onlyOneLabel = images[onlyOneIndex], np.expand_dims(labels[onlyOneIndex], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "print(type(np.where(labels==0)))\n",
    "print(type(np.where(labels == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onlyZeroIndex:  (1135,)  |  <class 'numpy.ndarray'>\nlabels[onlyZeroIndex]:  (980,) | <class 'numpy.ndarray'>\n\nimages:  (10000, 784) | <class 'numpy.ndarray'>\nonlyZeroImage:  (980, 784) | <class 'numpy.ndarray'>\nonlyZeroLabel:  (980, 1) | <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "\n",
    "# onlyZeroImage, onlyZeroLabel = images[onlyZeroIndex], np.expand_dims(labels[onlyZeroIndex], axis = 1)\n",
    "print(\"onlyZeroIndex: \", onlyOneIndex.shape, \" | \", type(onlyZeroIndex))\n",
    "print(\"labels[onlyZeroIndex]: \", labels[onlyZeroIndex].shape, \"|\", type(labels[onlyZeroIndex]))\n",
    "print()\n",
    "print(\"images: \", images.shape, \"|\", type(images))\n",
    "print(\"onlyZeroImage: \", onlyZeroImage.shape, \"|\", type(onlyZeroImage))\n",
    "print(\"onlyZeroLabel: \", onlyZeroLabel.shape, \"|\", type(onlyZeroLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onlyOneIndex:  (1135,)  |  <class 'numpy.ndarray'>\nlabels[onlyOneIndex]:  (1135,) | <class 'numpy.ndarray'>\n\nimages:  (10000, 784) | <class 'numpy.ndarray'>\nonlyOneImage:  (1135, 784) | <class 'numpy.ndarray'>\nonlyOneLabel:  (1135, 1) | <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "\n",
    "# onlyOneImage, onlyOneLabel = images[onlyOneIndex], np.expand_dims(labels[onlyOneIndex], axis=1)\n",
    "print(\"onlyOneIndex: \", onlyOneIndex.shape, \" | \", type(onlyOneIndex))\n",
    "print(\"labels[onlyOneIndex]: \", labels[onlyOneIndex].shape, \"|\", type(labels[onlyOneIndex]))\n",
    "print()\n",
    "print(\"images: \", images.shape, \"|\", type(images))\n",
    "print(\"onlyOneImage: \", onlyOneImage.shape, \"|\", type(onlyOneImage))\n",
    "print(\"onlyOneLabel: \", onlyOneLabel.shape, \"|\", type(onlyOneLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape, labels.shape =  (2115, 784) (2115, 1)\nimages.shape, labels.shape =  (2115, 784) (2115, 1)\n"
     ]
    }
   ],
   "source": [
    "images = np.vstack((onlyZeroImage, onlyOneImage)) # stacking arrays as rows vertically (rows on top of each other)\n",
    "labels = np.vstack((onlyZeroLabel, onlyOneLabel))\n",
    "print(\"images.shape, labels.shape = \", images.shape, labels.shape)\n",
    "\n",
    "images, labels = shuffle(images, labels) # shuffles the rows among each array: images and labels, but\n",
    "# the objects themselves are kept separate so images remains images, and labels remains labels.\n",
    "print(\"images.shape, labels.shape = \", images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images:  (2115, 784)\nlabels:  (2115, 1)\na.shape =  (4, 3)\n[[8 1 1]\n [4 5 6]\n [2 6 4]\n [1 2 3]]\n\n[[10 12 13]\n [ 1  1  0]\n [ 2  4  2]\n [ 7  8  9]]\na.shape =  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "print(\"images: \", images.shape)\n",
    "print(\"labels: \", labels.shape)\n",
    "\n",
    "# testing shuffle with vstack \n",
    "a = np.vstack(([1,2,3], [4,5,6], [2,6,4], [8,1,1]))\n",
    "a\n",
    "b = np.vstack(([7,8,9],[1,1,0], [2,4,2], [10,12,13]))\n",
    "b\n",
    "print(\"a.shape = \", a.shape)\n",
    "a, b = shuffle(a, b) # returns shuffled indices and sets values of a and b by the shuffled indices. \n",
    "# so both a and b are shuffled in the same order. \n",
    "print(a)\n",
    "print()\n",
    "print(b)\n",
    "print(\"a.shape = \", a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImageNum, trainingImageNum = 20, 100\n",
    "testImages, testingLabels = images[:testImageNum, :], labels[:testImageNum]\n",
    "trainingImages, trainingLabels = images[testImageNum : testImageNum + trainingImageNum , :], \\\n",
    "                                 labels[testImageNum : testImageNum + trainingImageNum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testImages.shape:  (20, 784)\ntestingLabels.shape:  (20, 1)\n\ntrainingImages.shape:  (100, 784)\ntrainingLabels.shape:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "print(\"testImages.shape: \", testImages.shape)\n",
    "print(\"testingLabels.shape: \", testingLabels.shape)\n",
    "print()\n",
    "print(\"trainingImages.shape: \", trainingImages.shape)\n",
    "print(\"trainingLabels.shape: \", trainingLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpoch = 100\n",
    "totalCost = 0\n",
    "costArray = []\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x7f8a9c23f198>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STUDY MODE\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What weights do I need? And how to initialize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    learningRate_x = tf.Variable(tf.constant(0.001))\n",
    "    learningRate_rec = tf.Variable(tf.constant(0.000001))\n",
    "    learningRate_sg = tf.Variable(tf.constant(0.0001))\n",
    "    \n",
    "    hiddenStates = tf.Variable(tf.random_normal([784, 3]))\n",
    "    \n",
    "    W_x = tf.Variable(tf.random_normal([784, 784], stddev=0.45) * tf.constant(0.2))\n",
    "    W_rec = tf.Variable(tf.random_normal([784, 784], stddev=0.035) * tf.constant(0.2))\n",
    "    W_fc = tf.Variable(tf.random_normal([784, 1], stddev=0.95) * tf.constant(0.2))\n",
    "    \n",
    "    W_sg_1 = tf.Variable(tf.random_normal([784, 784], stddev=0.35) * tf.constant(0.2))\n",
    "    W_sg_2 = tf.Variable(tf.random_normal([784, 784], stddev=0.35) * tf.constant(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(hiddenStates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 1])\n",
    "    update = []\n",
    "    hiddenLayerUpdate = []\n",
    "    \n",
    "    layer1 = tf.add(tf.matmul(x, W_x), \n",
    "                    tf.matmul(tf.expand_dims(hiddenStates[:,0],axis=0), W_rec))\n",
    "    layer1A = tanh(layer1)\n",
    "    hiddenLayerUpdate.append(tf.assign(hiddenStates[:,1], tf.squeeze(layer1A)))\n",
    "\n",
    "    # # ----- Time Stamp 1 Syn Grad Update --------------------------------------------\n",
    "    grad_1sg_part_1 = tf.matmul(layer1A, W_sg_1)\n",
    "    grad_1sg_part_2 = derivativeTanh(layer1)\n",
    "    grad_1sg_part_rec = tf.expand_dims(hiddenStates[:,0], axis=0)\n",
    "    grad_1sg_part_x = x\n",
    "    \n",
    "    grad_1sg_rec = tf.matmul(tf.transpose(grad_1sg_part_rec), \n",
    "                             tf.multiply(grad_1sg_part_1, grad_1sg_part_2))\n",
    "    grad_1sg_x = tf.matmul(tf.transpose(grad_1sg_part_x), \n",
    "                           tf.multiply(grad_1sg_part_1, grad_1sg_part_2))\n",
    "    \n",
    "    update.append(tf.assign(W_rec, tf.add(W_rec, tf.multiply(learningRate_rec, grad_1sg_rec))))\n",
    "    update.append(tf.assign(W_x, tf.add(W_x, tf.multiply(learningRate_rec, grad_1sg_x))))\n",
    "    \n",
    "    grad_true_0 = tf.matmul(tf.multiply(grad_1sg_part_1, grad_1sg_part_2), \n",
    "                            tf.transpose(W_rec))\n",
    "    # end of time stamp 1 --------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    layer2 = tf.add(tf.matmul(x, W_x), tf.matmul(tf.expand_dims(hiddenStates[:,1],axis=0), W_rec))\n",
    "    layer2A = tanh(layer2)\n",
    "    hiddenLayerUpdate.append(tf.assign(hiddenStates[:,2], tf.squeeze(layer2A)))\n",
    "\n",
    "\n",
    "    # # ----- Time Stamp 2 Syn Grad Update ----------------------------------------------\n",
    "    grad_2sg_part_1 = tf.matmul(layer2A, W_sg_2)\n",
    "    grad_2sg_part_2 = derivativeTanh(layer2)\n",
    "    grad_2sg_part_rec = tf.expand_dims(hiddenStates[:,1],axis=0)\n",
    "    grad_2sg_part_x = x \n",
    "    \n",
    "    grad_2sg_rec = tf.matmul(tf.transpose(grad_2sg_part_rec), \n",
    "                             tf.multiply(grad_2sg_part_1, grad_2sg_part_2))\n",
    "\n",
    "    grad_2sg_x = tf.matmul(tf.transpose(grad_2sg_part_x),\n",
    "                         tf.multiply(grad_2sg_part_1, grad_2sg_part_2))\n",
    "    \n",
    "    update.append(tf.assign(W_rec, tf.add(W_rec, tf.multiply(learningRate_rec, grad_2sg_rec))))\n",
    "    update.append(tf.assign(W_x, tf.add(W_x, tf.multiply(learningRate_rec, grad_2sg_x))))\n",
    "    # HELP: shouldn't the xlayer have learningRate_x not learningRate_rec? Same for\n",
    "    # previous time stamp?\n",
    "    \n",
    "    grad_true_1_from_2 = tf.matmul(tf.multiply(grad_2sg_part_1, grad_2sg_part_2), \n",
    "                                   tf.transpose(W_rec))\n",
    "    # end of time stamp 2 --------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # # ----- Time Stamp 1 True Gradient Update ----------------------------------------\n",
    "    grad_true_1_part_1 = tf.subtract(grad_1sg_part_1, grad_true_1_from_2)\n",
    "    grad_true_1_part_2 = tf.expand_dims(hiddenStates[:,1],axis=0)\n",
    "    grad_true_1 = tf.matmul(tf.transpose(grad_true_1_part_2), grad_true_1_part_1)\n",
    "    update.append(tf.assign(W_sg_1, \n",
    "                            tf.subtract(W_sg_1, tf.multiply(learningRate_sg, grad_true_1))))\n",
    "    # end of true time stamp 1 ---------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    # # ----- Fully Connected for Classification ------\n",
    "    layer3 = tf.matmul(tf.expand_dims(hiddenStates[:,2], axis=0), W_fc)\n",
    "    layer3A = sigmoid(layer3)\n",
    "    # -------------------------------------------------\n",
    "\n",
    "    # # -- MAN BACK PROP --------------------------------\n",
    "    costFunction = tf.multiply(tf.square(tf.subtract(layer3A, y)), tf.constant(0.5))\n",
    "    # ---------------------------------------------------\n",
    "    \n",
    "    # # -- AUTO BACK PROP ------------------------------\n",
    "    costFunctionAuto = tf.train.GradientDescentOptimizer(0.1).minimize(costFunction)\n",
    "    # ---------------------------------------------------\n",
    "\n",
    "    # # ------- FC weight update ---------------------\n",
    "    grad_fc_part_1 = tf.subtract(layer3A, y)\n",
    "    grad_fc_part_2 = derivativeSigmoid(layer3)\n",
    "    grad_fc_part_3 = tf.expand_dims(hiddenStates[:,2], axis=0)\n",
    "    grad_fc = tf.matmul(tf.transpose(grad_fc_part_3), \n",
    "                        tf.multiply(grad_fc_part_1, grad_fc_part_2))\n",
    "    update.append(tf.assign(W_fc, tf.subtract(W_fc, tf.multiply(learningRate_x, grad_fc))))\n",
    "    \n",
    "    grad_true_2_from_3 = tf.matmul(tf.multiply(grad_fc_part_1, grad_fc_part_2), tf.transpose(W_fc))\n",
    "    # end FC weight update ---------------------------\n",
    "\n",
    "    # # ----- Time Stamp 2 True Gradient Update -------------------------------------------\n",
    "    grad_true_2_part_1 = tf.subtract(grad_2sg_part_1, grad_true_2_from_3)\n",
    "    grad_true_2_part_2 = tf.expand_dims(hiddenStates[:,2], axis=0)\n",
    "    grad_true_2 = tf.matmul(tf.transpose(grad_true_2_part_2), grad_true_2_part_1)\n",
    "    update.append(tf.assign(W_sg_2, tf.subtract(W_sg_2, tf.multiply(learningRate_sg, grad_true_2))))\n",
    "    # end time stamp 2 true update ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    totalCost = 0\n",
    "    \n",
    "    for i in range(numEpoch):\n",
    "        for currentImageIndex in range(len(trainingImages)):\n",
    "            \n",
    "            currentImage = np.expand_dims(trainingImages[currentImageIndex], axis=0)\n",
    "            currentIndex = np.expand_dims(trainingLabels[currentImageIndex], axis=0)\n",
    "            \n",
    "            # if you want to do manual backprop, run this line\n",
    "            output = sess.run([costFunction, update, hiddenLayerUpdate],\n",
    "                              feed_dict={x:currentImage, y:currentIndex})\n",
    "            \n",
    "            # if you want to do auto differential uncomment this line\n",
    "            #output = sess.run([costFunction, costFunctionAuto, hiddenLayerUpdate],\n",
    "            #                  feed_dict={x:currentImage, y:currentIndex})\n",
    "            \n",
    "            totalCost = totalCost + output[0].sum()\n",
    "            \n",
    "        print(\"Current iteration: \", i, \" current cost: \", totalCost)\n",
    "        costArray.append(totalCost)\n",
    "        totalCost = 0 \n",
    "    \n",
    "    plt.plot(np.arange(numEpoch), costArray)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    for currentImageIndex in range(len(testingImages))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}