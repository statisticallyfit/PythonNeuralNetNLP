{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: http://seba1511.net/tutorials/beginner/pytorch_with_examples.html#annotations:E9HdvPynEemYwidYvwe30g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using autograd, the forward pass of your \n",
    "# network will define a computational graph; \n",
    "# nodes in the graph will be Tensors, and edges \n",
    "# will be functions that produce output Tensors \n",
    "# from input Tensors. Backpropagating through this \n",
    "#  graph then allows you to easily compute gradients.\n",
    "\n",
    "# We wrap our PyTorch Tensors in Variable objects; \n",
    "# a Variable represents a node in a computational graph.\n",
    "# If x is a Variable then x.data is a Tensor, \n",
    "# and x.grad is another Variable holding the gradient \n",
    "# of x with respect to some scalar value.\n",
    "\n",
    "# PyTorch Variables have the same API as PyTorch \n",
    "# Tensors: (almost) any operation that you can perform \n",
    "# on a Tensor also works on Variables; the difference \n",
    "# is that using Variables defines a computational graph,\n",
    "# allowing you to automatically compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "dtype\n",
    "# dtype = torch.cuda.FloatTensor # runs on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = batch size\n",
    "# D_int = input dimension\n",
    "# H = hidden dimension\n",
    "# D_out = output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2877,  1.7476, -0.9897,  ...,  0.9788,  1.8288, -2.5328],\n        [-0.2216, -1.1619, -0.3167,  ..., -1.2521, -0.1662,  1.4233],\n        [-0.8671,  0.2041,  0.8794,  ...,  0.0019,  0.3129, -0.2765],\n        ...,\n        [-1.1136, -0.3622,  0.0525,  ...,  0.0321, -0.9496, -0.5120],\n        [ 0.4586, -0.4575, -0.1369,  ...,  0.2476, -0.3643,  0.0021],\n        [-1.6410,  1.4266,  0.2169,  ..., -0.3201, -0.3449, -0.5649]])\n"
     ]
    }
   ],
   "source": [
    "# Create random Tensors to hold input and outputs, and wrap them in \n",
    "# Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute \n",
    "# gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "X = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "print(X)\n",
    "Y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients \n",
    "# with respect to these Variables during the backward pass.\n",
    "W1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "W2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 100])\n2\ntensor([[-0.6212, -0.9507,  0.1739,  ..., -0.2685, -0.2253,  0.1302],\n        [ 0.3207,  1.5027, -0.9396,  ...,  0.5319,  1.3009, -0.2934],\n        [ 0.2509,  0.5461, -0.9063,  ..., -2.0845,  0.6393,  1.6020],\n        ...,\n        [-1.6332,  0.0274,  0.2031,  ..., -1.8663,  0.7127, -1.5207],\n        [ 0.7908,  0.4172, -1.3753,  ..., -1.0964, -0.3508,  0.7271],\n        [-0.0022,  0.1598,  0.1027,  ...,  0.3834,  1.9736, -1.7433]],\n       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(W1.size())\n",
    "print(W1.dim())\n",
    "\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n2\n"
     ]
    }
   ],
   "source": [
    "print(W2.size())\n",
    "print(W2.dim())\n",
    "\n",
    "#print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 0.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  0 ; loss =  tensor(1.7772, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  50 ; loss =  tensor(0.8522, grad_fn=<SumBackward0>)\niter =  100 ; loss =  tensor(0.4124, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  150 ; loss =  tensor(0.2012, grad_fn=<SumBackward0>)\niter =  200 ; loss =  tensor(0.0990, grad_fn=<SumBackward0>)\niter =  250 ; loss =  tensor(0.0492, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  300 ; loss =  tensor(0.0249, grad_fn=<SumBackward0>)\niter =  350 ; loss =  tensor(0.0129, grad_fn=<SumBackward0>)\niter =  400 ; loss =  tensor(0.0069, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  450 ; loss =  tensor(0.0039, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "learningRate = 1e-6\n",
    "NUM_ITER = 500\n",
    "\n",
    "for t in range(NUM_ITER):\n",
    "    # Forward pass: compute predicted y using operations on Variables; \n",
    "    # these  are exactly the same operations we used to compute the \n",
    "    # forward pass using Tensors, but we do not need to keep \n",
    "    # references to intermediate values since we are not implementing \n",
    "    # the backward pass by hand.\n",
    "    \n",
    "    h = X.mm(W1) # activation for hidden layer\n",
    "    hRELU = h.clamp(min = 0)\n",
    "    yPred = hRELU.mm(W2) # activation for output layer\n",
    "\n",
    "    # Compute and print loss using operations on Variables.\n",
    "    # Now loss is a Variable of shape (1,) and loss.data is a Tensor \n",
    "    # of shape (1,); loss.data[0] is a scalar value holding \n",
    "    # the loss.\n",
    "    loss = (yPred - Y).pow(2).sum()\n",
    "    \n",
    "    if t % 50 == 0:\n",
    "        print(\"iter = \", t, \"; loss = \", loss)\n",
    "\n",
    "    \n",
    "    #gradYPred = 2.0 * (yPred - Y)\n",
    "    #gradW2 = hRELU.t().mm(gradYPred)\n",
    "    #gradHiddenRELU = gradYPred.mm(W2.t())\n",
    "    #gradH = gradHiddenRELU.clone()\n",
    "    #gradH[h < 0] = 0\n",
    "    #gradW1 = X.t().mm(gradH)\n",
    "\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will \n",
    "    # compute the gradient of loss with respect to all Variables \n",
    "    # with requires_grad=True. After this call w1.grad and w2.grad \n",
    "    # will be Variables holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Learning rule: Update weights\n",
    "    # Update weights using gradient descent; w1.data and w2.data are \n",
    "    # Tensors, w1.grad and w2.grad are Variables and w1.grad.data \n",
    "    # and w2.grad.data are Tensors.\n",
    "    W1.data -= learningRate * W1.grad.data # gradW1\n",
    "    W2.data -= learningRate * W2.grad.data # gradW2\n",
    "    \n",
    "    # Necessary state-maintenance step: manually set the gradients to \n",
    "    # zero after updating weights (??)\n",
    "    W1.grad.data.zero_()\n",
    "    W2.grad.data.zero_()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
