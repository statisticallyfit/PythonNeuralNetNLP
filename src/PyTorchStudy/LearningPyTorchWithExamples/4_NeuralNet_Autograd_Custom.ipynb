{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: http://seba1511.net/tutorials/beginner/pytorch_with_examples.html#annotations:E9HdvPynEemYwidYvwe30g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the hood, each primitive autograd operator is really two \n",
    "# functions that operate on Tensors. The forward function computes \n",
    "# output Tensors from input Tensors. The backward function receives \n",
    "# the gradient of the output Tensors with respect to some scalar value, \n",
    "# and computes the gradient of the input Tensors with respect to that \n",
    "# same scalar value.\n",
    "# \n",
    "# In PyTorch we can easily define our own autograd operator by defining \n",
    "# a subclass of torch.autograd.Function and implementing the forward and \n",
    "# backward functions. We can then use our new autograd operator by \n",
    "# constructing an instance and calling it like a function, passing \n",
    "# Variables containing input data.\n",
    "# \n",
    "# In this example we define our own custom autograd function for \n",
    "# performing the ReLU nonlinearity, and use it to implement our \n",
    "# two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "     \"\"\"\n",
    "     We can implement our own custom autograd Functions by \n",
    "     subclassing torch.autograd.Function and implementing the forward\n",
    "     and backward passes which operate on Tensors.\n",
    "     \"\"\"\n",
    "     def forward(self, input):\n",
    "         \"\"\"\n",
    "         In the forward pass we receive a Tensor containing the input\n",
    "         and return a Tensor containing the output. \n",
    "         Can cache arbitrary Tensors to use in the backward pass\n",
    "         using the save_for_backward method.\n",
    "         :param input: \n",
    "         :return: \n",
    "         \"\"\"\n",
    "         self.save_for_backward(input)\n",
    "         return input.clamp(min = 0)\n",
    "     \n",
    "     def backward(self, gradOutput):\n",
    "         \"\"\"\n",
    "         In the backward pass we receive a Tensor containing the \n",
    "         gradient of the loss with respect to the output, and\n",
    "         we must compute the gradient of the loss with respect\n",
    "         to the input. \n",
    "         :param gradOutput: \n",
    "         :return: \n",
    "         \"\"\"\n",
    "         input, = self.saved_tensors \n",
    "         gradInput = gradOutput.clone()\n",
    "         gradInput[input < 0] = 0\n",
    "         return gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "dtype\n",
    "# dtype = torch.cuda.FloatTensor # runs on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = batch size\n",
    "# D_int = input dimension\n",
    "# H = hidden dimension\n",
    "# D_out = output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0347,  1.5838,  1.5527,  ...,  0.5372,  0.4005,  0.7786],\n        [-0.9178, -1.5622, -1.8196,  ..., -0.4261, -0.3000, -0.5551],\n        [-0.2520,  0.3164, -2.2183,  ..., -1.0488,  0.3053,  0.5154],\n        ...,\n        [-1.3053,  1.8415,  1.9728,  ..., -0.5269, -0.1232,  0.6168],\n        [-0.0996, -1.1780,  0.2153,  ..., -0.7121,  0.1249, -0.0278],\n        [ 1.3499, -0.5643, -0.1094,  ..., -0.0339, -1.7313, -1.7772]])\n"
     ]
    }
   ],
   "source": [
    "# Create random Tensors to hold input and outputs, and wrap them in \n",
    "# Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute \n",
    "# gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "X = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "print(X)\n",
    "Y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients \n",
    "# with respect to these Variables during the backward pass.\n",
    "W1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "W2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 100])\n2\ntensor([[ 0.0667, -0.1819, -3.5454,  ..., -0.4270,  3.2726,  0.2115],\n        [-0.1273,  0.5673,  2.0690,  ..., -0.6186,  2.3017, -0.6124],\n        [ 0.7222,  0.2129,  0.0300,  ...,  0.7340,  0.0220,  0.0967],\n        ...,\n        [-0.5892,  0.7849,  0.8009,  ...,  1.9636,  1.4814,  0.4510],\n        [ 1.4187, -0.5076,  0.8678,  ...,  0.0096, -0.3200,  1.1108],\n        [-0.1953, -1.2390, -0.9696,  ...,  1.1496, -1.4478,  0.2723]],\n       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(W1.size())\n",
    "print(W1.dim())\n",
    "\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n2\n"
     ]
    }
   ],
   "source": [
    "print(W2.size())\n",
    "print(W2.dim())\n",
    "\n",
    "#print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 0.0
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  0 ; loss =  tensor(33318572.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  50 ; loss =  tensor(13660.6172)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  100 ; loss =  tensor(359.0471)\niter =  150 ; loss =  tensor(15.0678)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  200 ; loss =  tensor(0.8159)\niter =  250 ; loss =  tensor(0.0520)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  300 ; loss =  tensor(0.0039)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  350 ; loss =  tensor(0.0005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  400 ; loss =  tensor(0.0001)\niter =  450 ; loss =  tensor(0.0000)\n"
     ]
    }
   ],
   "source": [
    "learningRate = 1e-6\n",
    "NUM_ITER = 500\n",
    "\n",
    "for t in range(NUM_ITER):\n",
    "    # Forward pass: compute predicted y using operations on Variables; \n",
    "    # these  are exactly the same operations we used to compute the \n",
    "    # forward pass using Tensors, but we do not need to keep \n",
    "    # references to intermediate values since we are not implementing \n",
    "    # the backward pass by hand.\n",
    "    # Compute ReLU using custom autograd operation\n",
    "    \n",
    "    #h = X.mm(W1) # activation for hidden layer\n",
    "    #hRELU = h.clamp(min = 0)\n",
    "    #yPred = hRELU.mm(W2) # activation for output layer\n",
    "    relu = MyReLU()\n",
    "    yPred = relu(X.mm(W1)).mm(W2)\n",
    "\n",
    "    # Compute and print loss using operations on Variables.\n",
    "    # Now loss is a Variable of shape (1,) and loss.data is a Tensor \n",
    "    # of shape (1,); loss.data[0] is a scalar value holding \n",
    "    # the loss.\n",
    "    loss = (yPred - Y).pow(2).sum()\n",
    "    \n",
    "    if t % 50 == 0:\n",
    "        print(\"iter = \", t, \"; loss = \", loss.data[0])\n",
    "\n",
    "    \n",
    "    #gradYPred = 2.0 * (yPred - Y)\n",
    "    #gradW2 = hRELU.t().mm(gradYPred)\n",
    "    #gradHiddenRELU = gradYPred.mm(W2.t())\n",
    "    #gradH = gradHiddenRELU.clone()\n",
    "    #gradH[h < 0] = 0\n",
    "    #gradW1 = X.t().mm(gradH)\n",
    "\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will \n",
    "    # compute the gradient of loss with respect to all Variables \n",
    "    # with requires_grad=True. After this call w1.grad and w2.grad \n",
    "    # will be Variables holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Learning rule: Update weights\n",
    "    # Update weights using gradient descent; w1.data and w2.data are \n",
    "    # Tensors, w1.grad and w2.grad are Variables and w1.grad.data \n",
    "    # and w2.grad.data are Tensors.\n",
    "    W1.data -= learningRate * W1.grad.data # gradW1\n",
    "    W2.data -= learningRate * W2.grad.data # gradW2\n",
    "    \n",
    "    # Necessary state-maintenance step: manually set the gradients to \n",
    "    # zero after updating weights (??)\n",
    "    W1.grad.data.zero_()\n",
    "    W2.grad.data.zero_()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
