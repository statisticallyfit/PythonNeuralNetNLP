{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: http://seba1511.net/tutorials/beginner/pytorch_with_examples.html#annotations:E9HdvPynEemYwidYvwe30g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch autograd looks a lot like TensorFlow: in both frameworks we \n",
    "# define a computational graph, and use automatic differentiation to \n",
    "# compute gradients. The biggest difference between the two is that \n",
    "# TensorFlowâ€™s computational graphs are static and PyTorch uses dynamic\n",
    "# computational graphs.\n",
    "# \n",
    "# In TensorFlow, we define the computational graph once and then execute \n",
    "# the same graph over and over again, possibly feeding different input \n",
    "# data to the graph. In PyTorch, each forward pass defines a new \n",
    "# computational graph.\n",
    "# \n",
    "# Static graphs are nice because you can optimize the graph up front; \n",
    "# for example a framework might decide to fuse some graph operations \n",
    "# for efficiency, or to come up with a strategy for distributing the \n",
    "# graph across many GPUs or many machines. If you are reusing the same \n",
    "# graph over and over, then this potentially costly up-front \n",
    "# optimization can be amortized as the same graph is rerun over and over.\n",
    "# \n",
    "# One aspect where static and dynamic graphs differ is control flow. \n",
    "# For some models we may wish to perform different computation for \n",
    "# each data point; for example a recurrent network might be unrolled \n",
    "# for different numbers of time steps for each data point; this \n",
    "# unrolling can be implemented as a loop. With a static graph the \n",
    "# loop construct needs to be a part of the graph; for this reason \n",
    "# TensorFlow provides operators such as tf.scan for embedding loops \n",
    "# into the graph. With dynamic graphs the situation is simpler: since \n",
    "# we build graphs on-the-fly for each example, we can use normal \n",
    "# imperative flow control to perform computation that differs for \n",
    "# each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = batch size\n",
    "# D_int = input dimension\n",
    "# H = hidden dimension\n",
    "# D_out = output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 1000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "X = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "Y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "W1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "W2 = tf.Variable(tf.random_normal((H, D_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n(100, 10)\n<tf.Variable 'Variable:0' shape=(1000, 100) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(W1.shape)\n",
    "print(W2.shape)\n",
    "\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 0.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  0 ; loss =  27540472.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  50 ; loss =  16195.615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  100 ; loss =  708.3488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  150 ; loss =  60.090553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  200 ; loss =  6.5851736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  250 ; loss =  0.80509347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  300 ; loss =  0.10387031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  350 ; loss =  0.01398183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  400 ; loss =  0.0021588965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  450 ; loss =  0.00048308386\n"
     ]
    }
   ],
   "source": [
    "# Forward pass: Compute the predicted Y using operations on Tensorflow\n",
    "# Tensors. \n",
    "# Note that this code does not perform any numeric operations: just sets up\n",
    "# the computational graph that we will later execute. \n",
    "h = tf.matmul(X, W1) #hidden layer activation\n",
    "hRELU = tf.maximum(h, tf.zeros(1))\n",
    "yPred = tf.matmul(hRELU, W2)\n",
    "\n",
    "# Compute loss using operations on tensorflow Tensors\n",
    "loss = tf.reduce_sum((Y - yPred) ** 2.0)\n",
    "\n",
    "# Compute gradient of loss with respect to W1 and W2 matrices\n",
    "gradW1, gradW2 = tf.gradients(loss, [W1, W2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learningRate = 1e-6\n",
    "\n",
    "newW1 = W1.assign(W1 - learningRate * gradW1)\n",
    "newW2 = W2.assign(W2 - learningRate * gradW2)\n",
    "\n",
    "# Now we have built the computational graph above. \n",
    "\n",
    "# Enter a tensorflow session to actually execute the graph.\n",
    "NUM_ITER = 500\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    # Run the graph once to initialize the Variables W1 and W2. \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create numpy arrays holding the actual data for the inputs\n",
    "    # X and the targets Y\n",
    "    Xvalue = np.random.randn(N, D_in)\n",
    "    Yvalue = np.random.randn(N, D_out)\n",
    "    \n",
    "    for t in range(NUM_ITER):\n",
    "        # Execute the graph many times. Each time it executes we want \n",
    "        # to bind  x_value to x and y_value to y, specified with the \n",
    "        # feed_dict argument. \n",
    "        # Each time we execute the graph we want to compute the values \n",
    "        # for loss, new_w1, and new_w2; the values of these Tensors \n",
    "        # are returned as numpy  arrays.\n",
    "        lossValue, _, _ = sess.run([loss, newW1, newW2],\n",
    "                                   feed_dict= {X: Xvalue, Y: Yvalue})\n",
    "        if t % 50 == 0:\n",
    "            print(\"iter = \", t, \"; loss = \", lossValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
