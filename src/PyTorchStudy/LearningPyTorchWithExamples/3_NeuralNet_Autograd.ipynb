{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: http://seba1511.net/tutorials/beginner/pytorch_with_examples.html#annotations:E9HdvPynEemYwidYvwe30g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using autograd, the forward pass of your \n",
    "# network will define a computational graph; \n",
    "# nodes in the graph will be Tensors, and edges \n",
    "# will be functions that produce output Tensors \n",
    "# from input Tensors. Backpropagating through this \n",
    "#  graph then allows you to easily compute gradients.\n",
    "\n",
    "# We wrap our PyTorch Tensors in Variable objects; \n",
    "# a Variable represents a node in a computational graph.\n",
    "# If x is a Variable then x.data is a Tensor, \n",
    "# and x.grad is another Variable holding the gradient \n",
    "# of x with respect to some scalar value.\n",
    "\n",
    "# PyTorch Variables have the same API as PyTorch \n",
    "# Tensors: (almost) any operation that you can perform \n",
    "# on a Tensor also works on Variables; the difference \n",
    "# is that using Variables defines a computational graph,\n",
    "# allowing you to automatically compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "dtype\n",
    "# dtype = torch.cuda.FloatTensor # runs on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = batch size\n",
    "# D_int = input dimension\n",
    "# H = hidden dimension\n",
    "# D_out = output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3766, -0.1936, -0.5011,  ..., -1.1732, -0.9163,  0.9096],\n        [-0.4446, -0.4733, -1.0679,  ...,  2.5234,  0.3103, -1.0592],\n        [ 0.9649, -0.6487, -0.7849,  ..., -0.0245, -0.6961, -0.1528],\n        ...,\n        [-1.1061,  0.2995, -0.1649,  ..., -0.8803, -0.0596, -0.6238],\n        [ 1.0835,  1.4910, -0.1038,  ..., -0.7648,  0.5591,  0.2665],\n        [-0.2231, -0.4785,  1.0952,  ..., -0.7110, -0.4988, -0.1576]])\n"
     ]
    }
   ],
   "source": [
    "# Create random Tensors to hold input and outputs, and wrap them in \n",
    "# Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute \n",
    "# gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "X = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "print(X)\n",
    "Y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients \n",
    "# with respect to these Variables during the backward pass.\n",
    "W1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "W2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 100])\n2\ntensor([[ 0.4023,  0.7879, -0.9253,  ...,  0.6129,  0.6854, -0.1731],\n        [ 0.4399, -1.3762, -0.7482,  ...,  0.5881, -0.9118, -0.7574],\n        [-0.3286, -0.6265,  1.7217,  ...,  0.4157,  1.0880, -0.1702],\n        ...,\n        [ 0.2141, -1.0338,  1.1188,  ...,  1.8133,  0.3519, -1.0181],\n        [-1.1927,  1.1318,  0.3350,  ..., -1.4112,  0.0026,  1.6291],\n        [-0.0387,  0.3654,  0.2027,  ..., -0.2694,  0.3746, -0.2575]],\n       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(W1.size())\n",
    "print(W1.dim())\n",
    "\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n2\n"
     ]
    }
   ],
   "source": [
    "print(W2.size())\n",
    "print(W2.dim())\n",
    "\n",
    "#print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 0.0
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  0 ; loss =  tensor(0.0001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  50 ; loss =  tensor(0.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  100 ; loss =  tensor(0.0000)\niter =  150 ; loss =  tensor(0.0000)\niter =  200 ; loss =  tensor(0.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  250 ; loss =  tensor(0.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  300 ; loss =  tensor(8.6453e-06)\niter =  350 ; loss =  tensor(7.3214e-06)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  400 ; loss =  tensor(6.2060e-06)\niter =  450 ; loss =  tensor(5.4699e-06)\n"
     ]
    }
   ],
   "source": [
    "learningRate = 1e-6\n",
    "NUM_ITER = 500\n",
    "\n",
    "for t in range(NUM_ITER):\n",
    "    # Forward pass: compute predicted y using operations on Variables; \n",
    "    # these  are exactly the same operations we used to compute the \n",
    "    # forward pass using Tensors, but we do not need to keep \n",
    "    # references to intermediate values since we are not implementing \n",
    "    # the backward pass by hand.\n",
    "    \n",
    "    h = X.mm(W1) # activation for hidden layer\n",
    "    hRELU = h.clamp(min = 0)\n",
    "    yPred = hRELU.mm(W2) # activation for output layer\n",
    "\n",
    "    # Compute and print loss using operations on Variables.\n",
    "    # Now loss is a Variable of shape (1,) and loss.data is a Tensor \n",
    "    # of shape (1,); loss.data[0] is a scalar value holding \n",
    "    # the loss.\n",
    "    loss = (yPred - Y).pow(2).sum()\n",
    "    \n",
    "    if t % 50 == 0:\n",
    "        print(\"iter = \", t, \"; loss = \", loss.data[0])\n",
    "\n",
    "    \n",
    "    #gradYPred = 2.0 * (yPred - Y)\n",
    "    #gradW2 = hRELU.t().mm(gradYPred)\n",
    "    #gradHiddenRELU = gradYPred.mm(W2.t())\n",
    "    #gradH = gradHiddenRELU.clone()\n",
    "    #gradH[h < 0] = 0\n",
    "    #gradW1 = X.t().mm(gradH)\n",
    "\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will \n",
    "    # compute the gradient of loss with respect to all Variables \n",
    "    # with requires_grad=True. After this call w1.grad and w2.grad \n",
    "    # will be Variables holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Learning rule: Update weights\n",
    "    # Update weights using gradient descent; w1.data and w2.data are \n",
    "    # Tensors, w1.grad and w2.grad are Variables and w1.grad.data \n",
    "    # and w2.grad.data are Tensors.\n",
    "    W1.data -= learningRate * W1.grad.data # gradW1\n",
    "    W2.data -= learningRate * W2.grad.data # gradW2\n",
    "    \n",
    "    # Necessary state-maintenance step: manually set the gradients to \n",
    "    # zero after updating weights (??)\n",
    "    W1.grad.data.zero_()\n",
    "    W2.grad.data.zero_()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
