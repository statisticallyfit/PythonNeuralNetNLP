{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCE: http://seba1511.net/tutorials/beginner/pytorch_with_examples.html#annotations:E9HdvPynEemYwidYvwe30g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational graphs and autograd are a very powerful paradigm for \n",
    "defining complex operators and automatically taking derivatives; \n",
    "# however for large neural networks raw autograd can be a bit too \n",
    "# low-level.\n",
    "\n",
    "# In PyTorch, the nn package serves this same purpose. The nn package \n",
    "# defines a set of Modules, which are roughly equivalent to neural \n",
    "# network layers. A Module receives input Variables and computes output \n",
    "# Variables, but may also hold internal state such as Variables \n",
    "# containing learnable parameters. The nn package also defines a set \n",
    "# of useful loss functions that are commonly used when training neural \n",
    "# networks.\n",
    "# \n",
    "# In this example we use the nn package to implement our two-layer \n",
    "# network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = batch size\n",
    "# D_int = input dimension\n",
    "# H = hidden dimension\n",
    "# D_out = output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1297,  1.1201, -0.0847,  ..., -1.4060,  1.0154, -0.9522],\n        [ 1.0397, -0.7903, -0.2978,  ...,  0.3343,  1.1755, -0.4516],\n        [ 0.0675, -0.6584,  1.0866,  ...,  0.4873,  0.0354, -0.0601],\n        ...,\n        [-1.0190, -0.8026, -0.3897,  ...,  0.5976,  1.5955,  0.9465],\n        [-1.0203,  1.7008,  2.2229,  ..., -1.0355,  0.2799,  1.1730],\n        [-0.8226,  1.1083, -0.1294,  ...,  2.6123, -0.5172, -0.7365]])\n"
     ]
    }
   ],
   "source": [
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "X = Variable(torch.randn(N, D_in))\n",
    "Y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n  (0): Linear(in_features=1000, out_features=100, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=100, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Use the nn package to define our model as a sequence of layers. \n",
    "# nn.Sequential is a Module which contains other Modules and applies\n",
    "# them in sequence to produce its output. \n",
    "# Each Linear Module computes output from input using a linear function, \n",
    "# and holds internal Variables for its weight and bias. \n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss()\n"
     ]
    }
   ],
   "source": [
    "# The nn package contains definitions of commonly used loss functions\n",
    "# In this case we use Mean Squared Error (MSE)\n",
    "lossFunction = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "print(lossFunction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 0.0
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  0 ; iter =  tensor(0.0001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  50 ; iter =  tensor(0.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  100 ; iter =  tensor(0.0000)\niter =  150 ; iter =  tensor(4.4593e-06)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  200 ; iter =  tensor(1.8420e-06)\niter =  250 ; iter =  tensor(7.6234e-07)\niter =  300 ; iter =  tensor(3.1716e-07)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  350 ; iter =  tensor(1.3307e-07)\niter =  400 ; iter =  tensor(5.6927e-08)\niter =  450 ; iter =  tensor(2.5378e-08)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learningRate = 1e-4\n",
    "NUM_ITER = 500\n",
    "\n",
    "for t in range(NUM_ITER):\n",
    "    # Forward pass: compute predicted y by passing x to the model. \n",
    "    # Module objects  override the __call__ operator so you can call them \n",
    "    # like functions. When doing so you pass a Variable of input data to the \n",
    "    # Module and it produces  a Variable of output data.\n",
    "    yPred = model(X) # Variable type of output data\n",
    "    \n",
    "    # Compute and print loss. Pass Variables containing the predicted\n",
    "    # and true values of Y\n",
    "    # Loss function returns a Variable containing the loss. \n",
    "    loss = lossFunction(yPred, Y)\n",
    "    \n",
    "    \n",
    "    if t % 50 == 0:\n",
    "        print(\"iter = \", t, \"; iter = \", loss.data[0])\n",
    "    \n",
    "    # Zero the gradients before running backward pass (??)\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to all the \n",
    "    # learnable parameters of the model. \n",
    "    # Internally, the parameters of each Module are stored in Variables\n",
    "    # which have the attribute requires_grad set to True so the the\n",
    "    # backward() call will compute gradients for all learnable\n",
    "    # parameters in the model. \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights using gradient descent algo. \n",
    "    # Each parameter is a Variable so can access its data and gradients.\n",
    "    for param in model.parameters():\n",
    "        param.data -= learningRate * param.grad.data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2.0
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
