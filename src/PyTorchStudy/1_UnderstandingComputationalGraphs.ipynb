{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grad fn for a is None\nTrue\nThe grad fn for d is <ThAddBackward object at 0x7f68171652e8>\nFalse\nNone\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a\n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c\n",
    "\n",
    "L = 10 - d\n",
    "\n",
    "print(\"The grad fn for a is\", a.grad_fn)\n",
    "print(a.is_leaf) # leaf nodes do not have gradient information\n",
    "print(\"The grad fn for d is\", d.grad_fn) # addbackward = addition operation\n",
    "print(d.is_leaf)\n",
    "\n",
    "# NOTE: \n",
    "# The forward function of the grad_fn of 'd' receives inputs w3*b and w4*c\n",
    "# and adds them. This value is stored in 'd'\n",
    "# The backward function of the grad_fn = <AddBackward> takes the\n",
    "# incoming gradient from further layers, as its own input. This is dL/dd\n",
    "# coming along the edge leading from 'L' to 'd'. \n",
    "# This dL/dd is stored in the d.grad\n",
    "print(d.grad) # none yet\n",
    "# Then the backward function computes local gradients dd/d(w4*c) and\n",
    "# dd/d(w3*b)\n",
    "# Then multiplies the incoming gradient dL/dd with the local gradients\n",
    "# above respectively and sends the gradients to its inputs by \n",
    "# invoking the backward method of the grad_fn of their inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of workings behind backward() function in pytorch\n",
    "\"\"\"\n",
    "def backward(incomingGradients):\n",
    "    self.Tensor.grad = incomingGradients\n",
    "    \n",
    "    for input in self.inputs:\n",
    "        if input.grad_fn is not None: \n",
    "            newIncomingGradients = \\\n",
    "                incomingGradient * localGrad(self.Tensor, input)\n",
    "            \n",
    "            input.grad_fn.backward(newIncomingGradients)\n",
    "        else:\n",
    "            pass \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-34e4c4c0a2f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# NOTE: can only call backward() on scalar-valued tensor (0-dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# NOTE: can only call backward() on scalar-valued tensor (0-dim)\n",
    "L.backward()\n",
    "\n",
    "# This is because by definition, gradients can be computed with respect\n",
    "# to SCALAR variables only. Can't differentiate a vector with respect\n",
    "# to another vector here (Jacobian is key term here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1:  tensor([[11.8059, 10.0897, 10.1741],\n        [10.1589,  9.9698,  9.8614],\n        [ 8.7667, 10.2584,  9.1030]], grad_fn=<AddBackward>)\nL1 sum:  tensor(90.1878, grad_fn=<SumBackward0>)\nw1.grad:  tensor([[-1.6723, -0.1096,  0.0609],\n        [ 0.5642,  0.2451,  0.0183],\n        [-3.4399, -0.2316, -0.1421]])\nw2.grad:  tensor([[-0.5496,  0.0661,  0.0495],\n        [-0.1470,  0.1291,  0.1146],\n        [-0.9640, -0.3357, -0.7031]])\nw3.grad:  tensor([[ 0.3724,  0.2382,  0.0036],\n        [ 0.1215, -0.4364,  0.0702],\n        [ 0.4071,  0.1608, -0.5248]])\nw4.grad:  tensor([[ 1.0401,  0.0329, -0.2212],\n        [ 1.1378,  0.7225, -0.2121],\n        [ 0.4957,  0.1829,  1.4171]])\n"
     ]
    }
   ],
   "source": [
    "# Two possible workarounds: \n",
    "\n",
    "\n",
    "# METHOD 1: setting L to the sum of all the errors:\n",
    "torch.manual_seed(1)\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a\n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c\n",
    "\n",
    "# METHOD 1: setting L to the sum of all the errors:\n",
    "L = 10 - d\n",
    "print(\"L1: \", L)\n",
    "L = (10 - d).sum()\n",
    "print(\"L1 sum: \", L)\n",
    "L.backward()\n",
    "print(\"w1.grad: \", w1.grad)\n",
    "print(\"w2.grad: \", w2.grad)\n",
    "print(\"w3.grad: \", w3.grad)\n",
    "print(\"w4.grad: \", w4.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.grad:  tensor([[-1.6723, -0.1096,  0.0609],\n        [ 0.5642,  0.2451,  0.0183],\n        [-3.4399, -0.2316, -0.1421]])\nw2.grad:  tensor([[-0.5496,  0.0661,  0.0495],\n        [-0.1470,  0.1291,  0.1146],\n        [-0.9640, -0.3357, -0.7031]])\nw3.grad:  tensor([[ 0.3724,  0.2382,  0.0036],\n        [ 0.1215, -0.4364,  0.0702],\n        [ 0.4071,  0.1608, -0.5248]])\nw4.grad:  tensor([[ 1.0401,  0.0329, -0.2212],\n        [ 1.1378,  0.7225, -0.2121],\n        [ 0.4957,  0.1829,  1.4171]])\n"
     ]
    }
   ],
   "source": [
    "# METHOD 2: \n",
    "torch.manual_seed(1)\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a\n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c\n",
    "\n",
    "# METHOD 2: if there is some reason you need to call backward on a \n",
    "# vector function then you can pass a torch.ones of the same size and\n",
    "# shape of teh tensor you are trying to call backward with\n",
    "L = 10 - d\n",
    "L.backward(torch.ones(L.shape))\n",
    "\n",
    "print(\"w1.grad: \", w1.grad)\n",
    "print(\"w2.grad: \", w2.grad)\n",
    "print(\"w3.grad: \", w3.grad)\n",
    "print(\"w4.grad: \", w4.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how backward used to take incoming gradients as it's input.\n",
    "# Doing the above makes the backward think that incoming gradient are \n",
    "# just Tensor of ones of same size as L, and it's able to backpropagate.\n",
    "# \n",
    "# In this way, we can have gradients for every Tensor , and we can \n",
    "# update them using Optimisation algorithm of our choice.\n",
    "learningRate = 0.5\n",
    "w1 = w1 - learningRate * w1.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2731, -0.8375, -0.0887],\n        [-0.4776, -1.0882,  0.4133],\n        [ 1.9873, -0.3054, -0.4396]], grad_fn=<ThSubBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidbit on gradient accumulation and retain_graph argument:\n",
    "https://hyp.is/q_cZGAFrEeqh-IuJztoNmg/blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
