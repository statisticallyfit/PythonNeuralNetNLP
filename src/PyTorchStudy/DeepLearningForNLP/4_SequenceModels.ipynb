{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Source:\n",
    "http://seba1511.net/tutorials/beginner/nlp/sequence_models_tutorial.html#annotations:QNRYtvyoEemz3m-NBWCG8A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4093fa5f30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1.6002,  1.3146, -0.6118]]), tensor([[-0.9419, -0.1675, -1.6990]]), tensor([[-2.0724,  1.5600, -0.5075]]), tensor([[-1.6533, -0.0907, -1.0677]]), tensor([[-0.4728, -0.0388, -0.0063]])]\n\n\n(tensor([[[-0.1100,  0.1423,  0.2453]]]), tensor([[[-0.6245, -0.7920,  1.2385]]]))\n"
     ]
    }
   ],
   "source": [
    "# Small example of LSTM\n",
    "\n",
    "# input_size (dimension) = 3, hidden_size (here, output) = 3\n",
    "lstm = nn.LSTM(3,3)\n",
    "\n",
    "# Create five 1 x 3  vectors to be inputs\n",
    "inputs = [autograd.Variable(torch.randn((1, 3)))\n",
    "          for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn((1, 1, 3))))\n",
    "\n",
    "print(inputs)\n",
    "print(\"\\n\")\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = \n tensor([[[-0.3927, -0.0463,  0.5614]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[-0.3927, -0.0463,  0.5614]]], grad_fn=<ViewBackward>), tensor([[[-0.6644, -0.3326,  0.9072]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[-0.1232, -0.0005,  0.2808]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[-0.1232, -0.0005,  0.2808]]], grad_fn=<ViewBackward>), tensor([[[-0.2591, -0.0009,  0.5684]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[0.0109, 0.0576, 0.1792]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[0.0109, 0.0576, 0.1792]]], grad_fn=<ViewBackward>), tensor([[[0.0528, 0.1260, 0.6644]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[0.0646, 0.1672, 0.2290]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[0.0646, 0.1672, 0.2290]]], grad_fn=<ViewBackward>), tensor([[[0.1667, 0.2820, 0.6441]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[0.1181, 0.0708, 0.3723]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[0.1181, 0.0708, 0.3723]]], grad_fn=<ViewBackward>), tensor([[[0.2253, 0.1735, 0.7573]]], grad_fn=<ViewBackward>)) \n\n"
     ]
    }
   ],
   "source": [
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(\"out = \\n\", out, \"\\nhidden = \\n\", hidden, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs =  tensor([[[ 1.6002,  1.3146, -0.6118]],\n\n        [[-0.9419, -0.1675, -1.6990]],\n\n        [[-2.0724,  1.5600, -0.5075]],\n\n        [[-1.6533, -0.0907, -1.0677]],\n\n        [[-0.4728, -0.0388, -0.0063]]]) \n\nout =  tensor([[[-0.4970,  0.0303,  0.1713]],\n\n        [[-0.2354,  0.2378,  0.1109]],\n\n        [[-0.0393,  0.1080,  0.1264]],\n\n        [[ 0.0043,  0.1939,  0.2011]],\n\n        [[ 0.0627,  0.0760,  0.3533]]], grad_fn=<CatBackward>) \n\nhidden =  (tensor([[[0.0627, 0.0760, 0.3533]]], grad_fn=<ViewBackward>), tensor([[[0.1189, 0.1811, 0.7124]]], grad_fn=<ViewBackward>)) \n\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states\n",
    "# throughout the sequence. the second is just the most \n",
    "# recent hidden state (compare the last slice of \"out\" with \n",
    "# \"hidden\" below, they are the same). The reason for this \n",
    "# is that: \"out\" will give you access to all hidden states \n",
    "# in the sequence \"hidden\" will allow you to continue \n",
    "# the sequence and backpropogate, by passing it as an \n",
    "# argument  to the lstm at a later time.\n",
    "# Add the extra 2nd dimension.\n",
    "\n",
    "# concatenate the tensor inputs along the rows\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "# clean out hidden state (erase previous state for sake of this example)\n",
    "hidden = (autograd.Variable(torch.randn(1,1,3)), \n",
    "          autograd.Variable(torch.randn((1,1,3))))\n",
    "\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "\n",
    "\n",
    "print(\"inputs = \", inputs, \"\\n\")\n",
    "print(\"out = \", out, \"\\n\")\n",
    "print(\"hidden = \", hidden, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
