{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Source:\n",
    "http://seba1511.net/tutorials/beginner/nlp/sequence_models_tutorial.html#annotations:QNRYtvyoEemz3m-NBWCG8A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4093fa5f30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1.6002,  1.3146, -0.6118]]), tensor([[-0.9419, -0.1675, -1.6990]]), tensor([[-2.0724,  1.5600, -0.5075]]), tensor([[-1.6533, -0.0907, -1.0677]]), tensor([[-0.4728, -0.0388, -0.0063]])]\n\n\n(tensor([[[-0.1100,  0.1423,  0.2453]]]), tensor([[[-0.6245, -0.7920,  1.2385]]]))\n"
     ]
    }
   ],
   "source": [
    "# Small example of LSTM\n",
    "\n",
    "# input_size (dimension) = 3, hidden_size (here, output) = 3\n",
    "lstm = nn.LSTM(3,3)\n",
    "\n",
    "# Create five 1 x 3  vectors to be inputs\n",
    "inputs = [autograd.Variable(torch.randn((1, 3)))\n",
    "          for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn((1, 1, 3))))\n",
    "\n",
    "print(inputs)\n",
    "print(\"\\n\")\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = \n tensor([[[-0.3927, -0.0463,  0.5614]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[-0.3927, -0.0463,  0.5614]]], grad_fn=<ViewBackward>), tensor([[[-0.6644, -0.3326,  0.9072]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[-0.1232, -0.0005,  0.2808]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[-0.1232, -0.0005,  0.2808]]], grad_fn=<ViewBackward>), tensor([[[-0.2591, -0.0009,  0.5684]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[0.0109, 0.0576, 0.1792]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[0.0109, 0.0576, 0.1792]]], grad_fn=<ViewBackward>), tensor([[[0.0528, 0.1260, 0.6644]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[0.0646, 0.1672, 0.2290]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[0.0646, 0.1672, 0.2290]]], grad_fn=<ViewBackward>), tensor([[[0.1667, 0.2820, 0.6441]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[0.1181, 0.0708, 0.3723]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[0.1181, 0.0708, 0.3723]]], grad_fn=<ViewBackward>), tensor([[[0.2253, 0.1735, 0.7573]]], grad_fn=<ViewBackward>)) \n\n"
     ]
    }
   ],
   "source": [
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(\"out = \\n\", out, \"\\nhidden = \\n\", hidden, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs =  tensor([[[ 1.6002,  1.3146, -0.6118]],\n\n        [[-0.9419, -0.1675, -1.6990]],\n\n        [[-2.0724,  1.5600, -0.5075]],\n\n        [[-1.6533, -0.0907, -1.0677]],\n\n        [[-0.4728, -0.0388, -0.0063]]]) \n\nout =  tensor([[[-0.4970,  0.0303,  0.1713]],\n\n        [[-0.2354,  0.2378,  0.1109]],\n\n        [[-0.0393,  0.1080,  0.1264]],\n\n        [[ 0.0043,  0.1939,  0.2011]],\n\n        [[ 0.0627,  0.0760,  0.3533]]], grad_fn=<CatBackward>) \n\nhidden =  (tensor([[[0.0627, 0.0760, 0.3533]]], grad_fn=<ViewBackward>), tensor([[[0.1189, 0.1811, 0.7124]]], grad_fn=<ViewBackward>)) \n\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states\n",
    "# throughout the sequence. the second is just the most \n",
    "# recent hidden state (compare the last slice of \"out\" with \n",
    "# \"hidden\" below, they are the same). The reason for this \n",
    "# is that: \"out\" will give you access to all hidden states \n",
    "# in the sequence \"hidden\" will allow you to continue \n",
    "# the sequence and backpropogate, by passing it as an \n",
    "# argument  to the lstm at a later time.\n",
    "# Add the extra 2nd dimension.\n",
    "\n",
    "# concatenate the tensor inputs along the rows\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "# clean out hidden state (erase previous state for sake of this example)\n",
    "hidden = (autograd.Variable(torch.randn(1,1,3)), \n",
    "          autograd.Variable(torch.randn((1,1,3))))\n",
    "\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "\n",
    "\n",
    "print(\"inputs = \", inputs, \"\\n\")\n",
    "print(\"out = \", out, \"\\n\")\n",
    "print(\"hidden = \", hidden, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: An LSTM for Part-of-Speech Tagging\n",
    "\n",
    "In this section, we will use an LSTM to get part of speech tags. \n",
    "\n",
    "$$\n",
    "The model is as follows: let our input sentence be\n",
    "w_1, ..., w_M, where w_i \\in V, and V = the vocabulary.\n",
    "<br> Also let:\n",
    "<br> T = tag set, \n",
    "<br> y_i = tag of word w_i\n",
    "<br> h_i = hidden state at timestep i\n",
    "\n",
    "The output is a sequence \\hat y_1, ..., \\hat y_M where \\hat y_i \\in T\n",
    "$$\n",
    "\n",
    "# major TODO edit above: how to do latex in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tagToIndex = {\"DET\":0, \"NN\":1, \"V\":2}\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "def prepareSequence(seq, toIndex):\n",
    "    indices = [toIndex[w] for w in seq]\n",
    "    tensorIndices = torch.LongTensor(indices)\n",
    "    return autograd.Variable(tensorIndices)\n",
    "\n",
    "\n",
    "trainingData = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "\n",
    "wordToIndex = {}\n",
    "\n",
    "for sent, tags in trainingData:\n",
    "    for word in sent: \n",
    "        if word not in wordToIndex:\n",
    "            wordToIndex[word] = len(wordToIndex)\n",
    "            \n",
    "        \n",
    "print(wordToIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddingDim, hiddenDim, vocabSize, tagsetSize):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hiddenDim = hiddenDim \n",
    "        self.wordEmbeddings = nn.Embedding(vocabSize, embeddingDim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs and outputs hidden states\n",
    "        # with dimensionality hiddenDim\n",
    "        self.lstm = nn.LSTM(embeddingDim, hiddenDim)\n",
    "        \n",
    "        # The Linear layer that maps from hidden state\n",
    "        # space to the tag space\n",
    "        self.hiddenToTagLayer = nn.Linear(hiddenDim, tagsetSize)\n",
    "        self.hiddenLayer = self.initHiddenLayer()\n",
    "        \n",
    "    def initHiddenLayer(self):\n",
    "        # Before doing anything we have NO hidden state.\n",
    "        # Creating one here (?)\n",
    "        # The axes semantics are (numLayers, miniBatchSize, hiddenDim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hiddenDim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hiddenDim)))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embed = self.wordEmbeddings(sentence)\n",
    "        \n",
    "        lstmOut, self.hiddenLayer = self.lstm(\n",
    "            embed.view(len(sentence), 1, -1), \n",
    "            self.hiddenLayer\n",
    "        )\n",
    "        \n",
    "        tagSpace = self.hiddenToTagLayer(lstmOut.view(len(sentence), -1))\n",
    "        \n",
    "        tagScores = F.log_softmax(tagSpace)\n",
    "        \n",
    "        return tagScores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n  (wordEmbeddings): Embedding(9, 6)\n  (lstm): LSTM(6, 6)\n  (hiddenToTagLayer): Linear(in_features=6, out_features=3, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LSTMTagger(embeddingDim = EMBEDDING_DIM, \n",
    "                   hiddenDim = HIDDEN_DIM,\n",
    "                   vocabSize = len(wordToIndex),\n",
    "                   tagsetSize = len(tagToIndex))\n",
    "\n",
    "lossFunction = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the scores before training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
