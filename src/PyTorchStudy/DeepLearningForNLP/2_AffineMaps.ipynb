{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Deep learning consists of composing linearities with non-linearities in clever ways. The introduction of non-linearities allows for powerful models. In this section, we will play with these core components, make up an objective function, and see how the model is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fecb3649eb0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=3, bias=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3) # maps from R^5 to R^3\n",
    "lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1755, -0.3268, -0.5069],\n        [-0.6602,  0.2260,  0.1089]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "data = autograd.Variable(torch.randn(2, 5))\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5404, -2.2102],\n        [ 2.1130, -0.0040]])\ntensor([[0.0000, 0.0000],\n        [2.1130, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
    "# Note that non-linearites typically don't have parameters like affine maps do.\n",
    "# That is, they don't have weights that are updated during training.\n",
    "data = autograd.Variable(torch.randn(2, 2))\n",
    "print(data)\n",
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3800, -1.3505,  0.3455,  0.5046,  1.8213])\ntensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584])\ntensor(1.)\ntensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  after removing the cwd from sys.path.\n/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  \"\"\"\n/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  \n"
     ]
    }
   ],
   "source": [
    "# Softmax is also in torch.nn.functional\n",
    "data = autograd.Variable(torch.randn(5))\n",
    "print(data)\n",
    "print(F.softmax(data))\n",
    "print(F.softmax(data).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
     ]
    }
   ],
   "source": [
    "# Example: Logistic Regression bag of Words Classifier. \n",
    "\n",
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"), \n",
    "        (\"Give it to me\".split(), \"ENGLISH\"), \n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "testData = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "            (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# mapping each word in the vocab to a unique integer which will be\n",
    "# its index into the bag of words vector\n",
    "wordToIx = {}\n",
    "\n",
    "for sent, _ in data + testData:\n",
    "    for word in sent: \n",
    "        if word not in wordToIx:\n",
    "            wordToIx[word] = len(wordToIx)\n",
    "            \n",
    "print(wordToIx)\n",
    "\n",
    "VOCAB_SIZE = len(wordToIx)\n",
    "NUM_LABELS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module): # inheriting from nn.Module\n",
    "    \n",
    "    def __init__(self, numLabels, vocabSize):\n",
    "        # Calls the init function of nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # nn.Linear() provides the affine map.\n",
    "        self.linear = nn.Linear(vocabSize, numLabels)\n",
    "        \n",
    "    def forward(self, bowVector):\n",
    "        # Pass the input through the linear layer, then pass\n",
    "        # that through log_softmax\n",
    "        return F.log_softmax(self.linear(bowVector))\n",
    "    \n",
    "    \n",
    "    \n",
    "def makeBowVector(sentence, wordToIx):\n",
    "    vec = torch.zeros(len(wordToIx))\n",
    "    \n",
    "    for word in sentence: \n",
    "        vec[wordToIx[word]] += 1\n",
    "    \n",
    "    return vec.view(1, -1)\n",
    "\n",
    "def makeTarget(label, labelToIx):\n",
    "    return torch.LongTensor([labelToIx[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\ntensor([[ 0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,\n         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,\n          0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,\n          0.0119, -0.0334],\n        [ 0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,\n          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,\n          0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,\n          0.1344,  0.0406]], requires_grad=True)\nParameter containing:\ntensor([0.0631, 0.1465], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "\n",
    "# BoWClassifier (our module) will store knowledge of the nn.Linear's\n",
    "# parameters (? when you assign a component to a class variable in the\n",
    "# __init__ function of our module?)\n",
    "for param in model.parameters():\n",
    "    print(param) # parameters are: numlabels, and vocabsize\n",
    "    \n",
    "    # first output below is A (matrix) \n",
    "    # the second output is b (vector)\n",
    "    # where the affine linear map is f(x) = Ax + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:  (['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH')\nsample[0]:  ['me', 'gusta', 'comer', 'en', 'la', 'cafeteria']\nbowVector:  tensor([[1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.]])\nlogprobs:  tensor([[-0.5378, -0.8771]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# To run the model, pass in a BoW vector but wrapped in an\n",
    "# autograd.Variable\n",
    "sample = data[0]\n",
    "print(\"sample: \", sample)\n",
    "print(\"sample[0]: \", sample[0])\n",
    "\n",
    "bowVector = makeBowVector(sample[0], wordToIx)\n",
    "print(\"bowVector: \", bowVector)\n",
    "\n",
    "# TODO: does calling model call forward pass as well?\n",
    "# How else are the log softmax probs below obtained?\n",
    "logProbabilities = model(autograd.Variable(bowVector))\n",
    "print(\"logprobs: \", logProbabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SPANISH': 0, 'ENGLISH': 1}\n{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
     ]
    }
   ],
   "source": [
    "# Defining which of the above values corresponds to the log\n",
    "# probability of ENGLISH and which to SPANISH\n",
    "labelToIx = {\"SPANISH\": 0, \"ENGLISH\": 1}\n",
    "\n",
    "print(labelToIx)\n",
    "print(wordToIx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance:  ['Yo', 'creo', 'que', 'si'] \nlabel:  SPANISH \nlogProbs:  tensor([[-0.9297, -0.5020]], grad_fn=<LogSoftmaxBackward>)\ninstance:  ['it', 'is', 'lost', 'on', 'me'] \nlabel:  ENGLISH \nlogProbs:  tensor([[-0.6388, -0.7506]], grad_fn=<LogSoftmaxBackward>)\n\nmodel params:  Parameter containing:\ntensor([[ 0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,\n         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,\n          0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,\n          0.0119, -0.0334],\n        [ 0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,\n          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,\n          0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,\n          0.1344,  0.0406]], requires_grad=True)\n\nmodel params? corresponding to 'creo':  tensor([-0.1488, -0.1313], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    " #Testing the model first: \n",
    "# 1. pass instances through to get log probabilities, \n",
    "# 2. compute a  loss function\n",
    "# 3. compute gradient of loss function\n",
    "# 4. update parameters with a graident step\n",
    "\n",
    "# Run on test data before we train, as a sample before-after showcase\n",
    "\n",
    "# Step 1: get log probs\n",
    "for instance, label in testData:\n",
    "    bowVec = autograd.Variable(makeBowVector(instance, wordToIx))\n",
    "    logProbs = model(bowVec)\n",
    "    \n",
    "    print(\"instance: \", instance, \n",
    "          \"\\nlabel: \", label, \n",
    "          \"\\nlogProbs: \", logProbs)\n",
    "    \n",
    "    \n",
    "# Print the matrix column corresponding to \" creo\"\n",
    "n = next(model.parameters())\n",
    "i = wordToIx[\"creo\"]\n",
    "print(\"\\nmodel params: \", n)\n",
    "print(\"\\nmodel params? corresponding to 'creo': \", n[:, i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH'), (['Give', 'it', 'to', 'me'], 'ENGLISH'), (['No', 'creo', 'que', 'sea', 'una', 'buena', 'idea'], 'SPANISH'), (['No', 'it', 'is', 'not', 'a', 'good', 'idea', 'to', 'get', 'lost', 'at', 'sea'], 'ENGLISH')]\nb1:  tensor([[1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.]]) \nb2:  tensor([[1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.]]) \nb3:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.]]) \nb4:  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n         1., 1., 1., 1., 1., 0., 0., 0.]])\n\nt1:  tensor([0]) \nt2:  tensor([1]) \nt3:  tensor([0]) \nt4:  tensor([1])\n\nlogprobs1:  tensor([[-0.0180, -4.0287]], grad_fn=<LogSoftmaxBackward>) \nlogprobs2:  tensor([[-1.2500, -0.3376]], grad_fn=<LogSoftmaxBackward>) \nlogprobs3:  tensor([[-0.2201, -1.6219]], grad_fn=<LogSoftmaxBackward>) \nlogprobs4:  tensor([[-2.6927, -0.0701]], grad_fn=<LogSoftmaxBackward>)\n\nloss1:  tensor(0.0180, grad_fn=<NllLossBackward>) \nloss2:  tensor(0.3376, grad_fn=<NllLossBackward>) \nloss3:  tensor(0.2201, grad_fn=<NllLossBackward>) \nloss4:  tensor(0.0701, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Going through the forloop below manually to understand how \n",
    "# bow vectors are made: \n",
    "\n",
    "print(data)\n",
    "i1, l1 = data[0]\n",
    "i2, l2 = data[1]\n",
    "i3, l3 = data[2]\n",
    "i4, l4 = data[3]\n",
    "\n",
    "#print(wordToIx)\n",
    "b1 = autograd.Variable(makeBowVector(i1, wordToIx))\n",
    "b2 = autograd.Variable(makeBowVector(i2, wordToIx))\n",
    "b3 = autograd.Variable(makeBowVector(i3, wordToIx))\n",
    "b4 = autograd.Variable(makeBowVector(i4, wordToIx))\n",
    "\n",
    "print(\"b1: \", b1, \"\\nb2: \", b2, \"\\nb3: \", b3, \"\\nb4: \", b4)\n",
    "\n",
    "t1 = autograd.Variable(makeTarget(l1, labelToIx))\n",
    "t2 = autograd.Variable(makeTarget(l2, labelToIx))\n",
    "t3 = autograd.Variable(makeTarget(l3, labelToIx))\n",
    "t4 = autograd.Variable(makeTarget(l4, labelToIx))\n",
    "\n",
    "print(\"\\nt1: \", t1, \"\\nt2: \", t2, \"\\nt3: \", t3, \"\\nt4: \", t4)\n",
    "\n",
    "\n",
    "## ----------------------------------------------------------------- \n",
    "\n",
    "# Run the forward pass to get log probabilities\n",
    "logProbs1 = model(b1)\n",
    "logProbs2 = model(b2)\n",
    "logProbs3 = model(b3)\n",
    "logProbs4 = model(b4)\n",
    "print(\"\\nlogprobs1: \", logProbs1, \"\\nlogprobs2: \", logProbs2, \n",
    "      \"\\nlogprobs3: \", logProbs3, \"\\nlogprobs4: \", logProbs4)\n",
    "\n",
    "# Step 2: Compute the loss function\n",
    "loss1 = lossFunction(logProbs1, t1)\n",
    "loss2 = lossFunction(logProbs2, t2)\n",
    "loss3 = lossFunction(logProbs3, t3)\n",
    "loss4 = lossFunction(logProbs4, t4)\n",
    "print(\"\\nloss1: \", loss1, \"\\nloss2: \", loss2,\n",
    "      \"\\nloss3: \", loss3, \"\\nloss4: \", loss4)\n",
    "\n",
    "# Step 3: compute gradients with respect to loss\n",
    "loss1.backward()\n",
    "loss2.backward()\n",
    "loss3.backward()\n",
    "loss4.backward()\n",
    "#print(\"\\nb1.grad: \", b1.grad)\n",
    "# which variable has the .grad property here?\n",
    "\n",
    "# Step 4: Update parameters with a gradient step\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preparing to train the data now:\n",
    "\n",
    "# Steps to train the model: \n",
    "# 1. pass instances through to get log probabilities, \n",
    "# 2. compute a  loss function\n",
    "# 3. compute gradient of loss function\n",
    "# 4. update parameters with a gradient step\n",
    "\n",
    "\n",
    "lossFunction = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "# Usually we use 5 and 30 epochs for training data but here\n",
    "# we use 100 since this data is small\n",
    "\n",
    "NUM_ITER = 100\n",
    "\n",
    "for epoch in range(NUM_ITER):\n",
    "    for instance, label in data:\n",
    "        # Pytorch accumulates gradients so need to clear\n",
    "        # them out each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 1: Make the BOW vector and wrap the target in a \n",
    "        # Variable as an integer. For example, if the target is\n",
    "        # SPANISH, then wrap the integer 0. If target is ENGLISH\n",
    "        # wrap the integer 1. (as per labelToIx)\n",
    "        # The loss function then knows that the 0th element of the\n",
    "        # log probabilities is the log probability corresponding\n",
    "        # to SPANISH\n",
    "        bowVec = autograd.Variable(makeBowVector(instance, wordToIx))\n",
    "        target = autograd.Variable(makeTarget(label, labelToIx))\n",
    "        \n",
    "        # Run the forward pass to get log probabilities\n",
    "        logProbs = model(bowVec)\n",
    "        \n",
    "        # Step 2: Compute the loss function\n",
    "        loss = lossFunction(logProbs, target)\n",
    "            \n",
    "        # Step 3: compute gradients with respect to loss\n",
    "        loss.backward()\n",
    "        # Step 4: Update parameters with a gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "# TODO: why getting error here?\n",
    "#for instance, label in testData:\n",
    "#    bowVec = autograd.Variable(instance, labelToIx)\n",
    "#    logProbs = model(bowVec) # forward pass\n",
    "#    print(\"test log probs: \", logProbabilities)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
