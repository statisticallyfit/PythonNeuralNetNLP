{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Source:\n",
    "http://seba1511.net/tutorials/beginner/nlp/sequence_models_tutorial.html#annotations:QNRYtvyoEemz3m-NBWCG8A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4093fa5f30>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.5525,  0.6355, -0.3968]]), tensor([[-0.6571, -1.6428,  0.9803]]), tensor([[-0.0421, -0.8206,  0.3133]]), tensor([[-1.1352,  0.3773, -0.2824]]), tensor([[-2.5667, -1.4303,  0.5009]])]\n\n\n(tensor([[[ 0.5438, -0.4057,  1.1341]]]), tensor([[[-1.1115,  0.3501, -0.7703]]]))\n"
     ]
    }
   ],
   "source": [
    "# Small example of LSTM\n",
    "\n",
    "# input_size (dimension) = 3, hidden_size (here, output) = 3\n",
    "lstm = nn.LSTM(3,3)\n",
    "\n",
    "# Create five 1 x 3  vectors to be inputs\n",
    "inputs = [autograd.Variable(torch.randn((1, 3)))\n",
    "          for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn((1, 1, 3))))\n",
    "\n",
    "print(inputs)\n",
    "print(\"\\n\")\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = \n tensor([[[-0.2682,  0.0304, -0.1526]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[-0.2682,  0.0304, -0.1526]]], grad_fn=<ViewBackward>), tensor([[[-1.0766,  0.0972, -0.5498]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[-0.5370,  0.0346, -0.1958]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[-0.5370,  0.0346, -0.1958]]], grad_fn=<ViewBackward>), tensor([[[-1.1552,  0.1214, -0.2974]]], grad_fn=<ViewBackward>)) \n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = \n tensor([[[-0.3947,  0.0391, -0.1217]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[-0.3947,  0.0391, -0.1217]]], grad_fn=<ViewBackward>), tensor([[[-1.0727,  0.1104, -0.2179]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[-0.1854,  0.0740, -0.0979]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[-0.1854,  0.0740, -0.0979]]], grad_fn=<ViewBackward>), tensor([[[-1.0530,  0.1836, -0.1731]]], grad_fn=<ViewBackward>)) \n\nout = \n tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<CatBackward>) \nhidden = \n (tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<ViewBackward>), tensor([[[-1.1298,  0.4467,  0.0254]]], grad_fn=<ViewBackward>)) \n\n"
     ]
    }
   ],
   "source": [
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(\"out = \\n\", out, \"\\nhidden = \\n\", hidden, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs =  tensor([[[-0.5525,  0.6355, -0.3968]],\n\n        [[-0.6571, -1.6428,  0.9803]],\n\n        [[-0.0421, -0.8206,  0.3133]],\n\n        [[-1.1352,  0.3773, -0.2824]],\n\n        [[-2.5667, -1.4303,  0.5009]]]) \n\nout =  tensor([[[-0.0187,  0.1713, -0.2944]],\n\n        [[-0.3521,  0.1026, -0.2971]],\n\n        [[-0.3191,  0.0781, -0.1957]],\n\n        [[-0.1634,  0.0941, -0.1637]],\n\n        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<CatBackward>) \n\nhidden =  (tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<ViewBackward>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<ViewBackward>)) \n\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states\n",
    "# throughout the sequence. the second is just the most \n",
    "# recent hidden state (compare the last slice of \"out\" with \n",
    "# \"hidden\" below, they are the same). The reason for this \n",
    "# is that: \"out\" will give you access to all hidden states \n",
    "# in the sequence \"hidden\" will allow you to continue \n",
    "# the sequence and backpropogate, by passing it as an \n",
    "# argument  to the lstm at a later time.\n",
    "# Add the extra 2nd dimension.\n",
    "\n",
    "# concatenate the tensor inputs along the rows\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "# clean out hidden state (erase previous state for sake of this example)\n",
    "hidden = (autograd.Variable(torch.randn(1,1,3)), \n",
    "          autograd.Variable(torch.randn((1,1,3))))\n",
    "\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "\n",
    "\n",
    "print(\"inputs = \", inputs, \"\\n\")\n",
    "print(\"out = \", out, \"\\n\")\n",
    "print(\"hidden = \", hidden, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: An LSTM for Part-of-Speech Tagging\n",
    "\n",
    "In this section, we will use an LSTM to get part of speech tags. \n",
    "\n",
    "$$\n",
    "The model is as follows: let our input sentence be\n",
    "w_1, ..., w_M, where w_i \\in V, and V = the vocabulary.\n",
    "Also let:\n",
    "T = tag set, \n",
    "y_i = tag of word w_i\n",
    "h_i = hidden state at timestep i\n",
    "\n",
    "The output is a sequence \\hat{y_1}, ..., \\hat{y_M} where \\hat{y_i} \\in T\n",
    "$$\n",
    "\n",
    "$$\n",
    "To predict, pass the lstm over the sentence. \n",
    "Also assign each tag a unique index (similar to using wordToIndex in the word embeddings section).\n",
    "Then the prediction rule for y-hat i is argmax of logsoftmax\n",
    "# (TODO)\n",
    "\n",
    "This means to take the log softmax of the affine map of\n",
    "the hidden state. \n",
    "The predicted tag y-hat i is the tag with maximum value\n",
    "in this vector. \n",
    "\n",
    "** NOTE; this implies the dimensionality of the target\n",
    "space of A is |T|\n",
    "$$\n",
    "\n",
    "# major TODO edit above: how to do latex in jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data: target, tags, wordindices ...\n",
    "\n",
    "tagToIndex = {\"DET\":0, \"NN\":1, \"V\":2}\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "\n",
    "trainingData = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "\n",
    "wordToIndex = {}\n",
    "\n",
    "for sent, tags in trainingData:\n",
    "    for word in sent: \n",
    "        if word not in wordToIndex:\n",
    "            wordToIndex[word] = len(wordToIndex)\n",
    "            \n",
    "        \n",
    "print(wordToIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "# Prepare data\n",
    "def prepareSequence(seq, toIndex):\n",
    "    indices = [toIndex[w] for w in seq]\n",
    "    tensorIndices = torch.LongTensor(indices)\n",
    "    return autograd.Variable(tensorIndices)\n",
    "\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddingDim, hiddenDim, vocabSize, tagsetSize):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hiddenDim = hiddenDim \n",
    "        self.wordEmbeddings = nn.Embedding(vocabSize, embeddingDim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs and outputs hidden states\n",
    "        # with dimensionality hiddenDim\n",
    "        self.lstm = nn.LSTM(embeddingDim, hiddenDim)\n",
    "        \n",
    "        # The Linear layer that maps from hidden state\n",
    "        # space to the tag space\n",
    "        self.hiddenToTagLayer = nn.Linear(hiddenDim, tagsetSize)\n",
    "        self.hiddenLayer = self.initHiddenLayer()\n",
    "        \n",
    "    def initHiddenLayer(self):\n",
    "        # Before doing anything we have NO hidden state.\n",
    "        # Creating one here (?)\n",
    "        # The axes semantics are (numLayers, miniBatchSize, hiddenDim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hiddenDim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hiddenDim)))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embed = self.wordEmbeddings(sentence)\n",
    "        \n",
    "        lstmOut, self.hiddenLayer = self.lstm(\n",
    "            embed.view(len(sentence), 1, -1), \n",
    "            self.hiddenLayer\n",
    "        )\n",
    "        \n",
    "        tagSpace = self.hiddenToTagLayer(lstmOut.view(len(sentence), -1))\n",
    "        \n",
    "        tagScores = F.log_softmax(tagSpace)\n",
    "        \n",
    "        return tagScores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n  (wordEmbeddings): Embedding(9, 6)\n  (lstm): LSTM(6, 6)\n  (hiddenToTagLayer): Linear(in_features=6, out_features=3, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LSTMTagger(embeddingDim = EMBEDDING_DIM, \n",
    "                   hiddenDim = HIDDEN_DIM,\n",
    "                   vocabSize = len(wordToIndex),\n",
    "                   tagsetSize = len(tagToIndex))\n",
    "\n",
    "lossFunction = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'ate', 'the', 'apple']\n\n tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# See the scores before training\n",
    "\n",
    "# Note that element i, j of the output is the score for\n",
    "# tag j for word i\n",
    "\n",
    "# at 0,0, the input is \"the dog ate the apple\"\n",
    "#print(trainingData)\n",
    "print(trainingData[0][0])\n",
    "\n",
    "inputs = prepareSequence(trainingData[0][0], wordToIndex)\n",
    "print(\"\\n\", inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1389, -1.2024, -0.9693],\n        [-1.1065, -1.2200, -0.9834],\n        [-1.1286, -1.2093, -0.9726],\n        [-1.1190, -1.1960, -0.9916],\n        [-1.0137, -1.2642, -1.0366]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "tagScores = model(inputs) # forward pass\n",
    "print(tagScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "# Also need to clear out the hidden state of the LSTM,\n",
    "# detaching it from its history in the last instance\n",
    "model.hiddenLayer = model.initHiddenLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1:  ['The', 'dog', 'ate', 'the', 'apple'] ; tags1 =  ['DET', 'NN', 'V', 'DET', 'NN'] \n\n\nsentenceIndices_1 =  tensor([0, 1, 2, 3, 4])\n\ntargetIndices_1 =  tensor([0, 1, 2, 0, 1])\n\ntagScores1 =  tensor([[-0.0462, -4.0106, -3.6096],\n        [-4.8205, -0.0286, -3.9045],\n        [-3.7876, -4.1355, -0.0394],\n        [-0.0185, -4.7874, -4.6013],\n        [-5.7881, -0.0186, -4.1778]], grad_fn=<LogSoftmaxBackward>)\ns2:  ['Everybody', 'read', 'that', 'book'] ; tags2 =  ['NN', 'V', 'DET', 'NN'] \n\n\nsentenceIndices_2 =  tensor([5, 6, 7, 8])\n\ntargetIndices_2 =  tensor([1, 2, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ntagScores2 =  tensor([[-4.4680, -0.0251, -4.3223],\n        [-3.6118, -3.7576, -0.0517],\n        [-0.0287, -4.8315, -3.8982],\n        [-4.7711, -0.0177, -4.6979]], grad_fn=<LogSoftmaxBackward>)\ntensor([[-0.0853, -2.9398, -3.5435],\n        [-5.2371, -0.0232, -4.0369],\n        [-3.9164, -4.1331, -0.0366],\n        [-0.0185, -4.7858, -4.6021],\n        [-5.8236, -0.0182, -4.1933]], grad_fn=<LogSoftmaxBackward>)\ntensor([[-0.0853, -2.9398, -3.5435]], grad_fn=<SplitBackward>)\ntensor([[-5.2371, -0.0232, -4.0369]], grad_fn=<SplitBackward>)\ntensor([[-3.9164, -4.1331, -0.0366]], grad_fn=<SplitBackward>)\ntensor([[-0.0185, -4.7858, -4.6021]], grad_fn=<SplitBackward>)\ntensor([[-5.8236, -0.0182, -4.1933]], grad_fn=<SplitBackward>)\n(tensor([-0.0853], grad_fn=<MaxBackward0>), tensor([0]))\n(tensor([-0.0232], grad_fn=<MaxBackward0>), tensor([1]))\n(tensor([-0.0366], grad_fn=<MaxBackward0>), tensor([2]))\n(tensor([-0.0185], grad_fn=<MaxBackward0>), tensor([0]))\n(tensor([-0.0182], grad_fn=<MaxBackward0>), tensor([1]))\n"
     ]
    }
   ],
   "source": [
    "# -------------\n",
    "# zero grad to zero because pytorch\n",
    "# accumulates gradients\n",
    "model.zero_grad()\n",
    "# Also need to clear out the hidden state of the LSTM,\n",
    "# detaching it from its history in the last instance\n",
    "model.hiddenLayer = model.initHiddenLayer()\n",
    "\n",
    "# -----------------\n",
    "\n",
    "\n",
    "\n",
    "#### First pass of training the model: to see what the\n",
    "# values look like\n",
    "sent1, tags1 = trainingData[0]\n",
    "print(\"s1: \", sent1, \"; tags1 = \", tags1, \"\\n\")\n",
    "\n",
    "# Step 2: get inputs ready for the network; that is, \n",
    "# turn them into Variables of word indices. \n",
    "sentenceIndices1 = prepareSequence(sent1, wordToIndex)\n",
    "targetIndices1 = prepareSequence(tags1, tagToIndex)\n",
    "print(\"\\nsentenceIndices_1 = \", sentenceIndices1)\n",
    "print(\"\\ntargetIndices_1 = \", targetIndices1)\n",
    "\n",
    "# Step 3: run the forward pass\n",
    "tagScores1 = model(sentenceIndices1)\n",
    "print(\"\\ntagScores1 = \", tagScores1)\n",
    "\n",
    "# Step 4: compute loss, gradients, and update\n",
    "# parameters by calling optimizer.step()\n",
    "loss1 = lossFunction(tagScores1, targetIndices1)\n",
    "loss1.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# -------------\n",
    "# zero grad to zero because pytorch\n",
    "# accumulates gradients\n",
    "model.zero_grad()\n",
    "# Also need to clear out the hidden state of the LSTM,\n",
    "# detaching it from its history in the last instance\n",
    "model.hiddenLayer = model.initHiddenLayer()\n",
    "\n",
    "# -----------------\n",
    "\n",
    "#### Second pass of training the model: to see what the\n",
    "# values look like\n",
    "sent2, tags2 = trainingData[1]\n",
    "print(\"s2: \", sent2, \"; tags2 = \", tags2, \"\\n\")\n",
    "\n",
    "# Step 2: get inputs ready for the network; that is, \n",
    "# turn them into Variables of word indices. \n",
    "sentenceIndices2 = prepareSequence(sent2, wordToIndex)\n",
    "targetIndices2 = prepareSequence(tags2, tagToIndex)\n",
    "print(\"\\nsentenceIndices_2 = \", sentenceIndices2)\n",
    "print(\"\\ntargetIndices_2 = \", targetIndices2)\n",
    "\n",
    "# Step 3: run the forward pass\n",
    "tagScores2 = model(sentenceIndices2)\n",
    "print(\"\\ntagScores2 = \", tagScores2)\n",
    "\n",
    "# Step 4: compute loss, gradients, and update\n",
    "# parameters by calling optimizer.step()\n",
    "loss2 = lossFunction(tagScores2, targetIndices2)\n",
    "loss2.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "## ---------------------------------\n",
    "# See the scores after training\n",
    "inputs = prepareSequence(trainingData[0][0], wordToIndex)\n",
    "predTagScores = model(inputs)\n",
    "\n",
    "print(predTagScores)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds \n",
    "# to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "row1, row2, row3, row4, row5 = predTagScores.split(1, dim=0)\n",
    "print(row1)\n",
    "print(row2)\n",
    "print(row3)\n",
    "print(row4)\n",
    "print(row5)\n",
    "\n",
    "# Identify max value per row\n",
    "print(row1.max(1)) # max value along dimension 1\n",
    "print(row2.max(1)) # max value along dimension 1\n",
    "print(row3.max(1)) # max value along dimension 1\n",
    "print(row4.max(1)) # max value along dimension 1\n",
    "print(row5.max(1)) # max value along dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/development/bin/python/conda3_ana/envs/pynlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0231, -4.0579, -5.1920],\n        [-6.8338, -0.0049, -5.5799],\n        [-5.8001, -5.6268, -0.0066],\n        [-0.0040, -6.1089, -6.3518],\n        [-7.5699, -0.0036, -5.7889]], grad_fn=<LogSoftmaxBackward>)\ntensor([[-0.0231, -4.0579, -5.1920]], grad_fn=<SplitBackward>)\ntensor([[-6.8338, -0.0049, -5.5799]], grad_fn=<SplitBackward>)\ntensor([[-5.8001, -5.6268, -0.0066]], grad_fn=<SplitBackward>)\ntensor([[-0.0040, -6.1089, -6.3518]], grad_fn=<SplitBackward>)\ntensor([[-7.5699, -0.0036, -5.7889]], grad_fn=<SplitBackward>)\n(tensor([-0.0231], grad_fn=<MaxBackward0>), tensor([0]))\n(tensor([-0.0049], grad_fn=<MaxBackward0>), tensor([1]))\n(tensor([-0.0066], grad_fn=<MaxBackward0>), tensor([2]))\n(tensor([-0.0040], grad_fn=<MaxBackward0>), tensor([0]))\n(tensor([-0.0036], grad_fn=<MaxBackward0>), tensor([1]))\n"
     ]
    }
   ],
   "source": [
    "# Training the model for real\n",
    "\n",
    "NUM_ITER = 300\n",
    "\n",
    "for epoch in range(NUM_ITER):\n",
    "    for sentence, tags in trainingData:\n",
    "        # Step 1: zero grad to zero because pytorch\n",
    "        # accumulates gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Also need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history in the last instance\n",
    "        model.hiddenLayer = model.initHiddenLayer()\n",
    "        \n",
    "        # Step 2: get inputs ready for the network; that is, \n",
    "        # turn them into Variables of word indices. \n",
    "        sentenceIndices = prepareSequence(sentence, wordToIndex)\n",
    "        targetIndices = prepareSequence(tags, tagToIndex)\n",
    "        \n",
    "        # Step 3: run the forward pass\n",
    "        tagScores = model(sentenceIndices)\n",
    "        \n",
    "        # Step 4: compute loss, gradients, and update\n",
    "        # parameters by calling optimizer.step()\n",
    "        loss = lossFunction(tagScores, targetIndices)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "## ---------------------------------\n",
    "# See the scores after training\n",
    "inputs = prepareSequence(trainingData[0][0], wordToIndex)\n",
    "predTagScores = model(inputs)\n",
    "\n",
    "print(predTagScores)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds \n",
    "# to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "row1, row2, row3, row4, row5 = predTagScores.split(1, dim=0)\n",
    "print(row1)\n",
    "print(row2)\n",
    "print(row3)\n",
    "print(row4)\n",
    "print(row5)\n",
    "\n",
    "# Identify max value per row\n",
    "print(row1.max(1)) # max value along dimension 1\n",
    "print(row2.max(1)) # max value along dimension 1\n",
    "print(row3.max(1)) # max value along dimension 1\n",
    "print(row4.max(1)) # max value along dimension 1\n",
    "print(row5.max(1)) # max value along dimension 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
