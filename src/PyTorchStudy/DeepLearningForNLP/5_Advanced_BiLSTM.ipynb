{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Source:\n",
    "http://seba1511.net/tutorials/beginner/nlp/advanced_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Making Dynamic Decisions and the Bi-LSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supporting paper on CRFs:\n",
    "http://www.cs.columbia.edu/~mcollins/crf.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6dfcdddf90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to make code more readable\n",
    "\n",
    "def toScalar(var):\n",
    "    # given: tensor variable\n",
    "    # returns a python float\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "def argmax(vec):\n",
    "    # given: tensor (variable?)\n",
    "    # returns: argmax, or index of maximum value in tensor\n",
    "    _, index = torch.max(vec, 1) # along dim=1\n",
    "    return toScalar(index)\n",
    "\n",
    "def prepareSequence(seq, toIndex):\n",
    "    # given: dict toIndex, seq (tensor?)\n",
    "    # return variable of indices\n",
    "    indices = [toIndex[w] for w in seq]\n",
    "    tensor = torch.LongTensor(indices)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "# Compute log sum exp in stable way for the forward algo\n",
    "def logSumExp(vec):\n",
    "    maxScore = vec[0, argmax(vec)]\n",
    "    maxScoreBroadcast = maxScore.view(1,-1).expand(1, vec.size()[1])\n",
    "    return maxScore + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - maxScoreBroadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocabSize, tagToIndex, embeddingDim, hiddenDim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embeddingDim = embeddingDim\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.vocabSize = vocabSize\n",
    "        self.tagToIndex = tagToIndex\n",
    "        self.tagsetSize = len(tagToIndex)\n",
    "        \n",
    "        self.wordEmbed = nn.Embedding(vocabSize, embeddingDim)\n",
    "        self.lstm = nn.LSTM(embeddingDim, hiddenDim // 2, \n",
    "                            num_layers=1, bidirectional=True)\n",
    "        \n",
    "        # Maps the output of LSTM into tag space\n",
    "        self.hiddenToTagLayer = nn.Linear(hiddenDim, self.tagsetSize)\n",
    "        \n",
    "        # Matrix of transition parameters. \n",
    "        # Entry i, j is the score of transition *to* i *from* j\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagsetSize, self.tagsetSize)\n",
    "        )\n",
    "        \n",
    "        # These two statements enforce the constraint that we never\n",
    "        # transfer to the start tag and never transfer from\n",
    "        # the stop tag\n",
    "        self.transitions.data[tagToIndex[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tagToIndex[STOP_TAG]] = -10000\n",
    "        \n",
    "        self.hidden = self.initHiddenLayer()\n",
    "        \n",
    "        \n",
    "    def initHiddenLayer(self):\n",
    "        return (autograd.Variable(torch.randn(2, 1, self.hiddenDim // 2)), \n",
    "                autograd.Variable(torch.randn(2, 1, self.hiddenDim // 2)))\n",
    "    \n",
    "    \n",
    "    def forwardAlgo(self, features):\n",
    "        # Do the forward algorithm to compute the partition funcion\n",
    "        initAlphas = torch.Tensor(1, self.tagsetSize).fill_(-10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        initAlphas[0][self.tagToIndex[START_TAG]] = 0.0\n",
    "        \n",
    "        # Wrap in a variable to get automatic backprop later on\n",
    "        forwardVar = autograd.Variable(initAlphas)\n",
    "        \n",
    "        # Iterate through the sentence\n",
    "        for currFeature in features:\n",
    "            alphas_t = [] # the forward variables at this timestep\n",
    "            \n",
    "            for nextTag in range(self.tagsetSize):\n",
    "                # broadcast the emission score: it is the same\n",
    "                # regardless of the previous tag\n",
    "\n",
    "                emissionScore = currFeature[nextTag].view(1,-1) \\\n",
    "                    .expand(1, self.tagsetSize)\n",
    "                \n",
    "                # the ith entry of transScore is the score of transitioning\n",
    "                # the nextTag from i\n",
    "                transScore = self.transitions[nextTag].view(1, -1)\n",
    "                \n",
    "                # The ith entry of nextTagVar is the value for the\n",
    "                # edge (i -> nextTag) before we do log-sum-exp\n",
    "                nextTagVar = forwardVar + transScore + emissionScore\n",
    "                \n",
    "                # The forward variable for this tag is the log-sum-exp \n",
    "                # for all the scores\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
