{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA = torch.cuda.is_available()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = '''As the eight strange beings applauded, one of them even cupping a hand over her lipsticked mouth to cheer, Joel tried to grasp what was happening. The nine of them sat in a fire rimmed cavern around a conference table shaped from warm volcanic rock. A chandelier of human bones dangled from the cavern’s ceiling, and it rattled around at random like wind chimes. A massive goat-man with reddish-black skin and wicked horns on his head towered above the seven others, who flanked him to either side. They looked like pure stereotype. A fat slob with sixteen chins, a used car saleman looking guy with gold and silver jewelry all over him, a sultry dominatrix in skin tight leather. On the other side a disheveled looking college drop out, a pretty boy staring in a mirror, a bald, muscular dude who looked like someone’s pissed off step-dad and a sour faced woman glancing jealously around the room. Just where the hell was he? Joel concentrated on his last memory. He remembered highlighting pages as his private jet, “The Holy Gust,” flew over the sapphire waters of the Bahamas. He had been reviewing his sermon for Sunday – dotting the I’s and crossing the crosses, a little god humor there, praise him – and the pilot’s voice had crackled over the intercom about turbulence. Kimberly, his personal assistant, had taken his plow out of her mouth and put on her seat belt. The plane had shook and then'''.lower()\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "vocab = set(words)\n",
    "w2i = {w:i for i,w in enumerate(vocab)}\n",
    "i2w = {i:w for i,w in enumerate(vocab)}\n",
    "\n",
    "\n",
    "from types import SimpleNamespace\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def generate_negative_samples(target_index, index_range, k):\n",
    "    '''\n",
    "    index_range: ranges of index to select from\n",
    "    '''\n",
    "    random_index = random.sample(index_range, 6)\n",
    "    \n",
    "    return  SimpleNamespace(\n",
    "                target=w2i[words[target_index]],\n",
    "                context=[w2i[word] for word in [words[index] for index in random_index]],\n",
    "                label=0\n",
    "    )\n",
    "        \n",
    "    \n",
    "\n",
    "def text_to_train(words, context_window=2, k=6):\n",
    "    '''\n",
    "    Make training data from words.\n",
    "    \n",
    "    For 1 positive sample, generate `k` negative samples\n",
    "    '''\n",
    "    pos = []\n",
    "    neg = []\n",
    "    context_range = range(-context_window, context_window+1)\n",
    "    for current_index in range(context_window, len(words) - context_window ) :\n",
    "        #Positive Samples\n",
    "        for relative_index in context_range:\n",
    "            if current_index + relative_index != current_index:\n",
    "                pos.append(SimpleNamespace(\n",
    "                    target=w2i[words[current_index]],\n",
    "                    context=w2i[words[current_index+relative_index]],\n",
    "                    label=1\n",
    "                ))\n",
    "        #Negative Samples\n",
    "        for _ in context_range:\n",
    "            \n",
    "            rand = random.random()\n",
    "            \n",
    "            lhs_index_range = None\n",
    "            rhs_index_range = None\n",
    "            # select from lhs of target\n",
    "            if  (current_index - context_window - 2*k) > 0:\n",
    "                #This also accounts for the fact that there should be ample samples on the LHS to select from\n",
    "                lhs_index_range = range(0, current_index - context_window)\n",
    "                \n",
    "            if (current_index + context_window + 2*k ) < len(words):\n",
    "                # If random value is >= 0.5 or there are not enough samples on the LHS\n",
    "                rhs_index_range = range(current_index + context_window, len(words))\n",
    "            \n",
    "            if lhs_index_range and rhs_index_range:\n",
    "                index_range = random.choice([lhs_index_range, rhs_index_range])\n",
    "            elif lhs_index_range:\n",
    "                index_range = lhs_index_range\n",
    "            else:\n",
    "                index_range = rhs_index_range\n",
    "\n",
    "            neg.append(\n",
    "                    generate_negative_samples(\n",
    "                        current_index,\n",
    "                        index_range=index_range,\n",
    "                        k=k\n",
    "                    )\n",
    "                )\n",
    "    return pos, neg\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(context=37, label=1, target=140)\n",
      "namespace(context=[45, 77, 88, 145, 40, 62], label=0, target=140)\n"
     ]
    }
   ],
   "source": [
    "pos_data, neg_data = text_to_train(words)\n",
    "\n",
    "print(pos_data[0])\n",
    "print(neg_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_data_point(data_point):\n",
    "    return data_point.target, data_point.context, data_point.label\n",
    "\n",
    "def data_to_variable(data, dtype=torch.LongTensor):\n",
    "    \n",
    "    tensor = Variable(dtype(data))\n",
    "    \n",
    "    if CUDA:\n",
    "        return tensor.cuda()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SkigGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.target_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "    def forward(self, target, positive_ctx, negative_ctx):\n",
    "        \n",
    "        target = data_to_variable([target])\n",
    "        positive_ctx = data_to_variable([positive_ctx])\n",
    "        negative_ctx = data_to_variable(negative_ctx)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pos_ = self.context_embedding(positive_ctx)\n",
    "        neg_ = self.context_embedding(negative_ctx)\n",
    "        target_ = self.target_embedding(target)\n",
    "        \n",
    "        \n",
    "        pos_dot = torch.matmul(pos_,torch.t(target_))\n",
    "        \n",
    "        neg_dot = torch.matmul(target_, torch.t(-neg_.squeeze()))\n",
    "        \n",
    "        # Calculate the loss\n",
    "        \n",
    "        loss = -(F.logsigmoid(pos_dot) + F.logsigmoid(neg_dot).sum())\n",
    "        \n",
    "        #Maximize `loss`, hence, minimize `-loss`\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model with dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "pos = 1\n",
    "neg = [10,11,12]\n",
    "\n",
    "target = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkigGram(20, 10).cuda()\n",
    "loss = model(target, pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 6.2671\n",
       "[torch.cuda.FloatTensor of size 1x1 (GPU 0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkigGram(len(vocab), 300)\n",
    "\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(context=[125, 96, 85, 109, 3, 139], label=0, target=140)\n"
     ]
    }
   ],
   "source": [
    "print(neg_data[1])\n",
    "tgt, ctx, lbl = unpack_data_point(neg_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140]\n",
      "[125, 96, 85, 109, 3, 139]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print([tgt])\n",
    "print(ctx)\n",
    "print([lbl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 140\n",
       "[torch.cuda.LongTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_variable([tgt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is \n",
      " 52470.0156\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Loss is \n",
      " 24601.7188\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Loss is \n",
      " 13144.7793\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Loss is \n",
      " 7350.9214\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Loss is \n",
      " 4317.2886\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Loss is \n",
      " 2652.1287\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Loss is \n",
      " 1772.1620\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Loss is \n",
      " 1362.9666\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Loss is \n",
      " 1184.9821\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Loss is \n",
      " 1107.6483\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "CPU times: user 1min 29s, sys: 9.97 s, total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for pos,neg in zip(pos_data, neg_data):\n",
    "        target, pos_ctx, label = unpack_data_point(pos)\n",
    "        target, neg_ctx, label = unpack_data_point(neg)\n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = model(target, pos_ctx, neg_ctx)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data[0]\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        print(\"Loss is\", total_loss)\n",
    "    losses.append(total_loss)\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
