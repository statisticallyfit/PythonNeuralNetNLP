{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Source: [xssChauhan/word2vec] (https://github.com/xssChauhan/word2vec/blob/master/pytorch/CBOW.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Source for text excerpt: \n",
    "# https://www.advancedwriters.com/blog/descriptive-essay-on-nature/\n",
    "\n",
    "text = '''Man has appreciated nature and still does. He is both challenged \n",
    "and pacified by it. Not only is nature beautiful, it is every changing through \n",
    "different seasons, or timelessly unchanged in it fixed elements such as its \n",
    "great mountain ranges. It has a wild beauty to it. There is a valley in central \n",
    "Africa that when you are there it seems as if you went back in time. This is \n",
    "the Zambezi river valley that starts in the wetlands of the Okavango swamps. \n",
    "The valley is 1500 miles of wilderness, totally unspoiled by man’s encroachment. \n",
    "You see only the wildness of nature. The river flows proudly through the valley. \n",
    "It is a surging force as it goes through rocky rapids, or wide and tranquil where \n",
    "it finds space. On its banks are mud flats and reeds, where crocodiles lie in the sun, \n",
    "and further away dense trees and forests of Mopani trees, interspersed with huge grey \n",
    "prehistoric baobab trees with branches that look like roots. In the day, the sun is a \n",
    "burning yellow fire, and everything wilts under it. Even the wild life finds shade and \n",
    "lies down. As the evening comes the setting sun paints the sky with streaks of pink and \n",
    "orange, and the animals emerge.\n",
    "\n",
    "They come individually or in groups. In the water large hippopotamus frolic, \n",
    "not intimidated by the presence of crocodiles. Nervous buck come dancing to \n",
    "the river.\n",
    "\n",
    "Large tan colored kudu, as tall as a horse, with their white flashes and meter \n",
    "long spiral horns, smaller dark brown impala with short spiked horns, tiny \n",
    "brown duiker.\n",
    "\n",
    "They carefully approach; stopping to be sure, no predators are near. They dip \n",
    "their heads gracefully to drink. Some suddenly will jump and struggle as a \n",
    "crocodile grabs it and drags it under the water. Elephants come and splash \n",
    "around squirting water over themselves with their long trunks, or rolling in \n",
    "the mud, which is to them a treat.\n",
    "\n",
    "Lions eventually arrive in a pride, causing the buck to move nervously away. \n",
    "The dusk gives way to the sudden blackness of the night sky studded with silver \n",
    "stars and a huge silver moon. Soon the animals were gone; the river flows on \n",
    "into the night.\n",
    "\n",
    "Not far away there was a noise like thunder that sounded constantly. In the \n",
    "early morning, flowing the river alive and sparkling in the sun, crocodiles \n",
    "basking in the warmth, animals drinking while it was still cool, the river \n",
    "broadened and flowed in channels around green islands. Then it fell down a \n",
    "100-meter chasm as a magnificent waterfall, 1708 meters wide. As the river \n",
    "fell down the chasm the sound was as thunder, and water spray rose high in \n",
    "the sky, white like the smoke of a bush fire. The bush is like a tropical \n",
    "forest as the spray rains down on it continually, and it is untouched by man. \n",
    "From here, it flows into a great lake and thence to the Indian Ocean.''' \\\n",
    "    .lower() # note no splitting here like in CBOW\n",
    "\n",
    "\n",
    "# NOTE: must download nltk's punkt tokenizer (technicalities evernote) for this to work. \n",
    "words = word_tokenize(text)\n",
    "\n",
    "vocabulary = set(words)\n",
    "wordToIndex = {w:i for i, w in enumerate(vocabulary)}\n",
    "indexToWord = {i:w for i, w in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n{'seasons': 0, 'appreciated': 1, '’': 2, 'roots': 3, 'spiral': 4, 'approach': 5, 'magnificent': 6, 'surging': 7, 'both': 8, 'tan': 9, 'noise': 10, 'into': 11, 'will': 12, 'splash': 13, 'rapids': 14, 'burning': 15, 'flows': 16, 'presence': 17, 'flashes': 18, 'be': 19, 'see': 20, 'kudu': 21, 'drinking': 22, 'wild': 23, 'squirting': 24, 'back': 25, 'totally': 26, '.': 27, 'day': 28, 'he': 29, 'they': 30, 'dark': 31, 'tiny': 32, ';': 33, 'studded': 34, 'some': 35, 'alive': 36, 'is': 37, 'spray': 38, 'rose': 39, 'lake': 40, 'meter': 41, 'sudden': 42, 'flats': 43, 'forests': 44, 'mopani': 45, 'challenged': 46, 'water': 47, 'smoke': 48, 'central': 49, 'meters': 50, 'mountain': 51, '100-meter': 52, 'banks': 53, 'even': 54, 'river': 55, 'dusk': 56, 'islands': 57, 'unspoiled': 58, 'wildness': 59, 'grabs': 60, 'thunder': 61, 'fire': 62, 'or': 63, 'individually': 64, 'dancing': 65, 'baobab': 66, 'basking': 67, 'yellow': 68, 'thence': 69, 'branches': 70, 'predators': 71, 'the': 72, 'down': 73, 'themselves': 74, 'gives': 75, 'are': 76, 'stopping': 77, 'jump': 78, 'were': 79, 'changing': 80, 'if': 81, 'green': 82, 'drags': 83, 'dip': 84, 'valley': 85, 'zambezi': 86, 'to': 87, 'sparkling': 88, 'nervous': 89, 'only': 90, 'sky': 91, 'cool': 92, 'swamps': 93, 'fell': 94, 'beauty': 95, 'okavango': 96, 'around': 97, 'pink': 98, 'lions': 99, 'away': 100, 'fixed': 101, 'such': 102, 'interspersed': 103, 'grey': 104, 'causing': 105, 'blackness': 106, 'soon': 107, 'flowed': 108, 'tropical': 109, 'nature': 110, 'africa': 111, 'short': 112, 'by': 113, 'force': 114, '1708': 115, 'animals': 116, ',': 117, 'chasm': 118, 'miles': 119, 'wetlands': 120, 'night': 121, 'tall': 122, 'beautiful': 123, 'untouched': 124, 's': 125, 'seems': 126, 'wide': 127, 'suddenly': 128, 'everything': 129, 'rains': 130, 'huge': 131, 'still': 132, 'it': 133, 'finds': 134, 'warmth': 135, 'here': 136, 'colored': 137, 'buck': 138, 'eventually': 139, 'you': 140, 'stars': 141, 'lies': 142, 'man': 143, 'rolling': 144, 'nervously': 145, 'time': 146, 'morning': 147, 'waterfall': 148, 'mud': 149, 'brown': 150, 'struggle': 151, 'channels': 152, 'emerge': 153, 'sound': 154, 'elephants': 155, 'indian': 156, 'early': 157, 'silver': 158, 'spiked': 159, 'on': 160, 'prehistoric': 161, 'sun': 162, 'flowing': 163, 'forest': 164, 'starts': 165, 'goes': 166, 'wilderness': 167, 'of': 168, 'shade': 169, 'hippopotamus': 170, 'further': 171, 'which': 172, 'its': 173, 'far': 174, 'a': 175, 'reeds': 176, 'rocky': 177, 'then': 178, 'tranquil': 179, 'near': 180, '1500': 181, 'gone': 182, 'while': 183, 'bush': 184, 'come': 185, 'pride': 186, 'unchanged': 187, 'high': 188, 'where': 189, 'sure': 190, 'moon': 191, 'went': 192, 'every': 193, 'life': 194, 'has': 195, 'encroachment': 196, 'heads': 197, 'their': 198, 'continually': 199, 'look': 200, 'through': 201, 'no': 202, 'dense': 203, 'gracefully': 204, 'proudly': 205, 'over': 206, 'streaks': 207, 'ocean': 208, 'broadened': 209, 'comes': 210, 'timelessly': 211, 'drink': 212, 'white': 213, 'pacified': 214, 'as': 215, 'carefully': 216, 'constantly': 217, 'trees': 218, 'frolic': 219, 'not': 220, 'lie': 221, 'trunks': 222, 'move': 223, 'in': 224, 'treat': 225, 'crocodile': 226, 'sounded': 227, 'smaller': 228, 'and': 229, 'arrive': 230, 'was': 231, 'there': 232, 'setting': 233, 'with': 234, 'when': 235, 'evening': 236, 'large': 237, 'like': 238, 'intimidated': 239, 'horns': 240, 'them': 241, 'duiker': 242, 'way': 243, 'that': 244, 'paints': 245, 'great': 246, 'under': 247, 'impala': 248, 'does': 249, 'from': 250, 'different': 251, 'space': 252, 'orange': 253, 'long': 254, 'crocodiles': 255, 'wilts': 256, 'horse': 257, 'elements': 258, 'ranges': 259, 'groups': 260, 'this': 261}\n"
     ]
    }
   ],
   "source": [
    "print(len(wordToIndex))\n",
    "print(wordToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNegativeSamples(targetIndex, indexRange, k):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param targetIndex: \n",
    "    :param indexRange: ranges of index to select from\n",
    "    :param k: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    randomIndicesSample = random.sample(population=indexRange, k=6)\n",
    "    \n",
    "    return SimpleNamespace(\n",
    "        target=wordToIndex[words[targetIndex]], \n",
    "        context=[wordToIndex[word] for word in [words[index] for index in randomIndicesSample]],\n",
    "        label = 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToTrain(words, contextWindowSize=2, k=6):\n",
    "    \"\"\"\n",
    "    Make training data from words. \n",
    "    For 1 positive sample, generate `k` negative samples\n",
    "    \n",
    "    :param words: \n",
    "    :param contextWindowSize: \n",
    "    :param k: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # TODO: are these samples words / tensors??\n",
    "    posSamples = []\n",
    "    negSamples = []\n",
    "    \n",
    "    contextRange = range(-contextWindowSize, contextWindowSize + 1)\n",
    "    \n",
    "    for currIndex in range(contextWindowSize, len(words) - contextWindowSize):\n",
    "        \n",
    "        # Create positive samples\n",
    "        for relativeIndex in contextRange:\n",
    "            if currIndex + relativeIndex != currIndex:\n",
    "                posSamples.append(SimpleNamespace(\n",
    "                    target=wordToIndex[words[currIndex]],\n",
    "                    context=wordToIndex[words[currIndex + relativeIndex]],\n",
    "                    label = 1\n",
    "                ))\n",
    "                \n",
    "        # Create negative samples\n",
    "        for _ in contextRange:\n",
    "            \n",
    "            randNum = random.random()\n",
    "            \n",
    "            leftSideIndexRange = None\n",
    "            rightSideIndexRange = None \n",
    "            \n",
    "            # Select from left hand side of target\n",
    "            if (currIndex - contextWindowSize - 2*k) > 0:\n",
    "                # This also accounts for the fact that there should be\n",
    "                # enough samples on the LHS to select from\n",
    "                leftSideIndexRange = range(0, currIndex - contextWindowSize)\n",
    "                \n",
    "            if (currIndex + contextWindowSize + 2*k) < len(words):\n",
    "                # If random value is >= 0.5 or there are not enough samples\n",
    "                # on the LHS, then ...\n",
    "                rightSideIndexRange = range(currIndex + contextWindowSize, len(words))\n",
    "                \n",
    "            if leftSideIndexRange and rightSideIndexRange:\n",
    "                # pick the left or right arbitrarily\n",
    "                indexRange = random.choice([leftSideIndexRange, rightSideIndexRange])\n",
    "            elif leftSideIndexRange:\n",
    "                indexRange = leftSideIndexRange\n",
    "            else:\n",
    "                indexRange = rightSideIndexRange\n",
    "                \n",
    "            negSamples.append(\n",
    "                generateNegativeSamples(\n",
    "                    targetIndex=currIndex, \n",
    "                    indexRange=indexRange, \n",
    "                    k=k\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    return posSamples, negSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[namespace(context=143, label=1, target=1), namespace(context=195, label=1, target=1), namespace(context=110, label=1, target=1), namespace(context=229, label=1, target=1), namespace(context=195, label=1, target=110), namespace(context=1, label=1, target=110), namespace(context=229, label=1, target=110), namespace(context=132, label=1, target=110), namespace(context=1, label=1, target=229), namespace(context=110, label=1, target=229)]\n\n\n[namespace(context=[229, 24, 204, 215, 37, 63], label=0, target=1), namespace(context=[160, 229, 133, 243, 234, 55], label=0, target=1), namespace(context=[209, 162, 201, 220, 220, 57], label=0, target=1), namespace(context=[228, 27, 220, 27, 175, 234], label=0, target=1), namespace(context=[133, 150, 224, 117, 72, 89], label=0, target=1), namespace(context=[44, 224, 37, 30, 226, 35], label=0, target=110), namespace(context=[140, 57, 227, 229, 73, 185], label=0, target=110), namespace(context=[133, 133, 87, 133, 168, 40], label=0, target=110), namespace(context=[66, 229, 215, 168, 56, 229], label=0, target=110), namespace(context=[159, 60, 229, 162, 87, 112], label=0, target=110)]\n"
     ]
    }
   ],
   "source": [
    "posData, negData = textToTrain(words)\n",
    "\n",
    "print(posData[:10])\n",
    "print(\"\\n\")\n",
    "print(negData[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpackDataPoint(dataPoint):\n",
    "    return dataPoint.target, dataPoint.context, dataPoint.label \n",
    "\n",
    "def dataToVariable(data, dtype=torch.LongTensor):\n",
    "    tensor = Variable(dtype(data))\n",
    "    return tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocabSize, embeddingSize):\n",
    "        super().__init__()\n",
    "        self.targetEmbedding = nn.Embedding(vocabSize, embeddingSize)\n",
    "        self.contextEmbedding = nn.Embedding(vocabSize, embeddingSize)\n",
    "        \n",
    "        \n",
    "    def forward(self, target, positiveContext, negativeContext):\n",
    "        targetTensor = dataToVariable([target])\n",
    "        posContextTensor = dataToVariable([positiveContext])\n",
    "        negContextTensor = dataToVariable([negativeContext])\n",
    "        \n",
    "        posEmbedding = self.contextEmbedding(posContextTensor)\n",
    "        negEmbedding = self.contextEmbedding(negContextTensor)\n",
    "        targetEmbedding = self.targetEmbedding(targetTensor)\n",
    "        \n",
    "        posDot = torch.matmul(posEmbedding, torch.t(targetEmbedding))\n",
    "        negDot = torch.matmul(targetEmbedding, torch.t(-negEmbedding.squeeze()))\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = -(F.logsigmoid(posDot) + F.logsigmoid(negDot).sum())\n",
    "        \n",
    "        # Maximize the `loss`, hence, minimize the `negative loss`\n",
    "        return loss \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testin the moddel with dummy data\n",
    "\n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "posSample = 1\n",
    "negSample = [10,11,12]\n",
    "target = 0\n",
    "\n",
    "model = SkipGram(vocabSize=20, embeddingSize=10)\n",
    "loss = model(target, posSample, negSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n  (targetEmbedding): Embedding(20, 10)\n  (contextEmbedding): Embedding(20, 10)\n)\ntensor([[3.6973]], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model \n",
    "del loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "learningRate = 0.001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(vocabSize=len(vocabulary), embeddingSize=300)\n",
    "optimizer = optim.SGD(model.parameters(), lr = learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(context=[160, 229, 133, 243, 234, 55], label=0, target=1)\n"
     ]
    }
   ],
   "source": [
    "print(negData[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
