{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Author: Ana-Maria Vintila, based off work from Srijith Rajamohan based off the work by Robert Guthrie\n",
    "\n",
    "Source: https://srijithr.gitlab.io/post/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "pth = os.getcwd()\n",
    "pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "Image(filename=pth + '/src/NLPstudy/images/Skip-gram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "Loading the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.tensor as Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "# Step 1: Initialization\n",
    "Here we set the context window size to $3$ words and the word embedding dimension to $10$, and also pass in the text corpora from which we build vocabulary.\n",
    "\n",
    "Tokenizing the text occurs later while reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "testSentence = \"\"\"Empathy for the poor may not come easily to people who never experienced it.\n",
    "They may blame the victims and insist their predicament can be overcome through determination\n",
    "and hard work.\n",
    "But they may not realize that extreme poverty can be psychologically and physically\n",
    "incapacitating — a perpetual cycle of bad diets, health care and education exacerbated\n",
    "by the shaming and self-fulfilling prophecies that define it in the public imagination.\n",
    "Gordon Parks — perhaps more than any artist — saw poverty as “the most savage of all human\n",
    "afflictions” and realized the power of empathy to help us understand it. It was neither an\n",
    "abstract problem nor political symbol, but something he endured growing up destitute in rural\n",
    "Kansas and having spent years documenting poverty throughout the world, including the United\n",
    "States.\n",
    "That sensitivity informed “Freedom’s Fearful Foe: Poverty,” his celebrated photo essay published\n",
    " in Life magazine in June 1961. He took readers into the lives of a Brazilian boy, Flavio\n",
    " da Silva, and his family, who lived in the ramshackle Catacumba favela in the hills outside\n",
    " Rio de Janeiro. These stark photographs are the subject of a new book, “Gordon Parks: The\n",
    "  Flavio Story” (Steidl/The Gordon Parks Foundation), which accompanies a traveling exhibition\n",
    "  co-organized by the Ryerson Image Centre in Toronto, where it opens this week, and\n",
    "  the J. Paul Getty Museum. Edited with texts by the exhibition’s co-curators, Paul Roth and\n",
    "  Amanda Maddox, the book also includes a recent interview with Mr. da Silva and essays by\n",
    "  Beatriz Jaguaribe, Maria Alice Rezende de Carvalho and Sérgio Burgi.\n",
    "\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "# Step 2: Build the $n$-grams\n",
    "Next we build the $n$-grams, or sequence of words, as a list of tuples.\n",
    "\n",
    "Each tuple is ([ `word`$_{i-2}$, `word`$_{i-1}$ ], `targetWord`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(testSentence) - CONTEXT_SIZE):\n",
    "    tup = [testSentence[j] for j in np.arange(i + 1, i + CONTEXT_SIZE + 1)]\n",
    "    # skip-gram way of appending:\n",
    "    ngrams.append( (testSentence[i], tup) )\n",
    "    # cbow# ngrams.append( (tup, testSentence[i + CONTEXT_SIZE]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "title": "markdown"
   },
   "source": [
    "# Step 3: Create Vocabulary\n",
    "Create the vocabulary by converting the text into a `set` to remove duplicate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "vocabulary = set(testSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "title": "markdown"
   },
   "source": [
    "# Step 4: Create Map of Words to Indices\n",
    "\n",
    "Creating word to index map that prints the key (word) corresponding to the given index in the dictionary argument. Basically, we get a list of tuples (number, word) from zipping the sequence $0,1,2,3 ....$ with the vocabulary word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "wordToIndex = {word : i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "def printKey(iWord, wordToIndexDict):\n",
    "    \"\"\"\n",
    "    Prints the key (the word) corresponding to the given index in the given dictionary.\n",
    "\n",
    "    :param iWord: index of a word in the given dict\n",
    "    :param wordToIndexDict: the dictionary\n",
    "    :return: key\n",
    "    \"\"\"\n",
    "    for key, index in wordToIndexDict.items():\n",
    "        if(index == iWord):\n",
    "            print(key)\n",
    "\n",
    "\n",
    "\n",
    "def clusterEmbeddings(filename, numClusters):\n",
    "    X = np.load(filename)\n",
    "    kmeans = KMeans(n_clusters=numClusters, random_state=  0).fit(X) # from sklearn\n",
    "    center = kmeans.cluster_centers_\n",
    "    distances = euclidean_distances(X, center)\n",
    "\n",
    "    for i in np.arange(0, distances.shape[1]):\n",
    "\n",
    "        # get the index of the minimum distance in the ith row of the dist matrix\n",
    "        iMinWord = np.argmin(distances[:, i])\n",
    "        print(iMinWord)\n",
    "        printKey(iWord=iMinWord, wordToIndexDict= wordToIndex)\n",
    "\n",
    "\n",
    "def readData(filePath):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data = urllib.request.urlopen(filePath)\n",
    "    data = data.read().decode('utf8')\n",
    "    tokenizedData = word_tokenize(data)\n",
    "\n",
    "    # note: stopwords are from nltk\n",
    "    stopWordsSet = set(stopwords.words('english'))\n",
    "    stopWordsSet.update(['.',',',':',';','(',')','#','--','...','\"'])\n",
    "    cleanedWords = [word for word in tokenizedData if word not in stopWordsSet]\n",
    "\n",
    "    return cleanedWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "markdown"
   },
   "source": [
    "# Step 6: Create Skip-Gram Model\n",
    "The skip-gram neural network has three components:\n",
    "\n",
    "1. embedding layer, created using pytorch's `nn.Embedding`, to convert tensors into word embeddings.\n",
    "2. hidden layer, in this case it is a linear layer.\n",
    "3. output layer, in this case also linear layer.\n",
    "\n",
    "### Forward Pass of Skip-Gram:\n",
    "1. Convert the tensor `inputs` to word embeddings via the skip-gram's `nn.Embedding` layer\n",
    "2. Pass the embeddings to the hidden layer and transform the result using the `relu` function\n",
    "3. Transform the hidden layer results using the output layer.\n",
    "4. Finally, create a probability distribution over words using the `softmax` function. (Here we actually use the `log_softmax` so the results are log probabilities instead of probabilities. )\n",
    "\n",
    "### Predictions:\n",
    "To generate predictions we need to execute the `forward` pass of the skip-gram model and get the index of the maximum probability from the output layer. Then that index is used to find the corresponding prediction word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "class SkipGramModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabSize: int, embeddingDim: int, contextSize: int):\n",
    "        super(SkipGramModeler, self).__init__()\n",
    "\n",
    "        # see docs: https://hyp.is/cv2pSAeqEeqIRHv7JAjgtA/pytorch.org/docs/stable/nn.html\n",
    "        # num_embeddings = size of the dictionary embeddings\n",
    "        # embedding_dim = the size of each embedding vector\n",
    "        # Creating an embedding model that contains (vocabSize) tensors each of size (embeddingDim)\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocabSize,\n",
    "                                       embedding_dim=embeddingDim,\n",
    "                                       padding_idx=contextSize)\n",
    "\n",
    "        # see nn.Linear docs\n",
    "        # https://hyp.is/XEDPhgerEeqFhHssJYoa-w/pytorch.org/docs/stable/nn.html\n",
    "        # note: in_features = size of each input sample\n",
    "        # note: out_features = size of each output sample\n",
    "        self.hiddenLayer = nn.Linear(in_features=embeddingDim,\n",
    "                                     out_features=128)\n",
    "\n",
    "        self.outputLayer = nn.Linear(in_features=128,\n",
    "                                     out_features=contextSize * vocabSize)\n",
    "\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: 1-dim tensor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # note: -1 implies the size inferred for that index from the size of data\n",
    "        # is a tensor\n",
    "        inputEmbeddings: Tensor = self.embeddings(inputs).view((1,-1))\n",
    "\n",
    "        # output at hidden layer\n",
    "        hiddenRELUResults: Tensor = F.relu(self.hiddenLayer(inputEmbeddings))\n",
    "        # output at final layer\n",
    "        outputResults: Tensor = self.outputLayer(hiddenRELUResults)\n",
    "\n",
    "        logProbs: Tensor = F.log_softmax(input=outputResults, dim=1).view(CONTEXT_SIZE, -1)\n",
    "\n",
    "        return logProbs\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, inputStr: str, wordToIndexDict: dict) -> list:\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputStr: single word (targetword) from which we predict context list\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        contextIndices: Tensor = torch.tensor([wordToIndexDict[inputStr]],\n",
    "                                              dtype=torch.long)\n",
    "\n",
    "        logProbs: Tensor = self.forward(contextIndices)\n",
    "\n",
    "        # get index of maximum log probability from output layer\n",
    "        #iMaxLogProbs: Tensor = torch.argmax(logProbs)\n",
    "\n",
    "        # returns log probs sorted in descending order and\n",
    "        # iSorted = indices of elements in the input tensor\n",
    "        logProbsDecr, iSorted = logProbs.sort(descending=True)\n",
    "\n",
    "        # same as logs.squeeze()[:3] (erasing first dimension)\n",
    "        # since the tensor is [[...]]\n",
    "\n",
    "        # getting sorted indices, the first one in each row of iSorted\n",
    "        # (there are three rows in the iSorted, 2-dim tensor)\n",
    "        numRows, numCols = iSorted.size()\n",
    "        iFirstCol = 0\n",
    "        indices = [iSorted[r][iFirstCol] for r in np.arange(0, numRows)]\n",
    "\n",
    "\n",
    "        keyIndFilteredPairs: list = []\n",
    "\n",
    "        for i in indices:\n",
    "\n",
    "            keyIndFilteredPairs.append( [ (key, index)\n",
    "                                       for key, index in wordToIndexDict.items()\n",
    "                                       if index == i ]  )\n",
    "\n",
    "        return keyIndFilteredPairs\n",
    "\n",
    "\n",
    "    def freezeLayer(self, layer, skipgramModel: SkipGramModeler):\n",
    "        \"\"\"\n",
    "\n",
    "        :param layer:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for name, child in skipgramModel.named_children():\n",
    "            print(\"\\nLog | name = {}, child = {}\".format(name, child))\n",
    "\n",
    "            if(name == layer):\n",
    "\n",
    "                # TODO: type of child?\n",
    "                for names, params in child.named_parameters():\n",
    "                    print(\"Log | names = {}, params = {}\".format(names, params))\n",
    "                    print(\"Log | params.size() = {}\".format(params.size()))\n",
    "                    params.requires_grad = False\n",
    "\n",
    "\n",
    "    def printLayerParamers(self):\n",
    "        for name, child in self.named_children():\n",
    "            print(\"\\nname = {}, child = {}\".format(name, child))\n",
    "\n",
    "            # TODO: type of child?\n",
    "            for names, params in child.named_parameters():\n",
    "                print(\"names = {}, params = {}\".format(names, params))\n",
    "                print(\"params.size() = {}\".format(params.size()))\n",
    "\n",
    "\n",
    "    def writeEmbeddingToFile(self, filename: str):\n",
    "        for i in self.embeddings.parameters():\n",
    "            weights = i.data.numpy()\n",
    "        np.save(filename, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "# Trial : testing out the predict() inner workings\n",
    "\n",
    "inputStr = \"psychologically\"\n",
    "\n",
    "contextIndices: Tensor = torch.tensor([wordToIndex[inputStr]],\n",
    "                                      dtype=torch.long)\n",
    "print(\"contextIndices: \", contextIndices)\n",
    "print(\"contextIndices dim: \", contextIndices.dim())\n",
    "print(\"contextIndices size: \", contextIndices.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "dummyModel = SkipGramModeler(vocabSize=len(vocabulary), embeddingDim=EMBEDDING_DIM,\n",
    "                         contextSize=CONTEXT_SIZE)\n",
    "\n",
    "logProbs: Tensor = dummyModel(contextIndices)\n",
    "\n",
    "\n",
    "# returns log probs sorted in descending order and\n",
    "# iSorted = indices of elements in the input tensor\n",
    "logProbsDecr, iSorted = logProbs.sort(descending=True)\n",
    "print(\"logProbsDecr dim : \", logProbsDecr.dim())\n",
    "print(\"logProbsDecr shape : \", logProbsDecr.shape)\n",
    "print(\"logProbsDecr squeezed: \", logProbsDecr.squeeze()[:, :5])\n",
    "print(\"logProbsDecr squeezed dim :  \", logProbsDecr.squeeze().dim())\n",
    "print(\"logProbsDecr squeezed shape: \", logProbsDecr.squeeze().shape)\n",
    "\n",
    "print(\"\\niSorted dim: \", iSorted.dim())\n",
    "# note: in this case, squeezing is same as the original tensor; has no effect\n",
    "print(\"iSorted: \", iSorted[:, :5])\n",
    "\n",
    "logProbsDecr = logProbsDecr.squeeze()   # logProbsDecr[0][:3]\n",
    "iSorted = iSorted.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "# getting sorted indices, the first one in each row of iSorted\n",
    "# (there are three rows in the iSorted, 2-dim tensor)\n",
    "numRows, numCols = iSorted.size()\n",
    "iFirstCol = 0\n",
    "indices = [iSorted[r][iFirstCol] for r in np.arange(0, numRows)]\n",
    "\n",
    "print(\"\\nindices = \", indices)\n",
    "\n",
    "\n",
    "# it length will be numRows of iSorted (numRows = 3)\n",
    "# which equals CONTEXT_SIZE\n",
    "keyIndFilteredPairs: list = []\n",
    "\n",
    "for i in indices:\n",
    "\n",
    "    keyIndFilteredPairs.append( [ (key, index)\n",
    "                                  for key, index in wordToIndex.items()\n",
    "                                  if index == i ]  )\n",
    "\n",
    "print(\"\\nlength of key,ind pairs: \", len(keyIndFilteredPairs))\n",
    "print(\"keyIndFilteredPairs: \", keyIndFilteredPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "title": "markdown"
   },
   "source": [
    "# Step 7: Train the Skip-Gram Model\n",
    "Training the model requires the following steps:\n",
    "\n",
    "1. Convert the context words into integer indices using the `wordToIndex` dictionary, and make their type a `Tensor`.\n",
    "2. Set the model gradients to zero so they do not accumulate artificially (feature of pytorch)\n",
    "3. Do the `forward` pass of the Skip-Gram model, resulting in the log probabilities of the context words.\n",
    "4. For each word in the correct target context wrods, convert it to an index using the `wordToIndex` dictionary and wrap it in a `Tensor` type.\n",
    "5. Compute the loss between the log probabilities and target contexts.\n",
    "6. Do the `backward` pass over the neural network to update the gradients by calling `loss.backward()`.\n",
    "7. Do one step using the optimizer, so that weights are updated using stochastic gradient descent.\n",
    "8. Increment the total loss by this epoch's current loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "learningRate = 0.001\n",
    "NUM_EPOCHS = 550\n",
    "\n",
    "losses = []\n",
    "lossFunction = nn.NLLLoss()\n",
    "skipGramModel = SkipGramModeler(vocabSize = len(vocabulary),\n",
    "                        embeddingDim=EMBEDDING_DIM,\n",
    "                        contextSize=CONTEXT_SIZE)\n",
    "# Using the stochastic-gradient descent optimizer.\n",
    "optimizer = optim.SGD(skipGramModel.parameters(), lr = learningRate)\n",
    "\n",
    "# Preserve the data by freezing the embedding layer\n",
    "#skipGramModel.freezeLayer(\"embeddingsSkipGram\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    totalLoss = 0\n",
    "\n",
    "    # note: skipgram predicts CONTEXT from single word\n",
    "    # while CBOW predicts single TARGET word from CONTEXT list\n",
    "    for contextWord, targetContext in ngrams:\n",
    "\n",
    "        # Step 1: Prepare the inputs to be passed to the model (means\n",
    "        # turn the words into integer indices and wrap them in tensors)\n",
    "        contextIndices: Tensor = torch.tensor([wordToIndex[contextWord]],\n",
    "                                              dtype=torch.long)\n",
    "\n",
    "        # Step 2:\n",
    "        skipGramModel.zero_grad()\n",
    "\n",
    "        # Step 3: run forward pass, getting log probs over the next words\n",
    "        logProbs = skipGramModel(contextIndices)\n",
    "\n",
    "        # Step 4: compute loss, where target word is wrapped in a tensor\n",
    "        targetContextTensor: Tensor = torch.tensor([wordToIndex[w] for w in targetContext],\n",
    "                                           dtype=torch.long)\n",
    "\n",
    "        loss = lossFunction(logProbs, targetContextTensor)\n",
    "\n",
    "        # Step 5: do backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        totalLoss += loss.item()\n",
    "\n",
    "    if(epoch % 50 == 0):\n",
    "        print(\"Epoch = {}, Total loss = {}\".format(epoch, totalLoss))\n",
    "\n",
    "    losses.append(totalLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "codecell"
   },
   "outputs": [],
   "source": [
    "skipGramModel.predict(inputStr = \"psychologically\", wordToIndexDict=wordToIndex)\n",
    "skipGramModel.writeEmbeddingToFile(\"embeddings_skipgrams.txt\")\n",
    "clusterEmbeddings(filename=\"embeddings_skipgrams.txt\", numClusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "codecell"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
