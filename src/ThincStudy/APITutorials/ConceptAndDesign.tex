% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{fancyvrb,newverbs,xcolor} % for code highlighting
\usepackage[top=2cm, bottom=1.5cm, left=2cm, right=2cm]{geometry} % for page margins

\usepackage[english]{babel}
% Ana: adding graphics package for images
\usepackage{graphics}
\usepackage{graphicx}

% change background color for inline code in
% markdown files. The following code does not work well for
% long text as the text will exceed the page boundary
%\definecolor{bgcolor}{HTML}{E0E0E0}
%\let\oldtexttt\texttt

% \renewcommand{\texttt}[1]{
% \colorbox{bgcolor}{\oldtexttt{#1}}
% }


%% Setting pythong ??? -----------------------------------------------------
%default_block_language: "lexer"
%default_inline_language: "lexer"


%% color and other settings for hyperref package -----------------------------
\hypersetup{
    bookmarksopen=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=RoyalBlue,
}

% Font Setup  ---------------------------------------------------------
\usepackage{unicode-math} % load 'fontspec' automatically
\setmainfont{Crimson}
%\setmainfont{Libertinus Sans} 
%\setmainfont{Alegreya}
\setmathfont{TeX Gyre Schola Math}


% Code syntax highlighting ---------------------------------------------------

% OLD PART -----------------
%\usepackage{minted}
%\usemintedstyle{manni}
%\setmonofont{Inconsolata}
% ---------------------------


% Preliminary macro things for code (snatched from macros in REPORT):  ------
\newcommand\CodeFontSizeSmall{\fontsize{9pt}{9pt}\selectfont}

\definecolor{originalmannibg}{HTML}{f2f2ff}
\colorlet{BasePurple}{originalmannibg!90}
\newcommand{\lighten}[3]{% Reference Color, Percentage, New Color Name
    \colorlet{#3}{#1!#2!white}
}
\lighten{BasePurple}{50}{mannibg}

% Code things --------------------
\usepackage{minted}
\usepackage{verbatim}  % has commenting



\usemintedstyle{manni}

%\setmonofont{Inconsolata} % setting code font
\setmonofont{Fira Mono}

% General code environment, used like: \begin{code}{python} .... \end{code}
% NOTE: this is how to nest two environments together: 
\newenvironment{code}[2][]
 {\vspace{-3pt}%
 \VerbatimEnvironment
  \begin{adjustwidth}{30pt}{30pt}
  \begin{minted}[
    fontsize=\CodeFontSizeSmall,
    breaklines, mathescape,
    style=manni, bgcolor=mannibg,  #1]{#2}}
 {\end{minted}\end{adjustwidth} 
     \vspace{-10pt}
 }
 
% TODO: test if possible to do \renewenvironment to renew the minted environment and just include this logic below whenever calling \begin{minted}[]{python} ... 
 
% Python code environment, used like \begin{pythonCode} ... \end{pythonCode}
\newenvironment{pythonCode}
 {\vspace{-3pt}%
 \VerbatimEnvironment
  \begin{adjustwidth}{30pt}{30pt}
  \begin{minted}[
    fontsize=\CodeFontSizeSmall,
    breaklines, mathescape,
    style=manni, bgcolor=mannibg]{python}}
 {\end{minted}\end{adjustwidth} 
     \vspace{-10pt}
 }



% General code output environment
\newenvironment{outputCode}
 {\VerbatimEnvironment
  \begin{adjustwidth}{30pt}{30pt}
  \begin{minted}[
    fontsize=\CodeFontSizeSmall,
    breaklines]{text}}
 {\end{minted}\end{adjustwidth}}


% Creating inline code font (equivalent to backticks in jupyter notebooks)
% Must use like: \pythoninline{...text here ... }
\newmintinline{python}{python3, fontsize=\CodeFontSizeSmall, bgcolor=mannibg}

%\newenvironment{mintInline}[1][]{\mintinline{latex}{#1}}{}
%\DeclareTextFontCommand{\mint}{\mintInline}



\author{}
\date{}

\begin{document}

Source:
\url{https://thinc.ai/docs/concept\#annotations:rRYs7HvtEeqjKcMi29YdQw}

\hypertarget{concept-and-design}{%
\section{\texorpdfstring{\href{https://hyp.is/rRYs7HvtEeqjKcMi29YdQw/thinc.ai/docs/concept}{Concept
and Design}}{Concept and Design}}\label{concept-and-design}}

\hypertarget{model-composition-problem}{%
\subsection{Model Composition Problem}\label{model-composition-problem}}

The
\href{https://hyp.is/9KYtjE5DEeqIa_-fEBkiMQ/thinc.ai/docs/concept}{central
problem for a neural network} implementation is this: during the forward
pass, you compute results that will later be useful during the backward
pass. How do you keep track of this arbitrary state, while making sure
that layers can be cleanly composed?

\hypertarget{example-uncomposable-model}{%
\subsubsection{Example: Uncomposable
Model:}\label{example-uncomposable-model}}

The most obvious idea is that we have some thing called a model, and
this thing holds some parameters (``weights'') and has a method to
predict from some inputs to some outputs using the current weights. So
far so good. But we also need a way to update the weights. The most
obvious API for this is to add an update method, which will take a batch
of inputs and a batch of correct labels, and compute the weight update.

\begin{minted}[]{python}
class UncomposableModel:
    def __init__(self, W):
        self.W = W

    def predict(self, inputs):
        return inputs @ self.W.T

    def update(self, inputs, targets, learningRate=0.001):
        guesses = self.predict(inputs)
        dGuesses = (guesses - targets) / targets.shape[0]  # gradient of loss w.r.t. output

        # The @ is newish Python syntax for matrix multiplication
        dInputs = dGuesses @ self.W

        dW = dGuesses.T @ inputs  # gradient of parameters
        self.W -= learningRate * dW  # update weights

        return dInputs
\end{minted}

\hypertarget{problem-cannot-backprop-through-multiple-layers}{%
\subsubsection{Problem: Cannot Backprop Through Multiple
Layers}\label{problem-cannot-backprop-through-multiple-layers}}

The \mintinline[]{python}{update()} method only works as the outer-level
API. You wouldn't be able to put another layer with the same API after
this one and backpropagate through both of them. Let's look at the steps
for backpropagating through two matrix multiplications:

\begin{minted}[]{python}
def backpropTwoLayers(W1, W2, inputs, targets):
    hiddens = inputs @ W1.T
    guesses = hiddens @ W2.T

    dGuesses = (guesses-targets) / targets.shape[0]  # gradient of loss w.r.t. output
    dW2 = dGuesses @ hiddens.T
    dHiddens = dGuesses @ W2
    dW1 = dHiddens @ inputs.T
    dInputs = dHiddens @ W1

    return dW1, dW2, dInputs
\end{minted}

To update the first layer, we must know the \textbf{gradient with
respect to its output}, but that is only revealed after the full forward
pass, gradient of loss, and backpropagation through the second layer.
Hence, the
\href{https://hyp.is/cL4BsnvxEeqOtAf6xyOeLw/thinc.ai/docs/concept}{\mintinline[]{python}{UncomposableModel}
is uncomposable}: the \mintinline[]{python}{update} method expects the
input and target to both be available. This only works for the outermost
API, but not for intermediate layers. We would need another API for
intermediate layers.

\hypertarget{solution-reverse-model-auto-differentiation}{%
\subsubsection{Solution: Reverse-Model
Auto-Differentiation}\label{solution-reverse-model-auto-differentiation}}

\href{https://hyp.is/ftIl0HvxEeqgBZeOXtiyiw/thinc.ai/docs/concept}{Solution
is to base the API around the \mintinline[]{python}{predict} method},
which doesn't have the same composition problem, since there is no
problem with writing
\mintinline[]{python}{model3.predict(model2.predict(model1.predict(X)))},
or
\mintinline[]{python}{model3.predict(model2.predict(X) + model1.predict(X))}.

\textbf{Key Idea of Thinc:} To fix the API problem directly to enable
model composition, both forwards and backwards.

\hypertarget{key-design-1-no-explicit-computational-graph---just-higher-order-functions}{%
\subsection{\texorpdfstring{Key Design (1):
\href{https://hyp.is/XMJRpk5FEeqjof-24zQhoA/thinc.ai/docs/concept}{No
(explicit) Computational Graph - Just Higher Order
Functions}}{Key Design (1): No (explicit) Computational Graph - Just Higher Order Functions}}\label{key-design-1-no-explicit-computational-graph---just-higher-order-functions}}

\begin{minted}[]{python}
from thinc.types import *
from typing import *

def reduceSumLayer(X: Floats3d) -> Tuple[Floats2d, Callable[[Floats2d], Floats3d]]:
    Y: Floats2d = X.sum(axis = 1)

    # Backward pass runs from gradient-of-output (dY) to gradient-of-input (dX)
    # This means we will always have two matching pairs:
    # ---> (inputToForward, outputOfBackprop) == (X, dX), and
    # ---> (outputOfForward, inputOfBackprop) == (Y, dY) TODO ??
    def backpropReduceSum(dY: Floats2d) -> Floats3d:
        (dyFirstDim, dySecDim) = dY.shape
        dX: Floats3d = np.zeros(X.shape) # TODO thinc uses just `zeros` function -- from where??
        dX += dY.reshape((dyFirstDim, 1, dySecDim))

        return dX # d_inputs

    # outputs, backpropFunc
    return Y, backpropReduceSum


def reluLayer(inputs: Floats2d) -> Tuple[Floats2d, Callable[[Floats2d], Floats2d]]:
    mask: Floats2d = inputs >= 0
    outputs: Floats2d = inputs * mask

    def backpropRelu(dOutputs: Floats2d) -> Floats2d:
        return dOutputs * mask # == dInputs

    return outputs, backpropRelu
\end{minted}

\hypertarget{example-chain-combinator-using-callbacks}{%
\subsubsection{Example: Chain Combinator (using
callbacks)}\label{example-chain-combinator-using-callbacks}}

The most basic we we will want to combine layers is in a feed-forward
relationship. Calling this combinator \mintinline[]{python}{chain()},
after the calculus chain rule:

\begin{minted}[]{python}
def chain(firstLayer, secondLayer):
    def forwardChain(X):
        Y, getdX = firstLayer(X)
        Z, getdY = secondLayer(Y)

        def backpropChain(dZ):
            dY = getdY(dZ)
            dX = getdX(dY)

            return dX

        return Z, backpropChain

    return forwardChain
\end{minted}

We can use the \mintinline[]{python}{chain()} combinator to build a
function that runs our \mintinline[]{python}{reduceSUmLayer} and
\mintinline[]{python}{reluLayer} layers in succession:

\begin{minted}[]{python}

# from thinc.api import glorot_uniform_init
import numpy as np

chainedForward = chain(firstLayer = reduceSumLayer, secondLayer = reluLayer)

B, S, W = 2, 10, 6 # (batch size, sequence length, width)

# TODO don't know which method thinc uses here: 'uniform' ???? Looked everywhere in thinc.api and thinc.backends but it's not available ...
X = np.random.uniform(low = 0, high = 1, size = (B, S, W))
dZ = np.random.uniform(low = 0, high = 1, size = (B, W))

# Returns Z, backpropChain
Z, getdX = chainedForward(X = X)
# The backprop chain in action:
dX = getdX(dZ = dZ)

assert dX.shape == X.shape
\end{minted}

\hypertarget{example-chain-combinator-no-callbacks}{%
\subsubsection{Example: Chain Combinator (No
Callbacks)}\label{example-chain-combinator-no-callbacks}}

Our \mintinline[]{python}{chain} combinator works because our
\textbf{layers return callbacks}, ensuring no distinction in API between
the outermost layer and a layer that is part of a larger network.
Imagine the alternative, where the function expects the gradient with
respect to the output along its input:

\begin{minted}[]{python}
def reduceSum_noCallback(X: Floats3d, dY: Floats2d) -> Tuple[Floats2d, Floats3d]:
    Y: Floats2d = X.sum(axis = 1)

    # This was in the backprop method of reduceSumLayer():
    (dyFirstDim, dySecDim) = dY.shape

    dX: Floats3d = np.zeros(X.shape) # TODO thinc uses just `zeros` function -- from where??
    dX += dY.reshape((dyFirstDim, 1, dySecDim))

    return Y, dX


def relu_noCallback(inputs: Floats2d, dOutputs: Floats2d) -> Tuple[Floats2d, Floats2d]:
    mask: Floats2d = inputs >= 0
    outputs: Floats2d = inputs * mask

    # NOTE: this was in the backprop of the relu() method
    dInputs: Floats2d = dOutputs * mask
    #def backpropRelu(dOutputs: Floats2d) -> Floats2d:
    #    return dOutputs * mask
    #return inputs * mask, backpropRelu
    return outputs, dInputs


# How do we call `firstLayer`?
# We can't, because its signature expects dY  as part of its input â€“ but we don't know dY yet!
# We can only  compute dY once we have Y. That's why layers must return callbacks.
def chain_noCallback(firstLayer, secondLayer):

    def forwardChain_noCallback(X, dZ):

        # NO CALLBACK:
        # Y, dX = firstLayer(X = X, dY = ???) # this is the stumbling block

        # WITH CALLBACK:  the callback way doesn't require firstLayer to take dY as its argument:
        # Y, getdX = firstLayer(X)

        raise NotImplementedError()
\end{minted}

\hypertarget{key-design-2-encapsulation-modularity}{%
\subsection{Key Design (2): Encapsulation,
Modularity}\label{key-design-2-encapsulation-modularity}}

The problem with no callbacks is more than just functional: the extra
parameters passed in the functions in the above No Callback case are not
just another kind of input variable to the network. The parameters are
not part of the neural network design. We can't just say that parameters
(like dY in the \mintinline[]{python}{reduceSum_noCallback}) are part of
the network because that is not how we want to use the network. We
\href{https://hyp.is/HQ2zhE53EeqE19foO7IuTQ/thinc.ai/docs/concept}{want
the parameters of a layer to be an internal detail - \textbf{we don't
want to have to pass in the parameters on each input}.}

\href{https://hyp.is/MgSHrk53EeqvG1tywXRcJg/thinc.ai/docs/concept}{Parameters
must be handled differently from input variables (of a network) because
we want to specify them at different times. We'd like to specify the
parameters once \emph{when we create the function} and then have them be
an internal detail that doesn't affect the function's signature.}

\href{https://hyp.is/mz0GKnv9Eeq3Jg9d-UOOhA/thinc.ai/docs/concept}{The
most direct approach is to introduce another layer of closures, and make
the parameters and their gradients arguments to the outer layer. The
gradients can then be incremented during the backward pass:}

\begin{minted}[]{python}

# TODO ERROR this code piece has errors: says dW, db are referenced before assignment!!!


def Linear(W, b, dW, db):

    def forwardLinear(X):  # X = inputs

        Y = X @ W.T + b  # Y = outputs

        def backwardLinear(dY):  # dY = d_outputs
            dW = np.zeros(shape = W.shape)
            db = np.zeros(shape = b.shape)

            dW += dY.T @ X
            db += dY.sum(axis = 0)
            #print(dW, db)
            dX = dY @ W

            return dX    # dX = d_inputs

        return Y, backwardLinear

    return forwardLinear


(numBatches, nIn, nOut) = 128, 16, 32

# Initializing the inputs to neural network
W = np.random.uniform(low = 0, high = 1, size = (nOut, nIn)) # matrix
b = np.random.uniform(low = 0, high = 1, size = (nOut, )) # vector

# Initializing the derivatives
# TODO ERROR: initializing the class, just to compile because otherwise RunTime error "dW, db are referenced before assignment"
dW = np.zeros(shape = W.shape)
db = np.zeros(shape = b.shape)

X = np.random.uniform(low = 0, high = 1, size = (numBatches, nIn))
YTrue = np.random.uniform(low = 0, high = 1, size = (numBatches, nOut))

linear = Linear(W = W, b = b, dW = dW, db = db)
YOut, getdX = linear(X = X) # forward linear

dY = (YOut - YTrue) / numBatches # YTrue.shape[0]

dX = getdX(dY = dY) # backward linear

# Now we can do an optimization step like:
W -= 0.001 * dW
b -= 0.001 * db
#dW.fill(0.0)
#db.fill(0.0)
dW
\end{minted}

\begin{minted}[]{python}
array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
\end{minted}

Use a
\href{https://hyp.is/si5GiE5NEeqQF1_W9FWCsQ/thinc.ai/docs/concept}{\mintinline[]{python}{Model}
class to \textbf{keep track of parameters, gradients, dimensions}} since
handling parameters and their gradients explicitly quickly gets
unwieldy.

Two possible approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Inheritance Approach:} introduce one class per layer type,
  with the forward pass implemented as a method on the class (like
  PyTorch)
\item
  \textbf{Composition Approach:}

  \begin{itemize}
  \tightlist
  \item
    Each layer constructs a \mintinline[]{python}{Model} instance, and
    passes its \mintinline[]{python}{forward} function to this
    \mintinline[]{python}{Model} instance upon construction
    \href{https://hyp.is/-ceWpn7rEeqqmEdE6eVieg/thinc.ai/docs/api-layers}{(example
    is in the \mintinline[]{python}{thinc.layers.linear})}. The
    \mintinline[]{python}{Model} object lets you pass in an
    \href{https://hyp.is/bPmnGH7sEeqgGAMHdNcZBw/thinc.ai/docs/concept}{\mintinline[]{python}{init}
    function to support \textbf{shape inference}}.
  \item
    In the \mintinline[]{python}{forward} method, the
    \mintinline[]{python}{Model} instance is passed in as a parameter,
    giving you access to the dimensions, parameters, gradients,
    attributes, and layers. THe second argument of
    \mintinline[]{python}{forward} is the input data and the third
    argument is a boolean that lets layers run differently during
    training and prediction (customary feature).
  \end{itemize}
\end{enumerate}

Want to be able to define complex neural networks passing
\href{https://hyp.is/fjUbCE5OEeqhWS9aPoAuWA/thinc.ai/docs/concept}{\textbf{only
genuine configuration} - shouldn't have to pass in a lot of variables
whose values are dictated by the rest of the network.}

In the
\href{https://hyp.is/-ceWpn7rEeqqmEdE6eVieg/thinc.ai/docs/api-layers}{\mintinline[]{python}{Linear}}
example, there are many ways for the inputs to
\mintinline[]{python}{Linear} to be invalid: the
\mintinline[]{python}{W} and \mintinline[]{python}{dW} variables could
be different shapes, size of \mintinline[]{python}{b} could fail to
match first dimension of \mintinline[]{python}{W}, the second dimension
of \mintinline[]{python}{W} could fail to match the second dimension of
the input, etc. With separate inputs like these there is
\href{https://hyp.is/IiqFkHv-EeqFqTdLTRzgfQ/thinc.ai/docs/concept}{no
way we can expect functions to validate their inputs reliably}, leading
to upredictable logic errors that making debugging hard.

In a network with two \mintinline[]{python}{Linear} layers, only one
dimension is an actual hyperparameter. The input size to the first layer
and output size of the second layer are both \textbf{determined by the
shape of the data.} Thus the only free variable is number of hidden
units (this determines output size of the first layer and input size of
second layer).
\href{https://hyp.is/9PUiHk53EeqInoNDcqoGLw/thinc.ai/docs/concept}{Goal
to have missing dimensions \textbf{inferred layer} based on input and
output data.}

\textbf{Example: Initialization logic:}

To make this work, we need to specify the \textbf{initialization logic}
for each layer in the network. For example, the initialization logic for
the \mintinline[]{python}{Linear} and \mintinline[]{python}{chain}
layers is:

\begin{minted}[]{python}
from typing import Optional

from thinc.api import Model, glorot_uniform_init
from thinc.types import Floats2d
from thinc.util import get_width


def initLogic(model: Model,
              X: Optional[Floats2d] = None,
              Y: Optional[Floats2d] = None) -> None:

    if X is not None:
        model.set_dim(name = "nI", value = get_width(X = X))

    if Y is not None:
        model.set_dim(name = "nO", value = get_width(Y))


    W: Floats2d = model.ops.alloc2f(d0 = model.get_dim(name = "nO"),
                                    d1 = model.get_dim(name = "nI"))

    b: Floats1d = model.ops.alloc1f(d0 = model.get_dim(name = "nO"))

    glorot_uniform_init(ops = model.ops, shape = W.shape)

    model.set_param(name = "W", value = W)
    model.set_param(name = "b", value = b)
\end{minted}

\end{document}
