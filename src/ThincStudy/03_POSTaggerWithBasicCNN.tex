% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{fancyvrb,newverbs,xcolor} % for code highlighting
\usepackage[top=2cm, bottom=1.5cm, left=2cm, right=2cm]{geometry} % for page margins

\usepackage[english]{babel}
% Ana: adding graphics package for images
\usepackage{graphics}
\usepackage{graphicx}

% change background color for inline code in
% markdown files. The following code does not work well for
% long text as the text will exceed the page boundary
%\definecolor{bgcolor}{HTML}{E0E0E0}
%\let\oldtexttt\texttt

% \renewcommand{\texttt}[1]{
% \colorbox{bgcolor}{\oldtexttt{#1}}
% }


%% Setting pythong ??? -----------------------------------------------------
%default_block_language: "lexer"
%default_inline_language: "lexer"


%% color and other settings for hyperref package -----------------------------
\hypersetup{
    bookmarksopen=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=RoyalBlue,
}

% Font Setup  ---------------------------------------------------------
\usepackage{unicode-math} % load 'fontspec' automatically
\setmainfont{Crimson}
%\setmainfont{Libertinus Sans} 
%\setmainfont{Alegreya}
\setmathfont{TeX Gyre Schola Math}


% Code syntax highlighting ---------------------------------------------------

% OLD PART -----------------
%\usepackage{minted}
%\usemintedstyle{manni}
%\setmonofont{Inconsolata}
% ---------------------------


% Preliminary macro things for code (snatched from macros in REPORT):  ------
\newcommand\CodeFontSizeSmall{\fontsize{9pt}{9pt}\selectfont}

\definecolor{originalmannibg}{HTML}{f2f2ff}
\colorlet{BasePurple}{originalmannibg!90}
\newcommand{\lighten}[3]{% Reference Color, Percentage, New Color Name
    \colorlet{#3}{#1!#2!white}
}
\lighten{BasePurple}{50}{mannibg}

% Code things --------------------
\usepackage{minted}
\usepackage{verbatim}  % has commenting



\usemintedstyle{manni}

%\setmonofont{Inconsolata} % setting code font
\setmonofont{Fira Mono}

% General code environment, used like: \begin{code}{python} .... \end{code}
% NOTE: this is how to nest two environments together: 
\newenvironment{code}[2][]
 {\vspace{-3pt}%
 \VerbatimEnvironment
  \begin{adjustwidth}{30pt}{30pt}
  \begin{minted}[
    fontsize=\CodeFontSizeSmall,
    breaklines, mathescape,
    style=manni, bgcolor=mannibg,  #1]{#2}}
 {\end{minted}\end{adjustwidth} 
     \vspace{-10pt}
 }
 
% TODO: test if possible to do \renewenvironment to renew the minted environment and just include this logic below whenever calling \begin{minted}[]{python} ... 
 
% Python code environment, used like \begin{pythonCode} ... \end{pythonCode}
\newenvironment{pythonCode}
 {\vspace{-3pt}%
 \VerbatimEnvironment
  \begin{adjustwidth}{30pt}{30pt}
  \begin{minted}[
    fontsize=\CodeFontSizeSmall,
    breaklines, mathescape,
    style=manni, bgcolor=mannibg]{python}}
 {\end{minted}\end{adjustwidth} 
     \vspace{-10pt}
 }



% General code output environment
\newenvironment{outputCode}
 {\VerbatimEnvironment
  \begin{adjustwidth}{30pt}{30pt}
  \begin{minted}[
    fontsize=\CodeFontSizeSmall,
    breaklines]{text}}
 {\end{minted}\end{adjustwidth}}


% Creating inline code font (equivalent to backticks in jupyter notebooks)
% Must use like: \pythoninline{...text here ... }
\newmintinline{python}{python3, fontsize=\CodeFontSizeSmall, bgcolor=mannibg}

%\newenvironment{mintInline}[1][]{\mintinline{latex}{#1}}{}
%\DeclareTextFontCommand{\mint}{\mintInline}



\author{}
\date{}

\begin{document}

\href{https://github.com/explosion/thinc/blob/master/examples/03_pos_tagger_basic_cnn.ipynb}{Source}

\hypertarget{basic-cnn-part-of-speech-tagger-with-thinc}{%
\section{Basic CNN Part-of-Speech Tagger with
Thinc}\label{basic-cnn-part-of-speech-tagger-with-thinc}}

We implement a basic CNN for pos-tagging (without external dependencies)
in Thinc, and train the model on the Universal Dependencies
\href{https://github.com/UniversalDependencies/UD_Spanish-AnCora}{AnCora
corpus}.

This tutorial shows three different workflows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Composing the model in \textbf{code}
\item
  Composing the model using \textbf{only config file}
\item
  Composing the model in \textbf{code and configuring it via config}
  (recommended approach)
\end{enumerate}

\begin{minted}[]{python}
from thinc.api import prefer_gpu
from thinc.config import Config

prefer_gpu()
\end{minted}

\begin{minted}[]{python}
False
\end{minted}

Define the helper functions for loading data, and training and
evaluating a given model. * NOTE: need to call
\mintinline[]{python}{model.initialize} with a batch of input and output
data to initialize model and infer missing shape dimensions.

\begin{minted}[]{python}
import ml_datasets
from tqdm.notebook import tqdm
from thinc.api import fix_random_seed, Model
from thinc.optimizers import Optimizer

from thinc.types import Array2d
from typing import Optional, List


fix_random_seed(0)

def trainModel(model: Model, optimizer: Optimizer, numIters: int, batchSize: int):

    (trainX, trainY), (devX, devY) = ml_datasets.ud_ancora_pos_tags()
    # Need to do shape inference:
    model.initialize(X = trainX[:5], Y = trainY[:5])

    for epoch in range(numIters):
        loss: float = 0.0
        # todo: type??
        batches = model.ops.multibatch(batchSize, trainX, trainY, shuffle=True)

        for X, Y in tqdm(batches, leave = False):
            Yh, backprop = model.begin_update(X = X)
            # todo type ??
            dLoss = []

            for i in range(len(Yh)):
                dLoss.append(Yh[i] - Y[i])
                loss += ((Yh[i] - Y[i]) ** 2).sum()

            backprop(dLoss)
            model.finish_update(optimizer = optimizer)

        # todo type?
        score = evaluate(model = model, devX = devX, devY = devY, batchSize = batchSize)
        #print(f"{i}\t{loss:.2f}\t{score:.3f}")
        print("Epoch: {} | Loss: {} | Score: {}".format(epoch, loss, score))




# todo types??
def evaluate(model: Model, devX, devY, batchSize: int) -> float:

    numCorrect: float = 0.0
    total: float = 0.0

    for X, Y in model.ops.multibatch(batchSize, devX, devY):
        # todo type of ypred??
        Yh = model.predict(X = X)

        for yh, y in zip(Yh, Y):
            numCorrect += (y.argmax(axis = 1) == yh.argmax(axis=1)).sum()

            # todo: what is the name of the dimension shape[0]?
            total += y.shape[0]

    return float(numCorrect / total)
\end{minted}

\hypertarget{composing-the-model-in-code}{%
\subsection{1. Composing the Model in
Code}\label{composing-the-model-in-code}}

Here's the model definition, using \ldots{} * \mintinline[]{python}{>>}
operator for the \mintinline[]{python}{chain} combinator. *
\mintinline[]{python}{strings2arrays} to transform a sequence of strings
to a list of arrays * \mintinline[]{python}{with_array} transforms
sequences (the passed sequences of arrays) into a contiguous
two-dimensional array on the way into and out of the model it wraps.

Final model signature:
\mintinline[]{python}{Model[Sequence[str], Sequence[Array2d]]}

\begin{minted}[]{python}
from thinc.api import Model, chain, strings2arrays, with_array, HashEmbed, expand_window, Relu, Softmax, Adam, warmup_linear

width: int = 32
vectorWidth: int = 16
numClasses: int = 17
learnRate: float = 0.001
numIters: int = 10
batchSize: int = 128

with Model.define_operators(operators = {">>": chain}):

    modelFromCode = strings2arrays() >> with_array(

        layer = HashEmbed(nO = width, nV = vectorWidth, column=0)
        >> expand_window(window_size=1)
        >> Relu(nO = width, nI = width * 3)
        >> Relu(nO = width, nI = width)
        >> Softmax(nO = numClasses, nI = width)
    )
\end{minted}

\begin{minted}[]{python}
optimizer = Adam(learn_rate = learnRate)
\end{minted}

\begin{minted}[]{python}
modelFromCode
\end{minted}

\begin{minted}[]{python}
<thinc.model.Model at 0x7fedb7d69598>
\end{minted}

Training the model now:

\begin{minted}[]{python}
trainModel(model = modelFromCode,
           optimizer = optimizer,
           numIters = numIters,
           batchSize = batchSize)
#
\end{minted}

\begin{minted}[]{python}
HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 0 | Loss: 387245.6607032418 | Score: 0.43985546589781516



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 1 | Loss: 291325.42196020484 | Score: 0.540711062849868



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 2 | Loss: 259087.54757650197 | Score: 0.5839776833355176



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 3 | Loss: 230784.68576764315 | Score: 0.6207103139685095



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 4 | Loss: 212752.10526858037 | Score: 0.6424091513302005



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 5 | Loss: 203007.67740350612 | Score: 0.6580794937562017



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 6 | Loss: 196406.51488909405 | Score: 0.668376612435175



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 7 | Loss: 191051.0628584926 | Score: 0.6745923277104825



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 8 | Loss: 186566.58660080412 | Score: 0.681781588751802



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 9 | Loss: 182936.37714191142 | Score: 0.6885402430120008
\end{minted}

\hypertarget{composing-the-model-via-a-config-file}{%
\subsection{2. Composing the Model via a Config
File}\label{composing-the-model-via-a-config-file}}

Thinc's config system lets describe \textbf{arbitrary trees of objects}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The config can include values like hyperparameters or training
  settings, or references to functions and the values of their
  arguments.
\item
  Thinc then creates the config \textbf{bottom-up} so you can define one
  function with its arguments, then pass the return value into another
  function.
\end{enumerate}

To rebuild the model in the above config file we need to break down its
structure:

\begin{itemize}
\tightlist
\item
  \mintinline[]{python}{chain} (takes any number of positional
  arguments)
\item
  \mintinline[]{python}{strings2array} (with no arguments)
\item
  \mintinline[]{python}{with_array} (one argument \textbf{layer})

  \begin{itemize}
  \tightlist
  \item
    \textbf{layer:} \mintinline[]{python}{chain} (any number of
    positional arguments)
  \item
    \mintinline[]{python}{HashEmbed}
  \item
    \mintinline[]{python}{Relu}
  \item
    \mintinline[]{python}{Relu}
  \item
    \mintinline[]{python}{Softmax}
  \end{itemize}
\end{itemize}

\mintinline[]{python}{chain} takes arbitrarily many positional arguments
(layers to compose). In the config, positional arguments can be
expressed using \(*\) in the dot notation (For example,
\mintinline[]{python}{model.layer} could describe a function passed to
\mintinline[]{python}{model} as the argument
\mintinline[]{python}{layer}, while \mintinline[]{python}{model.*.relu}
defines a positional argument passed to \mintinline[]{python}{model}. In
this case, the argument name \mintinline[]{python}{relu} doesn't matter,
it just needs to be unique.)

\begin{itemize}
\tightlist
\item
  NOTE: not recommended to ``program via config files'' because it
  doesn't solve any problem and makes the model definition just as
  complicated.
\item
  NOTE: recommend instead the hybrid approach: wrap the model definition
  in a registered function and configure it via the config.
\item
  NOTE: need to keep function names so can't start using camelcase at
  the naming ``.v1'' part because otherwise we get this error when
  calling \mintinline[]{python}{registry.make_from_config(CONFIG)}:
  Cant't find \mintinline[]{python}{'withArray.v1'} in registry
  \mintinline[]{python}{thinc -> layers}. Available names:
  \mintinline[]{python}{CauchySimilarity.v1, Dropout.v1, Embed.v1, FeatureExtractor.v1, HashEmbed.v1}

  \begin{itemize}
  \tightlist
  \item
    NOTE: can also get \mintinline[]{python}{ConfigValidationError} if
    the names like \mintinline[]{python}{learn_rate} are not spelled
    correctly (to match later function arguments).
  \end{itemize}
\end{itemize}

\begin{minted}[]{python}
CONFIG_STR: str = """
[hyper_params]
width = 32
vector_width = 16
learn_rate = 0.001

[training]
n_iter = 10
batch_size = 128

[model]
@layers = "chain.v1"

[model.*.strings2arrays]
@layers = "strings2arrays.v1"

[model.*.with_array]
@layers = "with_array.v1"

[model.*.with_array.layer]
@layers = "chain.v1"

[model.*.with_array.layer.*.hashembed]
@layers = "HashEmbed.v1"
nO = ${hyper_params:width}
nV = ${hyper_params:vector_width}
column = 0

[model.*.with_array.layer.*.expand_window]
@layers = "expand_window.v1"
window_size = 1

[model.*.with_array.layer.*.relu1]
@layers = "Relu.v1"
nO = ${hyper_params:width}
nI = 96

[model.*.with_array.layer.*.relu2]
@layers = "Relu.v1"
nO = ${hyper_params:width}
nI = ${hyper_params:width}

[model.*.with_array.layer.*.softmax]
@layers = "Softmax.v1"
nO = 17
nI = ${hyper_params:width}

[optimizer]
@optimizers = "Adam.v1"
learn_rate = ${hyper_params:learn_rate}
"""
\end{minted}

When the config is loaded it is parsed as a dictionary and all
references to values from other sections (like
\mintinline[]{python}{${hyperParams:width}}) are replaced by their
defined values. The result is a nested dictionary describing the objects
defined in the config.

\begin{minted}[]{python}
from thinc.api import registry, Config

config: Config = Config().from_str(CONFIG_STR)
config
\end{minted}

\begin{minted}[]{python}
{'hyper_params': {'width': 32, 'vector_width': 16, 'learn_rate': 0.001},
 'training': {'n_iter': 10, 'batch_size': 128},
 'model': {'@layers': 'chain.v1',
  '*': {'strings2arrays': {'@layers': 'strings2arrays.v1'},
   'with_array': {'@layers': 'with_array.v1',
    'layer': {'@layers': 'chain.v1',
     '*': {'hashembed': {'@layers': 'HashEmbed.v1',
       'nO': 32,
       'nV': 16,
       'column': 0},
      'expand_window': {'@layers': 'expand_window.v1', 'window_size': 1},
      'relu1': {'@layers': 'Relu.v1', 'nO': 32, 'nI': 96},
      'relu2': {'@layers': 'Relu.v1', 'nO': 32, 'nI': 32},
      'softmax': {'@layers': 'Softmax.v1', 'nO': 17, 'nI': 32}}}}}},
 'optimizer': {'@optimizers': 'Adam.v1', 'learn_rate': 0.001}}
\end{minted}

Next, use \mintinline[]{python}{registry.make_from_config} to create the
objects and call the functions \textbf{bottom-up}.

\begin{minted}[]{python}
CONFIG: Config = registry.make_from_config(config)
CONFIG
\end{minted}

\begin{minted}[]{python}
{'hyper_params': {'width': 32, 'vector_width': 16, 'learn_rate': 0.001},
 'training': {'n_iter': 10, 'batch_size': 128},
 'model': <thinc.model.Model at 0x7fedb7d698c8>,
 'optimizer': <thinc.optimizers.Optimizer at 0x7fedb7cbd198>}
\end{minted}

Training the model, since we have declared the model, optimizer, and
training settings:

\begin{minted}[]{python}
modelFromConfig: Model = CONFIG["model"]
optimizer: Optimizer = CONFIG["optimizer"]
numIters: int = CONFIG["training"]["n_iter"]
batchSize: int = CONFIG["training"]["batch_size"]
\end{minted}

\begin{minted}[]{python}
modelFromConfig
\end{minted}

\begin{minted}[]{python}
<thinc.model.Model at 0x7fedb7d698c8>
\end{minted}

\begin{minted}[]{python}
trainModel(model = modelFromConfig,
           optimizer = optimizer,
           numIters = numIters,
           batchSize = batchSize)
\end{minted}

\begin{minted}[]{python}
HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 0 | Loss: 393883.2075020075 | Score: 0.41403778106453487



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 1 | Loss: 290904.9920806326 | Score: 0.5343642933368281



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 2 | Loss: 262409.2779584527 | Score: 0.5655739239510981



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 3 | Loss: 250073.04503394663 | Score: 0.5863179375807388



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 4 | Loss: 239642.0183173269 | Score: 0.604665530863273



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 5 | Loss: 226512.68565293401 | Score: 0.625390822458952



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 6 | Loss: 213239.2184684947 | Score: 0.6420159886170034



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 7 | Loss: 203582.5307474602 | Score: 0.6564506768015277



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 8 | Loss: 196849.01451857202 | Score: 0.6644262632692416



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 9 | Loss: 191774.66215213854 | Score: 0.6734128395708909
\end{minted}

\hypertarget{composing-the-model-with-code-and-config}{%
\subsection{3. Composing the Model with Code and
Config}\label{composing-the-model-with-code-and-config}}

\hypertarget{creating-the-code}{%
\subsubsection{Creating the Code:}\label{creating-the-code}}

Can register your own layers and model definitions using the
\mintinline[]{python}{@thinc.registry} decorator. These can later be
referenced in config files \(\rightarrow\) gives flexibility while
keeping config and model definitions concise.

\begin{itemize}
\tightlist
\item
  NOTE: The function you register will be filled in by the config --
  e.g.~the value of \mintinline[]{python}{width} defined in the config
  block will be passed in as the argument \mintinline[]{python}{width}.
  If arguments are missing, you'll see a validation error. If you're
  using \textbf{type hints} in the function, the values will be parsed
  to ensure they always have the right type. If the types are invalid --
  e.g.~if you're passing in a \mintinline[]{python}{list} instead of
  \mintinline[]{python}{int} as the value of
  \mintinline[]{python}{width} -- you'll see an error. This makes it
  easier to prevent bugs caused by incorrect values lower down in the
  network.
\end{itemize}

\begin{minted}[]{python}
import thinc
from thinc.api import Model, chain, strings2arrays, with_array, HashEmbed, expand_window, Relu, Softmax, Adam, warmup_linear

@thinc.registry.layers("CnnTagger.v1")
def createCnnTagger(width: int, vectorWidth: int, numClasses: int = 17):

    with Model.define_operators({">>": chain}):
        model: Model = strings2arrays() \
                       >> with_array(layer =
                                     HashEmbed(nO = width, nV = vectorWidth, column = 0)
                                     >> expand_window(window_size=1)
                                     >> Relu(nO = width, nI = width * 3)
                                     >> Relu(nO = width, nI = width)
                                     >> Softmax(nO = numClasses, nI = width)
                                     )

        return model
\end{minted}

\hypertarget{creating-the-config}{%
\subsubsection{Creating the Config:}\label{creating-the-config}}

The config now must only define one model block with @layers =
``CnnTagger.v1'' and the function arguments. Can optionally move
function arguments to a section like
\mintinline[]{python}{[hyper_param]} or could hard-code them into the
block.

Advantage of separate section: values are \textbf{preserved in the
parsed config object} (so not just passed into the function) so can
always print and view them.

\begin{minted}[]{python}
CONFIG_STR: str = """
[hyper_params]
width = 32
vector_width = 16
learn_rate = 0.001

[training]
n_iter = 10
batch_size = 128

[model]
@layers = "CnnTagger.v1"
width = ${hyper_params:width}
vectorWidth = ${hyper_params:vector_width}
numClasses = 17

[optimizer]
@optimizers = "Adam.v1"
learn_rate = ${hyper_params:learn_rate}
"""
\end{minted}

\begin{minted}[]{python}
CONFIG: Config = registry.make_from_config(Config().from_str(CONFIG_STR))
CONFIG
\end{minted}

\begin{minted}[]{python}
{'hyper_params': {'width': 32, 'vector_width': 16, 'learn_rate': 0.001},
 'training': {'n_iter': 10, 'batch_size': 128},
 'model': <thinc.model.Model at 0x7fedb3b760d0>,
 'optimizer': <thinc.optimizers.Optimizer at 0x7fedb3c406d8>}
\end{minted}

Training the model now:

\begin{minted}[]{python}
modelFromCodeAndConfig: Model = CONFIG["model"]
optimizer: Optimizer = CONFIG["optimizer"]
numIters = CONFIG["training"]["n_iter"]
batchSize = CONFIG["training"]["batch_size"]
\end{minted}

\begin{minted}[]{python}
modelFromCodeAndConfig
\end{minted}

\begin{minted}[]{python}
<thinc.model.Model at 0x7fedb3b760d0>
\end{minted}

\begin{minted}[]{python}
trainModel(model = modelFromCodeAndConfig,
           optimizer = optimizer,
           numIters = numIters,
           batchSize = batchSize)
\end{minted}

\begin{minted}[]{python}
HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 0 | Loss: 398764.6167009473 | Score: 0.3471252316851703



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 1 | Loss: 316407.5635571573 | Score: 0.4933817609945144



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 2 | Loss: 278551.48323122226 | Score: 0.5623724561436354



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 3 | Loss: 242246.51710079028 | Score: 0.6112556868178158



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 4 | Loss: 218306.53896981804 | Score: 0.6410798869189148



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 5 | Loss: 206447.67564318783 | Score: 0.6521446089903207



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 6 | Loss: 199336.3151329594 | Score: 0.6641641547937768



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 7 | Loss: 193730.89788747078 | Score: 0.6706607005785109



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 8 | Loss: 189242.50445418147 | Score: 0.6779248497556775



HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))


Epoch: 9 | Loss: 185335.72993982863 | Score: 0.6827364124838522
\end{minted}

\end{document}
